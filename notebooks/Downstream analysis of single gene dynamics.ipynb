{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55cb0ad",
   "metadata": {},
   "source": [
    "# GPA for transportation of gene expression datasets\n",
    "\n",
    "This notebook aims to apply GPA (our baseline model) to recover the trajectory of an empirical (EMT) gene expression dataset with unknown dynamics.\n",
    "We first reduce the dimensionality of the original data (175) to an accessible dimension through PCA, then apply GPA in the latent space, and reconstruct the result to the original high-dimensional space.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "* 6 snapshots at different timepoints (day 0, 1, 2, 3, 4, 8)\n",
    "* samplesize: differ by days\n",
    "* dimension: 175\n",
    "* The trajectory will be visualized in 2D principal component axes\n",
    "\n",
    "## Functionality\n",
    "\n",
    "* Given source and target days, GPA transports the source dataset toward the target dataset from a deterministic particle dynamics for the gradient flow of (Lipschitz regularized or limited transportation speed) KL divergergence.\n",
    "$$D_{KL}^L(P\\|Q) = \\sup_{\\| \\nabla \\phi \\| \\leq L} \\left \\{\\mathbb{E}_P[\\phi] - \\log \\mathbb{E}_Q [\\exp(\\phi)] \\right \\}$$\n",
    "$$\\partial_t P + \\nabla \\cdot \\left(P v\\right) = \\partial_t P - \\nabla \\cdot \\left(P \\nabla \\phi \\right) = 0$$\n",
    "$$\\dot{X} = - \\nabla \\phi_t(X)$$\n",
    "\n",
    "* Snapshots at all the intermediate points will be highlighted by default, but it can be specified.\n",
    "* $W_2$ distance will be calculated between the particle trajectory and the snapshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "def load_W(filename):\n",
    "    with open(filename, \"rb\") as fr:\n",
    "        W, b, p = pk.load(fr)\n",
    "\n",
    "    W = [tf.Variable(w,  dtype=tf.float32) for w in W]\n",
    "    b = [tf.Variable(b_,  dtype=tf.float32) for b_ in b]\n",
    "\n",
    "    return W, b, p\n",
    "\n",
    "def v(x, t, W, b):   # neural newtork for time-dependent vectorfield\n",
    "    num_layers = len(W)\n",
    "    activation_ftn = tf.nn.tanh\n",
    "        \n",
    "    h = tf.concat([x, t*tf.ones([x.shape[0], 1], dtype=tf.float32)], axis=1)\n",
    "    for l in range(0,num_layers-1):\n",
    "        h = activation_ftn(tf.add(tf.matmul(h, W[l]), b[l]))\n",
    "    out=tf.add(tf.matmul(h, W[-1]), b[-1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def time_integration(x0, T, dt):\n",
    "    x = tf.constant(x0, dtype=tf.float32)\n",
    "    xs = [x0]\n",
    "    for i in range(int(T/dt)):\n",
    "        vv = v(x, dt*i, W, b)\n",
    "        x += dt * vv\n",
    "        xs.append(x.numpy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def generate_animation(days, intermediate_days, X1_trpts, dt, physical_dt, img_src, d_red = 2, vs = None):\n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}    \n",
    "    \n",
    "    \n",
    "    for i, X1_trpt in enumerate(X1_trpts):  # trajectories\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        X1_trpt_vis = X1_trpt\n",
    "        \n",
    "        if type(vs) != type(None) and i < len(X1_trpts)-1:\n",
    "            X1_trpt_vis_next = X1_trpts[i+1]\n",
    "            vs_vis = (X1_trpt_vis_next-X1_trpt_vis) / dt\n",
    "            im = ax.quiver(X1_trpt_vis[:, 0], X1_trpt_vis[:, 1], vs_vis[:, 0], vs_vis[:, 1], \n",
    "                           width=0.003, headwidth=7, headlength=15, headaxislength=7, zorder=15)                            \n",
    "        else:\n",
    "            im = ax.scatter(X1_trpt_vis[:,0], X1_trpt_vis[:,1], color=colors[days[0]], \n",
    "                            alpha=1.0, s=0.7, zorder=10, label=f'day {days[0]}') # transported source \n",
    "            for t in days[1:]:\n",
    "                X2_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X2_vis[:,0], X2_vis[:,1], color=colors[t], \n",
    "                   alpha=1.0, s=0.7, zorder=5, label=f'day {t}') # target \n",
    "            \n",
    "            for t in intermediate_days:\n",
    "                X1_intermediate_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X1_intermediate_vis[:,0], X1_intermediate_vis[:,1], color='lightgray', \n",
    "                        alpha=0.3, s=0.7, zorder=1, label=f'day {t}')\n",
    "        #ax.set_xlim([-5,6])\n",
    "        #ax.set_ylim([-5,6])\n",
    "        \n",
    "        #leg = ax.legend(loc='upper right')\n",
    "        ttl = ax.text(0.5,1.05, \"t = %.3f\" % (physical_dt*i), \\\n",
    "                      bbox={'facecolor':'w', 'alpha':0.5, 'pad':5}, \\\n",
    "                      transform=ax.transAxes, ha=\"center\")\n",
    "        ims.append([im, ttl])#, leg])\n",
    "        \n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=200)\n",
    "    writergif = animation.PillowWriter(fps=3)\n",
    "    ani.save(img_src, writer=writergif)\n",
    "    plt.clf()\n",
    "    display(Image(filename = img_src))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3201aa-0747-44cf-837f-9310eef4786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for piecewise GPA (sample 3 - stem cell and sample 5 - synthetic)\n",
    "\n",
    "\n",
    "## Static plot function for piecewise GPA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_three_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#2ca02c',  # Green\n",
    "        middle_t: '#ff7f0e',  # Orange\n",
    "        intermediate_days[1]: '#8c564b',  # Brown\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax1.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=5, zorder = 1)\n",
    "\n",
    "    \n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=24)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=24)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax2.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "    # Create legend with smaller markers and tighter spacing\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles,\n",
    "        sorted_labels,\n",
    "        fontsize=20,         # Font size of text\n",
    "        loc='center',\n",
    "        ncol=len(sorted_labels),\n",
    "        markerscale=2,     # Scale down marker size in legend\n",
    "        handlelength=1.5,    # Length of the marker line\n",
    "        handletextpad=0.2    # Padding between marker and label text\n",
    "    )    \n",
    "\n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d6d3-f9f8-4757-b381-2442975fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (Sample 1 - EMT data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.0, alpha=0.75, s=10, zorder = 20,  label=f'Day {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=27)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=27)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=30)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=27)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "        \n",
    "    # Create legend with larger markers for scatter plots\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "        ncol=len(sorted_labels), markerscale=2)\n",
    "    \n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b79cf-56f9-44f8-b98e-77e37b64fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=20)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Proceed only if legend elements exist\n",
    "    if handles and labels:\n",
    "        # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "        time_labels = []\n",
    "        for label in labels:\n",
    "            try:\n",
    "                time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "                time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "            except ValueError:\n",
    "                time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "        # Sort legend by time values\n",
    "        time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "        sorted_labels = [item[1] for item in time_labels]\n",
    "        sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "        # **Increase marker size in legend**\n",
    "        for handle in sorted_handles:\n",
    "            if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "                handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "        # Create a separate figure for the legend\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "        ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "        # Create legend with larger markers for scatter plots\n",
    "        legend = ax_legend.legend(\n",
    "            sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "            ncol=len(sorted_labels), markerscale=6  # Increase scatter marker size\n",
    "        )\n",
    "    \n",
    "        # Save the legend separately\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "    else:\n",
    "        print(\"No legend elements found â€” skipping separate legend plot.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b995d3-f5f3-4e68-b20f-ec1800daae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data) - separate legend\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days, intermediate_days, X1_trpts, mats, d_red=26,\n",
    "    output_file_with_snapshots=None,\n",
    "    output_file_without_snapshots=None,\n",
    "    output_file_snapshots_only=None\n",
    "):\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    source_t, _, target_t = days[0], days[1], days[-1]\n",
    "\n",
    "    color_map = {\n",
    "        source_t: 'magenta',  # Blue\n",
    "        target_t: '#008080'   # Red\n",
    "    }\n",
    "\n",
    "    # --- Plot 1: WITH snapshots (No inline colorbar) ---\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder=1)\n",
    "\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax1.tick_params(axis='both', labelsize=32)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Save colorbar separately ---\n",
    "    fig_cb, ax_cb = plt.subplots(figsize=(10, 1))\n",
    "    cb = plt.colorbar(sm, cax=ax_cb, orientation='horizontal')\n",
    "    cb.set_ticks([0, 4])\n",
    "    cb.set_ticklabels([\"Pre-treatment\", \"Post-treatment\"])\n",
    "    cb.ax.tick_params(labelsize=24)\n",
    "    cb.set_label(\"Time\", fontsize=24)\n",
    "    cb_path = os.path.join(result_dir, \"trajectory_colorbar_only.png\")\n",
    "    fig_cb.savefig(cb_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_cb)\n",
    "    print(f\"Standalone colorbar saved to {cb_path}\")\n",
    "\n",
    "    # --- Plot 2: WITHOUT snapshots ---\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8, zorder=15, label='Pre-treatment')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8, zorder=10, label='Post-treatment')\n",
    "\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax2.tick_params(axis='both', labelsize=32)\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Legend (just for pre/post-treatment) ---\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(6, 2))\n",
    "        ax_legend.axis(\"off\")\n",
    "        ax_legend.legend(\n",
    "            handles, labels, fontsize=16, loc='center',\n",
    "            ncol=len(labels), markerscale=3, handletextpad=0.5\n",
    "        )\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close(fig_legend)\n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a373d4-d7ed-4142-8b91-9b5ad8633521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136033ed-955a-43c7-849c-6ee8c75c3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c365c-319c-4eff-bf24-67435d1f6137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6f201-8cf9-44a7-8e30-05d38a306d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594032d-8820-4228-b82d-0f576845ca33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e85a9-2644-4dae-a799-d814c319b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6faa1-cae0-45a1-ae38-eec202c26b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819b6a5-bd12-46e5-936a-4dc7f39da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (samples 1 - EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[2],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75def9f-4e12-4edb-98a5-648bb7d36b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (Sample 5 - synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636e70b-e41c-4e1b-977a-1684daa4e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (samples 3 - stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc9293-f90b-424c-988f-01d52bb13a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154d2ad-1e37-4b94-8b38-b70df9d4706e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a20c9-19ef-4817-b018-55ee0ed5174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85b29a-ff95-4834-86aa-eeaafb43ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21ad8-a556-4f35-86d4-49068a3a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_cell_types(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_cell_type_source=None, output_file_cell_type_target=None, output_file_cell_type_legend=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "\n",
    "    # Same PCA transformation and cell type extraction as before\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    cell_types_X1 = cell_types_by_day[source_t]\n",
    "    cell_types_X2 = cell_types_by_day[target_t]\n",
    "    unique_cell_types = np.unique(np.concatenate([cell_types_X1, cell_types_X2]))\n",
    "    cell_type_palette = dict(zip(unique_cell_types, sns.color_palette(\"tab20\", len(unique_cell_types))))\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 1: X1 colored, X2 gray (no legend)\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X1 == cell_type\n",
    "        ax1.scatter(X1_vis[idx, 0], X1_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax1.set_title(f\"Untreated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_source:\n",
    "        plt.savefig(output_file_cell_type_source, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 2: X2 colored, X1 gray (no legend)\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X2 == cell_type\n",
    "        ax2.scatter(X2_vis[idx, 0], X2_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax2.set_title(f\"Treated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_target:\n",
    "        plt.savefig(output_file_cell_type_target, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    # Use circle markers instead of patches for legend\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create circle markers for legend entries\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create figure and axis (just for the legend)\n",
    "    fig_leg, ax_leg = plt.subplots()\n",
    "    fig_leg.set_figwidth(8)  # Initial size; will be adjusted\n",
    "    fig_leg.set_figheight(6)\n",
    "    \n",
    "    # Hide axes\n",
    "    ax_leg.axis('off')\n",
    "    \n",
    "    # Add legend to axis (not directly to plt)\n",
    "    legend = ax_leg.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='center',\n",
    "        frameon=True,\n",
    "        fontsize=14,\n",
    "        ncol=1,\n",
    "        title='Cell Types',\n",
    "        title_fontsize=14,\n",
    "        borderpad=1\n",
    "    )\n",
    "    \n",
    "    # Resize the figure to tightly fit the legend\n",
    "    fig_leg.canvas.draw()\n",
    "    bbox = legend.get_window_extent().transformed(fig_leg.dpi_scale_trans.inverted())\n",
    "    fig_leg.set_size_inches(bbox.width + 0.5, bbox.height + 0.5)  # Add a little padding\n",
    "    \n",
    "    # Save only, no display\n",
    "    if output_file_cell_type_legend:\n",
    "        plt.savefig(output_file_cell_type_legend, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_leg)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baaf34-9a34-442b-aee1-171ebe303d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell types plots\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_dim20-f_Lip=5e-3-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 20\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_cell_type_source = f\"{result_dir}{exp_name}_cell_type_source.png\"\n",
    "output_file_cell_type_target = f\"{result_dir}{exp_name}_cell_type_target.png\"\n",
    "output_file_cell_type_legend = f\"{result_dir}{exp_name}_cell_type_legend.png\"\n",
    "\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_cell_types(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_cell_type_source=output_file_cell_type_source,\n",
    "    output_file_cell_type_target=output_file_cell_type_target,\n",
    "    output_file_cell_type_legend=output_file_cell_type_legend\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d059d1-044e-48f9-8f2a-6dc9089330b3",
   "metadata": {},
   "source": [
    "## Downstream Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199daedb-560c-437d-8ecc-921ceb5ed967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d58da-6be3-428f-81df-efd58cfc4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For clinical data to identify cells with most change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def plot_X1_hat_displacement_distribution(d_red=2, random_state=42, exp_memo='2'):\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Compute trajectory\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Extract first and last time point\n",
    "    X1_hat_first = X1_trpts[0].astype(np.float32)\n",
    "    X1_hat_last = X1_trpts[-1].astype(np.float32)\n",
    "\n",
    "    # Compute Euclidean distances for each cell\n",
    "    displacements = np.linalg.norm(X1_hat_last - X1_hat_first, axis=1)\n",
    "\n",
    "    # Compute summary statistics\n",
    "    mean_disp = np.mean(displacements)\n",
    "    std_disp = np.std(displacements)\n",
    "    iqr_disp = np.percentile(displacements, 75) - np.percentile(displacements, 25)\n",
    "    cv_disp = std_disp / mean_disp if mean_disp > 0 else np.nan\n",
    "\n",
    "    hist_counts, _ = np.histogram(displacements, bins=40)\n",
    "    hist_probs = hist_counts / hist_counts.sum()\n",
    "    entropy_disp = entropy(hist_probs)\n",
    "\n",
    "    # Save stats to CSV\n",
    "    stats_df = pd.DataFrame([{\n",
    "        \"exp_memo\": exp_memo,\n",
    "        \"mean_displacement\": mean_disp,\n",
    "        \"std_displacement\": std_disp,\n",
    "        \"iqr_displacement\": iqr_disp,\n",
    "        \"cv_displacement\": cv_disp,\n",
    "        \"entropy\": entropy_disp\n",
    "    }])\n",
    "\n",
    "    stats_output_path = f\"{output_dir}{exp_memo}_X1_hat_displacement_stats.csv\"\n",
    "    stats_df.to_csv(stats_output_path, index=False)\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(displacements, bins=40, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Displacement (Euclidean Distance)\", fontsize=16)\n",
    "    plt.ylabel(\"Number of Cells\", fontsize=16)\n",
    "    plt.title(\"Distribution of Cell Displacements\\nX1_hat First vs Last Time Point\", fontsize=18)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = f\"{output_dir}{exp_memo}_X1_hat_displacement_distribution.png\"\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"ðŸ“Š Displacement plot saved at: {output_path}\")\n",
    "    print(f\"ðŸ“„ Stats CSV saved at: {stats_output_path}\")\n",
    "\n",
    "    # Save histogram data\n",
    "    hist_counts, bin_edges = np.histogram(displacements, bins=40)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    \n",
    "    hist_df = pd.DataFrame({\n",
    "        \"bin_center\": bin_centers,\n",
    "        \"count\": hist_counts\n",
    "    })\n",
    "    \n",
    "    hist_output_path = f\"{output_dir}{exp_memo}_X1_hat_displacement_histogram.csv\"\n",
    "    hist_df.to_csv(hist_output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0a66b-1438-422e-9b96-0bdd321bcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the distribution plot of the cell transitions distances\n",
    "\n",
    "source_t, target_t = 0, 4\n",
    "exp_memo = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "X1_hat_labels = plot_X1_hat_displacement_distribution(d_red=2, random_state=40,\n",
    "    exp_memo = exp_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd24d37-ef1b-48c8-9ebc-a37c5f02db69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Comparison of distributions across datasets (Histogram)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define path and file names\n",
    "output_dir = result_dir + 'output/'\n",
    "\n",
    "\n",
    "# Histogram filenames\n",
    "histogram_files = {\n",
    "    \"In vitro\": \"Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"862\": \"Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"887\": \"Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"PA3\": \"Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "\n",
    "}\n",
    "\n",
    "# Define colors (all cyan as placeholder)\n",
    "colors = {\n",
    "    \"In vitro\": \"gray\",\n",
    "    \"PA3\": \"gray\",\n",
    "    \"862\": \"gray\",\n",
    "    \"887\": \"gray\"\n",
    "}\n",
    "\n",
    "# Vertical lines for each\n",
    "vlines_dict = {\n",
    "    \"862\": (1.3, 2.0),\n",
    "    \"887\": (0.8, 1.2),\n",
    "    \"PA3\": (1.2, 2.4),\n",
    "    \"In vitro\": (3.2, 3.8)\n",
    "}\n",
    "\n",
    "# Load histogram data\n",
    "hist_data = {}\n",
    "for label, file in histogram_files.items():\n",
    "    fpath = os.path.join(output_dir, file)\n",
    "    if os.path.exists(fpath):\n",
    "        df = pd.read_csv(fpath)\n",
    "        hist_data[label] = df\n",
    "    else:\n",
    "        print(f\"[Warning] File not found: {fpath}\")\n",
    "\n",
    "# Plot histograms with vertical stacking\n",
    "n = len(hist_data)\n",
    "fig, axes = plt.subplots(nrows=n, figsize=(8, 4 * n), sharex=True, constrained_layout=True)\n",
    "\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (label, df) in zip(axes, hist_data.items()):\n",
    "    color = colors.get(label, \"gray\")\n",
    "    bin_width = df[\"bin_center\"][1] - df[\"bin_center\"][0]\n",
    "\n",
    "    ax.bar(df[\"bin_center\"], df[\"count\"], width=bin_width,\n",
    "           color=color, edgecolor=\"black\", alpha=0.8)\n",
    "    ax.set_ylabel(\"Count\", fontsize=20)\n",
    "    #ax.set_title(label, fontsize=20)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.tick_params(axis=\"y\", labelsize=20)\n",
    "    ax.set_xlim(left=0)\n",
    "\n",
    "    # Add vertical lines\n",
    "    #v1, v2 = vlines_dict.get(label, (None, None))\n",
    "    #if v1 is not None and v2 is not None:\n",
    "    #    ax.axvline(v1, color=\"black\", linestyle=\"--\", linewidth=1.5)\n",
    "    #    ax.axvline(v2, color=\"black\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"Displacement (Euclidean Distance)\", fontsize=20)\n",
    "#axes[-1].tick_params(axis=\"x\", labelsize=18)\n",
    "\n",
    "# Ensure each subplot shows x-ticks even with shared x-axis\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"x\", labelsize=20, which='both', labelbottom=True)\n",
    "\n",
    "# Save figure\n",
    "output_path = os.path.join(output_dir, \"side_by_side_displacement_histograms_without_vlines.pdf\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"[âœ“] Plot saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d22682-de08-48a0-ad7e-d670ea483adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison of distributions across datasets (KDE)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "\n",
    "\n",
    "#result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "#output_dir = result_dir + 'output/'\n",
    "\n",
    "# === Files ===\n",
    "histogram_files = {\n",
    "    \"In vitro\": \"Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"PA3\": \"Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"862\": \"Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "    \"887\": \"Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64_X1_hat_displacement_histogram.csv\",\n",
    "}\n",
    "\n",
    "colors = {key: \"gray\" for key in histogram_files.keys()}\n",
    "hist_data_kde = {}\n",
    "\n",
    "# === Load and smooth ===\n",
    "y_max_global = 0\n",
    "for label, file in histogram_files.items():\n",
    "    fpath = os.path.join(output_dir, file)\n",
    "    if os.path.exists(fpath):\n",
    "        df = pd.read_csv(fpath)\n",
    "        data = np.repeat(df[\"bin_center\"].values, df[\"count\"].astype(int))\n",
    "        kde = gaussian_kde(data, bw_method='scott')\n",
    "        x_eval = np.linspace(min(data), max(data), 1000)\n",
    "        density = kde(x_eval)\n",
    "        hist_data_kde[label] = (x_eval, density)\n",
    "        y_max_global = max(y_max_global, max(density))\n",
    "    else:\n",
    "        print(f\"[Warning] File not found: {fpath}\")\n",
    "\n",
    "# === Plot ===\n",
    "n = len(hist_data_kde)\n",
    "fig, axes = plt.subplots(nrows=n, figsize=(8, 4 * n), sharex=True, constrained_layout=True)\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (label, (x_eval, density)) in zip(axes, hist_data_kde.items()):\n",
    "    color = colors.get(label, \"gray\")\n",
    "    ax.plot(x_eval, density, color=color, lw=2)\n",
    "    ax.fill_between(x_eval, density, alpha=0.3, color=color)\n",
    "\n",
    "    # === Find valleys (local minima) ===\n",
    "    valleys, _ = find_peaks(-density)\n",
    "    x_vals = x_eval[valleys]\n",
    "    y_vals = density[valleys]\n",
    "\n",
    "    # Keep the 2 smallest minima in x-value\n",
    "    sorted_idx = np.argsort(x_vals)\n",
    "    top_two_idx = sorted_idx[:2]\n",
    "    x_vals_top2 = x_vals[top_two_idx]\n",
    "    y_vals_top2 = y_vals[top_two_idx]\n",
    "\n",
    "    # === Plot only local minima ===\n",
    "    ax.plot(x_vals_top2, y_vals_top2, \"go\", label=\"Local Minima\", color = 'blue', markersize=10)\n",
    "    for xv in x_vals_top2:\n",
    "        ax.axvline(x=xv, color=\"black\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "    # Print x-axis values of local minima\n",
    "    print(f\"{label} local minima x-values: {np.round(x_vals_top2, 3)}\")\n",
    "\n",
    "    ax.set_ylabel(\"Density\", fontsize=23)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.tick_params(axis=\"y\", labelsize=23)\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(top=y_max_global * 1.05)  # Ensure consistent y-limits across plots\n",
    "    ax.set_title(\"\", fontsize=23)\n",
    "\n",
    "axes[-1].set_xlabel(\"Displacement (Euclidean Distance)\", fontsize=24)\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"x\", labelsize=22, which='both', labelbottom=True)\n",
    "\n",
    "# === Save KDE figure ===\n",
    "output_path = os.path.join(output_dir, \"KDE_displacement_local_minima.pdf\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"[âœ“] KDE plot saved to: {output_path}\")\n",
    "\n",
    "# === Save legend separately ===\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='blue', linestyle='None', label='Local Minimum', markersize=5),\n",
    "    #Line2D([0], [0], color='black', linestyle='--', label='Threshold Cutoff')\n",
    "]\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(4.5, 2))\n",
    "ax_legend.axis(\"off\")\n",
    "legend = ax_legend.legend(handles=legend_elements, loc=\"center\", fontsize=14)\n",
    "legend_path = os.path.join(output_dir, \"KDE_legend_only.pdf\")\n",
    "fig_legend.savefig(legend_path, bbox_inches='tight')\n",
    "print(f\"[âœ“] Legend saved to: {legend_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16402ddb-4ae2-4a36-b8b9-f8ca9ee12fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colormaps\n",
    "\n",
    "\n",
    "def generate_static_cluster_plot_deviation_colormap(\n",
    "    source_t, target_t, optimal_k, start_i, index, reverse=False, intermediate_t=[1, 2, 3],\n",
    "    d_red=2, random_state=42, exp_memo='experiment'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a static plot of all snapshots from X1_trpts, colored by sub-trajectories with gradient coloring.\n",
    "    Also generates a separate legend figure and individual plots per subgroup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model parameters\n",
    "    filename = f\"{result_dir}{exp_memo}.pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Compute trajectory integration\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[source_t]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Step 1: Reduce real data and predictions to PCA space\n",
    "    last_day = mats[target_t]\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    X1_hat_last = X1_trpts[-1].astype(np.float32)\n",
    "    X1_hat_first = X1_trpts[0].astype(np.float32)\n",
    "    displacements = np.linalg.norm(X1_hat_last - X1_hat_first, axis=1)\n",
    "\n",
    "    # 862, 3 and 5 (ER)\n",
    "    # 887 0.8 and 1.3 (ER)\n",
    "    # BMC 1.0 and 3.0 (ER)\n",
    "    # Rinath 2.89 and 2.90 (ER)\n",
    "\n",
    "    # 862, 1.3 and 2 (R)\n",
    "    # 887 0.8 and 1.2 (R)\n",
    "    # BMC 1.2 and 2.4 (R)\n",
    "    # Rinath 3.2 and 3.8 (R)\n",
    "\n",
    "    ## R genes\n",
    "    #862 local minima x-values: [1.288 2.365]\n",
    "    #887 local minima x-values: [0.699 1.149]\n",
    "    #BMC local minima x-values: [1.145 1.937]\n",
    "    #In vitro local minima x-values: [3.017 3.853]\n",
    "\n",
    "\n",
    "    # Assign labels based on displacement\n",
    "    X1_hat_labels = np.full(displacements.shape, 'low', dtype=object)\n",
    "    X1_hat_labels[(displacements > 0.699) & (displacements <= 1.149)] = 'medium'\n",
    "    X1_hat_labels[displacements > 1.149] = 'high'\n",
    "\n",
    "    # Define colormaps per subgroup (avoid lightest tones by clipping range)\n",
    "    label_to_cmap = {\n",
    "        'low': colormaps['Oranges'],\n",
    "        'medium': colormaps['Purples'],\n",
    "        'high': colormaps['Greens']\n",
    "    }\n",
    "    cmap_clip = slice(75, 256)\n",
    "    color_range = np.linspace(0, 1, 256)[cmap_clip]  # consistent use\n",
    "\n",
    "    # Set file path for main plot\n",
    "    output_file = f\"{result_dir}{exp_memo}_static_celltypes_plot_deviation_colormap.png\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "\n",
    "    total_steps = len([i for i in range(len(X1_trpts)) if i % index == 0 and i >= start_i])\n",
    "\n",
    "    for label in np.unique(X1_hat_labels):\n",
    "        cmap = label_to_cmap[label]\n",
    "        idx = (X1_hat_labels == label)\n",
    "\n",
    "        for step_idx, i in enumerate(range(start_i, len(X1_trpts), index)):\n",
    "            if np.isnan(X1_trpts[i]).any():\n",
    "                continue\n",
    "            X1_hat_vis = X1_trpts[i]\n",
    "            norm_val = step_idx / max(total_steps - 1, 1)\n",
    "            color_idx = int(norm_val * (len(color_range) - 1))\n",
    "            color = cmap(color_range[color_idx])\n",
    "            ax.scatter(X1_hat_vis[idx, 0], X1_hat_vis[idx, 1], color=color, alpha=0.9, s=3, zorder=1)\n",
    "\n",
    "            if i > start_i:\n",
    "                prev_X1_hat_vis = X1_trpts[i - index]\n",
    "                prev_idx = idx\n",
    "                ax.plot([\n",
    "                    prev_X1_hat_vis[prev_idx, 0], X1_hat_vis[idx, 0]\n",
    "                ], [\n",
    "                    prev_X1_hat_vis[prev_idx, 1], X1_hat_vis[idx, 1]\n",
    "                ], color=color, alpha=0.6, linewidth=1.2, zorder=0)\n",
    "\n",
    "    for t in intermediate_t:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1],\n",
    "                   color='lightgray', alpha=0.7, s=10, zorder=10)\n",
    "\n",
    "    ax.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=32)\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Static cluster plot saved to {output_file}\")\n",
    "\n",
    "    # --- Save colormap legend with thinner bars side by side ---\n",
    "    fig_legend, axs = plt.subplots(1, len(label_to_cmap), figsize=(20, 1.5))\n",
    "    if len(label_to_cmap) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, (label, base_cmap) in zip(axs, label_to_cmap.items()):\n",
    "        # Create a new clipped colormap\n",
    "        clipped_cmap = base_cmap(np.linspace(0, 1, 256)[cmap_clip])\n",
    "        custom_cmap = plt.matplotlib.colors.ListedColormap(clipped_cmap)\n",
    "    \n",
    "        # Use that in the legend\n",
    "        gradient = np.linspace(0, 1, cmap_clip.stop - cmap_clip.start).reshape(1, -1)\n",
    "        ax.imshow(gradient, aspect='auto', cmap=custom_cmap, extent=[0, 1, 0, 0.03])\n",
    "        ax.set_title(f\"{label.capitalize()} phenotypic shift \\nPre-treatment â†’ Post-treatment\", fontsize=20)\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "    legend_path = output_file.replace('.png', '_legend.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(legend_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Legend saved to {legend_path}\")\n",
    "\n",
    "    # --- Additional per-label plots ---\n",
    "    for current_label in np.unique(X1_hat_labels):\n",
    "        fig_lbl, ax_lbl = plt.subplots(figsize=(8, 6))\n",
    "        cmap = label_to_cmap[current_label]\n",
    "        idx = (X1_hat_labels == current_label)\n",
    "\n",
    "        for step_idx, i in enumerate(range(start_i, len(X1_trpts), index)):\n",
    "            if np.isnan(X1_trpts[i]).any():\n",
    "                continue\n",
    "            X1_hat_vis = X1_trpts[i]\n",
    "            norm_val = step_idx / max(total_steps - 1, 1)\n",
    "            color_idx = int(norm_val * (len(color_range) - 1))\n",
    "            color = cmap(color_range[color_idx])\n",
    "            ax_lbl.scatter(X1_hat_vis[idx, 0], X1_hat_vis[idx, 1],\n",
    "                           color=color, alpha=0.8, s=3, zorder=2)\n",
    "\n",
    "            if i > start_i:\n",
    "                prev_X1_hat_vis = X1_trpts[i - index]\n",
    "                prev_idx = idx\n",
    "                ax_lbl.plot([\n",
    "                    prev_X1_hat_vis[prev_idx, 0], X1_hat_vis[idx, 0]\n",
    "                ], [\n",
    "                    prev_X1_hat_vis[prev_idx, 1], X1_hat_vis[idx, 1]\n",
    "                ], color=color, alpha=0.6, linewidth=1.2)\n",
    "\n",
    "        ax_lbl.scatter(X1_vis[:, 0], X1_vis[:, 1], color='lightgray', alpha=0.7, s=10, zorder=1)\n",
    "        ax_lbl.scatter(X2_vis[:, 0], X2_vis[:, 1], color='lightgray', alpha=0.7, s=10, zorder=1)\n",
    "\n",
    "        ax_lbl.set_xlabel(\"PC 1\", fontsize=32)\n",
    "        ax_lbl.set_ylabel(\"PC 2\", fontsize=32)\n",
    "        ax_lbl.tick_params(axis='both', labelsize=32)\n",
    "        ax_lbl.set_title(\"\", fontsize=32)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_file.replace('.png', f'_label_{current_label}.png')}\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_lbl)\n",
    "\n",
    "    return X1_hat_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc842df4-67a1-4e19-8e47-6c597c411173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Clinical data (deviated cell vs non-deviated cells)\n",
    "\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "start_i = 0\n",
    "index = 1\n",
    "exp_memo = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "X1_hat_labels = generate_static_cluster_plot_deviation_colormap(\n",
    "    source_t, target_t, optimal_k, start_i, index, reverse = False, intermediate_t = [], d_red=2, random_state=42,\n",
    "    exp_memo = exp_memo)\n",
    "\n",
    "# Convert the labels into a DataFrame for saving\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"Cell_Index\": np.arange(len(X1_hat_labels)),  # Assuming each cell has an index\n",
    "    \"Cluster_Label\": X1_hat_labels\n",
    "})\n",
    "\n",
    "# Define the filename\n",
    "cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_deviation.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "df_clusters.to_csv(cluster_save_path, index=False)\n",
    "print(f\"Cluster labels saved to {cluster_save_path}\")\n",
    "\n",
    "# Return the saved filename for future use\n",
    "cluster_save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27aeca-abcb-4e6d-aa7d-947fe701c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all cell labels for each time point\n",
    "\n",
    "df_cls_indexed = df_cls.set_index(\"id\")\n",
    "\n",
    "cell_ids_by_day = {}\n",
    "for c in cls:\n",
    "    idx = df_cls['day'] == c\n",
    "    cell_ids_by_day[c] = df_cls_indexed.index[idx].tolist()  # extract index values for each day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e5f30-f664-4cc7-a8ab-19b2c346ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code without celltype annoation\n",
    "\n",
    "## Save the labels at a specific time point for each class of subtrajectory (this is an example for time 0)\n",
    "\n",
    "# Step 1: Load saved file\n",
    "df_loaded = pd.read_csv(cluster_save_path)\n",
    "\n",
    "# Step 2: Verify lengths match\n",
    "if len(df_loaded) != len(cell_ids_by_day[0]):\n",
    "    raise ValueError(\"âŒ Length mismatch: Cannot assign new cell IDs.\")\n",
    "\n",
    "# Step 3: Replace index or create new column\n",
    "df_loaded[\"Cell_ID\"] = cell_ids_by_day[0]  # Add as a column\n",
    "df_loaded = df_loaded[[\"Cell_ID\"] + [col for col in df_loaded.columns if col != \"Cell_ID\"]]  # Optional: move to front\n",
    "df_loaded = df_loaded.drop(columns=[\"Cell_Index\"])\n",
    "\n",
    "# Step 4: Save to new file\n",
    "new_path = cluster_save_path.replace(\".csv\", \"_with_cell_ids.csv\")\n",
    "df_loaded.to_csv(new_path, index=False)\n",
    "print(f\"âœ… Updated file saved to: {new_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146dabc6-4f8d-42ea-9e61-58c3706dc511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Old code\n",
    "# Step 1: Load saved file\n",
    "df_loaded = pd.read_csv(cluster_save_path)\n",
    "\n",
    "# Step 2: Verify lengths match\n",
    "if len(df_loaded) != len(cell_ids_by_day[0]):\n",
    "    raise ValueError(\"âŒ Length mismatch: Cannot assign new cell IDs.\")\n",
    "\n",
    "# Step 3: Replace index or create new column\n",
    "df_loaded[\"Cell_ID\"] = cell_ids_by_day[0]  # Add as a column\n",
    "df_loaded = df_loaded[[\"Cell_ID\"] + [col for col in df_loaded.columns if col != \"Cell_ID\"]]  # Optional: move to front\n",
    "df_loaded = df_loaded.drop(columns=[\"Cell_Index\"])\n",
    "\n",
    "# Step 4: Save to new file\n",
    "new_path = cluster_save_path.replace(\".csv\", \"_with_cell_ids.csv\")\n",
    "df_loaded.to_csv(new_path, index=False)\n",
    "print(f\"âœ… Updated file saved to: {new_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed35de-c555-4e9c-8736-11f90b424320",
   "metadata": {},
   "source": [
    "## Downstream analysis for single gene dynamcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2efbd-a69e-4583-b76c-f43904b1e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46bd4b-5c0f-4f32-bcc7-a900dec1de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_W(filename):\n",
    "    with open(filename, \"rb\") as fr:\n",
    "        W, b, p = pk.load(fr)\n",
    "\n",
    "    W = [tf.Variable(w,  dtype=tf.float32) for w in W]\n",
    "    b = [tf.Variable(b_,  dtype=tf.float32) for b_ in b]\n",
    "\n",
    "    return W, b, p\n",
    "\n",
    "def v(x, t, W, b):   # neural newtork for time-dependent vectorfield\n",
    "    num_layers = len(W)\n",
    "    activation_ftn = tf.nn.tanh\n",
    "        \n",
    "    h = tf.concat([x, t*tf.ones([x.shape[0], 1], dtype=tf.float32)], axis=1)\n",
    "    for l in range(0,num_layers-1):\n",
    "        h = activation_ftn(tf.add(tf.matmul(h, W[l]), b[l]))\n",
    "    out=tf.add(tf.matmul(h, W[-1]), b[-1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def time_integration(x0, T, dt):\n",
    "    x = tf.constant(x0, dtype=tf.float32)\n",
    "    xs = [x0]\n",
    "    for i in range(int(T/dt)):\n",
    "        vv = v(x, dt*i, W, b)\n",
    "        x += dt * vv\n",
    "        xs.append(x.numpy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def gene_dynamics_whole_saveonly(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day2]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=40)\n",
    "    kmeans.fit(last_day_reduced)\n",
    "    last_day_labels = kmeans.labels_\n",
    "    \n",
    "    #X1_hat_last_reduced = pca.transform(X1_hat_last)\n",
    "\n",
    "    X1_hat_last = X1_trpts[-1].astype(np.float32) \n",
    "    X1_hat_labels = kmeans.predict(X1_hat_last)\n",
    "    \n",
    "\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(last_day_labels)\n",
    "    print(f\"Number of unique labels in last_day_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(last_day_labels, red_colors)\n",
    "\n",
    "    #mask = last_day_labels == 0\n",
    "    \n",
    "    # Visualization in the original space \n",
    "    img_src = f\"{output_dir}{exp_memo}-movie-{d_red}D-{gene_of_interest}-dynamics-trajectory-source-only-nonormalized-mix-40000-034.gif\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    for i in indices:\n",
    "        if i % index == 0 and i <= max_i:\n",
    "            X1_trpt = X1_trpts[i]\n",
    "            if np.isnan(X1_trpt).any():\n",
    "                break\n",
    "            X1_hat = pca.inverse_transform(X1_trpt)\n",
    "            X1_hat_vis = reducer.transform(X1_hat)\n",
    "            #mask_2 = X1_hat_labels == 0\n",
    "            #X1_hat_vis = X1_hat_vis[mask_2]\n",
    "\n",
    "            # Plot all points in X1_hat_vis with colormap based on precomputed normalized gene expression values\n",
    "            gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "            all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update the list to exclude the used values\n",
    "            im = ax.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], c=gene_expression_values, cmap='viridis', alpha=1.0, s=0.5, zorder=10, vmin=vmin, vmax=vmax)\n",
    "            #ax.scatter(X1_intermediate_vis[:, 0], X1_intermediate_vis[:, 1], color=colors[t], alpha=0.5, s=0.5, zorder=5)\n",
    "            ax.scatter(vis_all_days[:, 0], vis_all_days[:, 1], color='lightgray', alpha=0.3, s=0.5, zorder=1)\n",
    "    \n",
    "            ttl = ax.text(0.5, 1.05, \"t = %.3f\" % (physical_dt*i), bbox={'facecolor': 'w', 'alpha': 0.5, 'pad': 5}, transform=ax.transAxes, ha=\"center\")\n",
    "            ims.append([im, ttl])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=200)\n",
    "    writergif = animation.PillowWriter(fps=3)\n",
    "    ani.save(img_src, writer=writergif)\n",
    "    # Close the figure to free up memory\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_file = f\"{output_dir}/_average_gene_expression_{gene_of_interest}.png\"\n",
    "\n",
    "    \n",
    "    # Plot averaged gene expressions with confidence intervals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the averaged gene expressions as a line\n",
    "    plt.plot(\n",
    "        extended_indices,\n",
    "        avg_gene_expressions,\n",
    "        label='Average Gene Expression',\n",
    "        color='green',\n",
    "        linestyle='-',  # Use a line instead of dots\n",
    "    )\n",
    "    \n",
    "    # Fill the confidence intervals\n",
    "    plt.fill_between(\n",
    "        extended_indices,\n",
    "        np.array(avg_gene_expressions) - np.array(ci_gene_expressions),\n",
    "        np.array(avg_gene_expressions) + np.array(ci_gene_expressions),\n",
    "        alpha=0.2,\n",
    "        color='lightgreen',\n",
    "        label='95% CI'\n",
    "    )\n",
    "    \n",
    "    # Ensure rescaled_indices and all_avg_expressions have the same length\n",
    "    assert len(rescaled_indices) == len(all_avg_expressions), (\n",
    "        f\"Length mismatch: rescaled_indices ({len(rescaled_indices)}) != all_avg_expressions ({len(all_avg_expressions)})\"\n",
    "    )\n",
    "    \n",
    "    # Plot the intermediate and boundary time points\n",
    "    plt.errorbar(\n",
    "        rescaled_indices,  # Use rescaled indices for the x-axis\n",
    "        all_avg_expressions,\n",
    "        yerr=all_ci_expressions,\n",
    "        fmt='o',\n",
    "        color='blue',\n",
    "        label='Discrete Points'\n",
    "    )\n",
    "    \n",
    "    # Update the x-axis ticks and labels\n",
    "    plt.xticks(\n",
    "        ticks=rescaled_indices,  # Tick positions based on rescaled indices\n",
    "        labels=combined_indices,  # Relabel using combined_indices\n",
    "        rotation=0,  # Optional: Rotate labels for better visibility\n",
    "        fontsize=10    # Adjust font size for readability\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Time Point (Day)')\n",
    "    plt.ylabel('Gene Expression')\n",
    "    plt.title(f'Average {gene_of_interest} Expression Over Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')  # Save with high resolution\n",
    "    \n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # (2) Plot averaged gene expression and confidence intervals for subgroups at each time point based on X1_hat_labels\n",
    "    # Perform KMeans clustering with the optimal number of clusters\n",
    "    X1_hat_last = X1_trpts[-1].astype(np.float32)\n",
    "    X1_hat_labels = kmeans.predict(X1_hat_last)\n",
    "    \n",
    "    # Initialize dictionaries to store subgroup averages and confidence intervals\n",
    "    subgroup_avg_gene_expressions = {label: [] for label in np.unique(X1_hat_labels)}\n",
    "    subgroup_ci_gene_expressions = {label: [] for label in np.unique(X1_hat_labels)}\n",
    "    \n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Compute averages and confidence intervals for subgroups\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]\n",
    "    \n",
    "        # Compute subgroup-specific averages and confidence intervals\n",
    "        for label in np.unique(X1_hat_labels):\n",
    "            subgroup_values = gene_expression_values[X1_hat_labels == label]\n",
    "            subgroup_avg_gene_expressions[label].append(np.mean(subgroup_values))\n",
    "            ci = stats.sem(subgroup_values) * stats.t.ppf((1 + 0.95) / 2., len(subgroup_values) - 1)\n",
    "            subgroup_ci_gene_expressions[label].append(ci)\n",
    "\n",
    "    \n",
    "    # Define the filename for saving the subgroup plot\n",
    "    subgroup_output_file = f\"{output_dir}/_average_gene_expression_{gene_of_interest}_subgroups.png\"\n",
    "    \n",
    "    # Plot subgroup averages and confidence intervals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label in np.unique(X1_hat_labels):\n",
    "        plt.plot(\n",
    "            extended_indices,\n",
    "            subgroup_avg_gene_expressions[label],\n",
    "            label=f'Subgroup {label} Average',\n",
    "            linestyle='-'\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            extended_indices,\n",
    "            np.array(subgroup_avg_gene_expressions[label]) - np.array(subgroup_ci_gene_expressions[label]),\n",
    "            np.array(subgroup_avg_gene_expressions[label]) + np.array(subgroup_ci_gene_expressions[label]),\n",
    "            alpha=0.2,\n",
    "            label=f'Subgroup {label} 95% CI'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Update the x-axis ticks and labels\n",
    "    plt.xticks(\n",
    "        ticks=rescaled_indices,  # Tick positions based on rescaled indices\n",
    "        labels=combined_indices,  # Relabel using combined_indices\n",
    "        rotation=0,  # Optional: Rotate labels for better visibility\n",
    "        fontsize=10  # Adjust font size for readability\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Time Point (Day)')\n",
    "    plt.ylabel('Gene Expression')\n",
    "    plt.title(f'Average {gene_of_interest} Expression Over Time by Subgroup')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the subgroup plot\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Subgroup plot saved at: {subgroup_output_file}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31253020-2946-4e55-9537-0556de620e54",
   "metadata": {},
   "source": [
    "## Downstream Analysis of Average Gene Dynamics for each Gene with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057fd3d-0935-4cbb-8af7-54dd4ad4ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_W(filename):\n",
    "    with open(filename, \"rb\") as fr:\n",
    "        W, b, p = pk.load(fr)\n",
    "\n",
    "    W = [tf.Variable(w,  dtype=tf.float32) for w in W]\n",
    "    b = [tf.Variable(b_,  dtype=tf.float32) for b_ in b]\n",
    "\n",
    "    return W, b, p\n",
    "\n",
    "def v(x, t, W, b):   # neural newtork for time-dependent vectorfield\n",
    "    num_layers = len(W)\n",
    "    activation_ftn = tf.nn.tanh\n",
    "        \n",
    "    h = tf.concat([x, t*tf.ones([x.shape[0], 1], dtype=tf.float32)], axis=1)\n",
    "    for l in range(0,num_layers-1):\n",
    "        h = activation_ftn(tf.add(tf.matmul(h, W[l]), b[l]))\n",
    "    out=tf.add(tf.matmul(h, W[-1]), b[-1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def time_integration(x0, T, dt):\n",
    "    x = tf.constant(x0, dtype=tf.float32)\n",
    "    xs = [x0]\n",
    "    for i in range(int(T/dt)):\n",
    "        vv = v(x, dt*i, W, b)\n",
    "        x += dt * vv\n",
    "        xs.append(x.numpy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def Average_gene_dynamics_whole_saveonly(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day1]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "\n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_file = f\"{output_dir}/_average_gene_expression_{gene_of_interest}.png\"\n",
    "\n",
    "    \n",
    "    # Plot averaged gene expressions with confidence intervals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the averaged gene expressions as a line\n",
    "    plt.plot(\n",
    "        extended_indices,\n",
    "        avg_gene_expressions,\n",
    "        label='Average Gene Expression',\n",
    "        color='orange',\n",
    "        linestyle='-',  # Use a line instead of dots\n",
    "    )\n",
    "    \n",
    "    # Fill the confidence intervals\n",
    "    plt.fill_between(\n",
    "        extended_indices,\n",
    "        np.array(avg_gene_expressions) - np.array(ci_gene_expressions),\n",
    "        np.array(avg_gene_expressions) + np.array(ci_gene_expressions),\n",
    "        alpha=0.2,\n",
    "        color='lightsalmon',\n",
    "        label='95% CI'\n",
    "    )\n",
    "    \n",
    "    # Ensure rescaled_indices and all_avg_expressions have the same length\n",
    "    assert len(rescaled_indices) == len(all_avg_expressions), (\n",
    "        f\"Length mismatch: rescaled_indices ({len(rescaled_indices)}) != all_avg_expressions ({len(all_avg_expressions)})\"\n",
    "    )\n",
    "    \n",
    "    # Plot the intermediate and boundary time points\n",
    "    plt.errorbar(\n",
    "        rescaled_indices,  # Use rescaled indices for the x-axis\n",
    "        all_avg_expressions,\n",
    "        yerr=all_ci_expressions,\n",
    "        fmt='o',\n",
    "        color='blue',\n",
    "        label='Discrete Points'\n",
    "    )\n",
    "    \n",
    "    # Update the x-axis ticks and labels\n",
    "    plt.xticks(\n",
    "        ticks=rescaled_indices,  # Tick positions based on rescaled indices\n",
    "        labels=combined_indices,  # Relabel using combined_indices\n",
    "        rotation=0,  # Optional: Rotate labels for better visibility\n",
    "        fontsize=32    # Adjust font size for readability\n",
    "    )\n",
    "\n",
    "    plt.yticks(fontsize=32)  # Increase y-axis tick font size\n",
    "\n",
    "    \n",
    "    plt.xlabel('Time', fontsize=32)\n",
    "    plt.ylabel('Gene Expression', fontsize=32)\n",
    "    plt.title(f'Average {gene_of_interest} Dynamics', fontsize=32)\n",
    "    #plt.legend(fontsize=16)  # Adjust font size as needed\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')  # Save with high resolution\n",
    "    \n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # **Save Separate Legend**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))\n",
    "    ax_legend.axis(\"off\")\n",
    "\n",
    "    legend_elements = [\n",
    "        mlines.Line2D([], [], color='orange', linestyle='-', linewidth=3, label='Average Gene Dynamics'),\n",
    "        mpatches.Patch(color='lightsalmon', alpha=0.7, label='95% Confidence Interval'),\n",
    "        mlines.Line2D([], [], color='blue', marker='o', linestyle='-', markersize=8, linewidth=2, label='Data Points')\n",
    "    ]\n",
    "\n",
    "    ax_legend.legend(handles=legend_elements, loc=\"center\", fontsize=32, title=\"\", title_fontsize=32, \n",
    "                     frameon=True, ncol=len(legend_elements), handletextpad=2, columnspacing=2)\n",
    "\n",
    "    legend_output_file = output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Subgroup trajectory plot saved at: {output_file}\")\n",
    "    print(f\"Legend plot saved separately at: {legend_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7e6c7-817a-4d15-960f-0aa9569e7dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Average gene dynamics for each single gene plot (Stem Cell data)\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,2,3]\n",
    "#intermediate_t = [2]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a728d8-6644-41cc-8c06-0a889c787e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf (Stem Cell data)\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/_average_gene_expression_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/_average_gene_expression.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19b418-b59c-49ce-b6bb-c1c23c31c1ed",
   "metadata": {},
   "source": [
    "## Downstream Analysis for Average gene dynamics on each subtrajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22381b4-85f3-40f4-801e-015bcd28f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for Stem cell data (Five time points: Time [0, 1, 2, 3, 4])\n",
    "\n",
    "## Subtrajectroies with_violin_plot\n",
    "\n",
    "def Average_gene_dynamics_whole_saveonly_with_violin_plot_sample_3(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day2]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=40)\n",
    "    kmeans.fit(last_day_reduced)\n",
    "    last_day_labels = kmeans.labels_\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "\n",
    "    #mask = last_day_labels == 0\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # (1) Perform clustering on the last day's cell states from `mats`\n",
    "    last_day = mats[day2]\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=40)\n",
    "    kmeans.fit(last_day_reduced)\n",
    "    last_day_labels = kmeans.labels_\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define colors for subgroups\n",
    "    subgroup_colors = ['red', 'blue', '#ffe119', '#f58231', '#3cb44b']\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subgroup_colors[i % len(subgroup_colors)] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename\n",
    "    subgroup_output_file = f\"{output_dir}/subtrajectories_violin_plots_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) Initialize Storage for Mean and CI\n",
    "    subgroup_avg_gene_expressions = {label: [] for label in unique_labels}\n",
    "    subgroup_ci_gene_expressions = {label: [] for label in unique_labels}\n",
    "    \n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized.copy()\n",
    "    \n",
    "    # (3) Compute Mean & Confidence Intervals\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:  # Apply truncation\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract gene expression values\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]\n",
    "    \n",
    "        # Compute subgroup averages & CI\n",
    "        for label in unique_labels:\n",
    "            mask = (X1_hat_labels == label)  # Use labels **only from step 1**\n",
    "            subgroup_values = np.array(gene_expression_values)[mask]\n",
    "    \n",
    "            if len(subgroup_values) > 0:\n",
    "                subgroup_avg_gene_expressions[label].append(np.mean(subgroup_values))\n",
    "                ci = stats.sem(subgroup_values) * stats.t.ppf((1 + 0.95) / 2., len(subgroup_values) - 1)\n",
    "                subgroup_ci_gene_expressions[label].append(ci)\n",
    "            else:\n",
    "                subgroup_avg_gene_expressions[label].append(np.nan)\n",
    "                subgroup_ci_gene_expressions[label].append(np.nan)\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    # (4) **Plot**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # **Get x-axis positions for the line plot (scale to [0, 4])**\n",
    "    num_points = len(next(iter(subgroup_avg_gene_expressions.values())))  # Number of time points\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Ensure correct x-spacing for trajectories\n",
    "    \n",
    "    # **Plot Predicted Trajectories & Confidence Intervals**\n",
    "    subgroup_legend_handles = []  # Store for separate legend\n",
    "\n",
    "\n",
    "    # **Plot Subgroup Averages & Confidence Intervals**\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # **Plot the Mean Trajectory Line**\n",
    "        line, = ax1.plot(\n",
    "            x_positions, subgroup_avg_gene_expressions[label], zorder=10,\n",
    "            linestyle='-', color=subgroup_color_map[label], linewidth=2,\n",
    "            label=f'Predicted Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Plot the Confidence Interval (Shaded Region)**\n",
    "        ax1.fill_between(\n",
    "            x_positions,\n",
    "            np.array(subgroup_avg_gene_expressions[label]) - np.array(subgroup_ci_gene_expressions[label]),\n",
    "            np.array(subgroup_avg_gene_expressions[label]) + np.array(subgroup_ci_gene_expressions[label]),\n",
    "            alpha=0.2, zorder=5, color=subgroup_color_map[label],\n",
    "            label=f'95% CI of Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Legend entry for Mean + Confidence Interval**\n",
    "        ci_patch = mpatches.Patch(\n",
    "            color=subgroup_color_map[label], alpha=0.2, label=f'95% CI of Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Store in Legend Handles**\n",
    "        subgroup_legend_handles.append(ci_patch)\n",
    "        subgroup_legend_handles.append(line)\n",
    "        \n",
    "    # (5) **Ensure Violin Plots are at `[0, 2, 4]`**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    # **Manually set violin plot positions to `[0, 2, 4]`**\n",
    "    violin_x_positions = np.array([0, 1, 2, 3, 4])  # Explicitly define positions\n",
    "    violin_colors = [\"black\", \"gray\", \"black\", \"gray\", \"black\"]  # Set distinct colors\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots One-by-One to Force Correct Positioning**\n",
    "    for i, (x_pos, data, color) in enumerate(zip(violin_x_positions, violin_data, violin_colors)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  # Must be wrapped in a list to avoid merging violins\n",
    "            ax=ax1,\n",
    "            inner=None,\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=color,  # âœ… Assign distinct colors\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  # Move to correct x-location\n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  # âœ… Extend range\n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 1, 2, 3, 4])  # âœ… Force labels at `[0, 2, 4]`\n",
    "    ax1.set_xticklabels([0, 1, 2, 3, 4], fontsize=35)\n",
    "    ax1.tick_params(axis='y', labelsize=35)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=35)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=35)\n",
    "    ax1.set_title(f'Subtrajectory {gene_of_interest} Expression', fontsize=34)\n",
    "    \n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (VERTICAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(4, 8))  # Tall aspect ratio for vertical layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = subgroup_legend_handles + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=18, title=\"Trajectories & Violin Plots\",\n",
    "        title_fontsize=18, ncol=1, frameon=True, handletextpad=1.5, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Subgroup trajectory plot saved at: {subgroup_output_file}\")\n",
    "    print(f\"Legend plot saved separately at: {legend_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14934b2-a129-40c6-99a3-3f33d701012b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for Stem cell data\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "## HR+ cancer gene markders\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,2,3]\n",
    "#intermediate_t = [0]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_with_violin_plot_sample_3(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a19cd-72fe-487a-99ed-c6c948575cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for Stem Cell data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/subtrajectories_violin_plots_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/subtrajectories_violin_plots.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd0641-e291-43b9-adcb-8f7802ef6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for EMT data (Three time points: Time [0, 2, 4])\n",
    "\n",
    "## Subtrajectroies with_violin_plot\n",
    "\n",
    "def Average_gene_dynamics_whole_saveonly_with_violin_plot_sample1(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day1]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X2_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "\n",
    "    #mask = last_day_labels == 0\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "   \n",
    "    \n",
    "    # (1) Perform clustering on the last day's cell states from `mats`\n",
    "    last_day = mats[day1]\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "\n",
    "    \n",
    "    # Define colors for subgroups\n",
    "    subgroup_colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subgroup_colors[i % len(subgroup_colors)] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename\n",
    "    subgroup_output_file = f\"{output_dir}/subtrajectories_violin_plots_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) Initialize Storage for Mean and CI\n",
    "    subgroup_avg_gene_expressions = {label: [] for label in unique_labels}\n",
    "    subgroup_ci_gene_expressions = {label: [] for label in unique_labels}\n",
    "    \n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized.copy()\n",
    "    \n",
    "    # (3) Compute Mean & Confidence Intervals\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:  # Apply truncation\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract gene expression values\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]\n",
    "    \n",
    "        # Compute subgroup averages & CI\n",
    "        for label in unique_labels:\n",
    "            mask = (X1_hat_labels == label)  # Use labels **only from step 1**\n",
    "            subgroup_values = np.array(gene_expression_values)[mask]\n",
    "    \n",
    "            if len(subgroup_values) > 0:\n",
    "                subgroup_avg_gene_expressions[label].append(np.mean(subgroup_values))\n",
    "                ci = stats.sem(subgroup_values) * stats.t.ppf((1 + 0.95) / 2., len(subgroup_values) - 1)\n",
    "                subgroup_ci_gene_expressions[label].append(ci)\n",
    "            else:\n",
    "                subgroup_avg_gene_expressions[label].append(np.nan)\n",
    "                subgroup_ci_gene_expressions[label].append(np.nan)\n",
    "    \n",
    "\n",
    "    \n",
    "    # (4) **Plot**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # **Get x-axis positions for the line plot (scale to [0, 4])**\n",
    "    num_points = len(next(iter(subgroup_avg_gene_expressions.values())))  # Number of time points\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Ensure correct x-spacing for trajectories\n",
    "    \n",
    "    # **Plot Predicted Trajectories & Confidence Intervals**\n",
    "    subgroup_legend_handles = []  # Store for separate legend\n",
    "\n",
    "\n",
    "    # **Plot Subgroup Averages & Confidence Intervals**\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # **Plot the Mean Trajectory Line**\n",
    "        line, = ax1.plot(\n",
    "            x_positions, subgroup_avg_gene_expressions[label], zorder=10,\n",
    "            linestyle='-', color=subgroup_color_map[label], linewidth=2,\n",
    "            label=f'Predicted Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Plot the Confidence Interval (Shaded Region)**\n",
    "        ax1.fill_between(\n",
    "            x_positions,\n",
    "            np.array(subgroup_avg_gene_expressions[label]) - np.array(subgroup_ci_gene_expressions[label]),\n",
    "            np.array(subgroup_avg_gene_expressions[label]) + np.array(subgroup_ci_gene_expressions[label]),\n",
    "            alpha=0.2, zorder=5, color=subgroup_color_map[label],\n",
    "            label=f'95% CI of Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Legend entry for Mean + Confidence Interval**\n",
    "        ci_patch = mpatches.Patch(\n",
    "            color=subgroup_color_map[label], alpha=0.2, label=f'95% CI of Trajectory {i+1}'\n",
    "        )\n",
    "    \n",
    "        # **Store in Legend Handles**\n",
    "        subgroup_legend_handles.append(line)\n",
    "        subgroup_legend_handles.append(ci_patch)\n",
    "\n",
    "        \n",
    "    # (5) **Ensure Violin Plots are at `[0, 2, 4]`**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    # **Manually set violin plot positions to `[0, 2, 4]`**\n",
    "    violin_x_positions = np.array([0, 2, 4])  # Explicitly define positions\n",
    "    violin_colors = [\"black\", \"gray\", \"black\", \"gray\", \"black\"]  # Set distinct colors\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots One-by-One to Force Correct Positioning**\n",
    "    for i, (x_pos, data, color) in enumerate(zip(violin_x_positions, violin_data, violin_colors)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  # Must be wrapped in a list to avoid merging violins\n",
    "            ax=ax1,\n",
    "            inner=None,\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=color,  # âœ… Assign distinct colors\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  # Move to correct x-location\n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  # âœ… Extend range\n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 2, 4])  # âœ… Force labels at `[0, 2, 4]`\n",
    "    ax1.set_xticklabels([0, 2, 4], fontsize=35)\n",
    "    ax1.tick_params(axis='y', labelsize=35)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=35)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=35)\n",
    "    ax1.set_title(f'Subtrajectory {gene_of_interest} Expression', fontsize=35)\n",
    "    \n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (VERTICAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(8, 2))  # Tall aspect ratio for vertical layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = subgroup_legend_handles + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=18, title=\"\",\n",
    "        title_fontsize=18, ncol=6, frameon=True, handletextpad=1.5, columnspacing=1.5\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Subgroup trajectory plot saved at: {subgroup_output_file}\")\n",
    "    print(f\"Legend plot saved separately at: {legend_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87145164-2bd8-4db7-be6e-fff3b3dd6fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for EMT data\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "## HR+ cancer gene markders\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]\n",
    "#intermediate_t = [0]\n",
    "\n",
    "d_red= 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_with_violin_plot_sample1(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f0cf4-b20d-4e9b-bb41-7a32dd945be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for EMT data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/subtrajectories_violin_plots_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/subtrajectories_violin_plots.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab550b-8770-436e-87b9-be0c99ed79b2",
   "metadata": {},
   "source": [
    "## Analysis of divergence of convergence of subtrajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bf418-fb76-4664-8484-8a9a369b16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dynamics of p-values and fold change arross subtrajectories (with csv files and visulization results) \n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def Compute_and_Plot_FoldChange_MeanDiff_PValues(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                                 intermediate_t=[1], d_red=2, random_state=42, exp_memo='2'):\n",
    "\n",
    "    # Load PCA and clustering results\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA transformation\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red if dim_red_method == 'EMT_PCA' else \"pca_%d.pkl\"\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "\n",
    "\n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "    # Extract gene expression values\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    all_gene_expression_values_normalized_X1 = np.concatenate(\n",
    "        [pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i]\n",
    "    )\n",
    "\n",
    "    # Initialize lists for storing results\n",
    "    fold_change_results = []\n",
    "    mean_diff_results = []\n",
    "    p_value_results = []\n",
    "\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "    eps = 1e-6  # Small constant to prevent division by zero\n",
    "\n",
    "    # Compute mean difference, fold-change, and p-values over time\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]\n",
    "\n",
    "        for label1 in unique_labels:\n",
    "            for label2 in unique_labels:\n",
    "                if label1 >= label2:\n",
    "                    continue\n",
    "\n",
    "                mask1 = (X1_hat_labels == label1)\n",
    "                mask2 = (X1_hat_labels == label2)\n",
    "\n",
    "                expr_values1 = np.array(gene_expression_values)[mask1]\n",
    "                expr_values2 = np.array(gene_expression_values)[mask2]\n",
    "\n",
    "                if len(expr_values1) > 1 and len(expr_values2) > 1:\n",
    "                    mean1, mean2 = np.mean(expr_values1), np.mean(expr_values2)\n",
    "                    \n",
    "                    # Compute Mean Difference\n",
    "                    mean_diff = mean1 - mean2\n",
    "                    \n",
    "                    # Compute Fold Change\n",
    "                    if mean1 <= 0 or mean2 <= 0 or np.isnan(mean1) or np.isnan(mean2):\n",
    "                        log2_fc = np.nan\n",
    "                    else:\n",
    "                        log2_fc = np.log2(mean1 / mean2)\n",
    "\n",
    "                    # Compute p-value\n",
    "                    t_stat, p_val = stats.ttest_ind(expr_values1, expr_values2, equal_var=False, nan_policy='omit')\n",
    "                else:\n",
    "                    mean_diff = np.nan\n",
    "                    log2_fc = np.nan\n",
    "                    p_val = np.nan\n",
    "\n",
    "                fold_change_results.append({\n",
    "                    \"Time\": time_idx,\n",
    "                    \"Cluster 1\": label1,\n",
    "                    \"Cluster 2\": label2,\n",
    "                    \"Log2 Fold Change\": log2_fc\n",
    "                })\n",
    "\n",
    "                mean_diff_results.append({\n",
    "                    \"Time\": time_idx,\n",
    "                    \"Cluster 1\": label1,\n",
    "                    \"Cluster 2\": label2,\n",
    "                    \"Mean Difference\": mean_diff\n",
    "                })\n",
    "\n",
    "                p_value_results.append({\n",
    "                    \"Time\": time_idx,\n",
    "                    \"Cluster 1\": label1,\n",
    "                    \"Cluster 2\": label2,\n",
    "                    \"p-value\": p_val\n",
    "                })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_fc = pd.DataFrame(fold_change_results)\n",
    "    df_md = pd.DataFrame(mean_diff_results)\n",
    "    df_pval = pd.DataFrame(p_value_results)\n",
    "\n",
    "    # Save as CSV files\n",
    "    output_dir = os.path.join(result_dir, \"output\", exp_memo)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Function to propagate NaNs forward **only if the last finite value was negative**\n",
    "    def propagate_nans_for_negative_fc(df, column=\"Log2 Fold Change\"):\n",
    "        \"\"\"Once NaN appears after a negative `column` value, all subsequent values become NaN.\"\"\"\n",
    "        df = df.copy()\n",
    "        log2_fc_values = df[column].values  # Extract column values as an array\n",
    "    \n",
    "        # Identify first NaN index\n",
    "        nan_mask = np.isnan(log2_fc_values)\n",
    "        if nan_mask.any():\n",
    "            first_nan_idx = np.where(nan_mask)[0][0]  # Find first NaN index\n",
    "    \n",
    "            # Check the last valid value before NaN\n",
    "            last_valid_idx = first_nan_idx - 1 if first_nan_idx > 0 else None\n",
    "            if last_valid_idx is not None and log2_fc_values[last_valid_idx] < 0:\n",
    "                # If last valid log2 fold change was negative, set all subsequent values to NaN\n",
    "                log2_fc_values[first_nan_idx:] = np.nan\n",
    "    \n",
    "        df[column] = log2_fc_values  # Update the DataFrame\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    # **Create and Save the Fold Change CSV with NaN Propagation for Negative Values**\n",
    "    df_fc_nan_propagated = df_fc.groupby([\"Cluster 1\", \"Cluster 2\"]).apply(propagate_nans_for_negative_fc)\n",
    "    df_fc_nan_propagated.to_csv(os.path.join(subtraj_dir, f\"fold_change_nan_propagated_{gene_of_interest}.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Saved fold change CSV with NaN propagation for negative values: fold_change_nan_propagated_{gene_of_interest}.csv\")\n",
    "\n",
    "\n",
    "    df_fc.to_csv(os.path.join(subtraj_dir, f\"fold_change_{gene_of_interest}.csv\"), index=False)\n",
    "    df_md.to_csv(os.path.join(subtraj_dir, f\"mean_difference_{gene_of_interest}.csv\"), index=False)\n",
    "    df_pval.to_csv(os.path.join(subtraj_dir, f\"p_values_{gene_of_interest}.csv\"), index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "    # -------- Visualization --------\n",
    "\n",
    "    ## **(1) Line Plot for Mean Difference (Each Cluster Pair)**\n",
    "    for (cluster1, cluster2), group in df_md.groupby([\"Cluster 1\", \"Cluster 2\"]):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(group[\"Time\"], group[\"Mean Difference\"], label=f\"Clusters {cluster1} vs {cluster2}\", marker='o', linestyle='-')\n",
    "        plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Mean Difference\")\n",
    "        plt.title(f\"Mean Difference Over Time: {gene_of_interest}\\nCluster {cluster1} vs {cluster2}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(subtraj_dir, f\"mean_difference_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    ## **(2) Line Plot for Fold-Change (All Cluster Pairs)**\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (cluster1, cluster2), group in df_fc.groupby([\"Cluster 1\", \"Cluster 2\"]):\n",
    "        plt.plot(group[\"Time\"], group[\"Log2 Fold Change\"], label=f\"Clusters {cluster1} vs {cluster2}\", marker='o')\n",
    "\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Log2 Fold Change\")\n",
    "    plt.title(f\"Log2 Fold Change Over Time - {gene_of_interest}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"fold_change_all_clusters_{gene_of_interest}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    ## **(3) Line Plot of p-values (All Cluster Pairs)**\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (cluster1, cluster2), group in df_pval.groupby([\"Cluster 1\", \"Cluster 2\"]):\n",
    "        plt.plot(group[\"Time\"], group[\"p-value\"], label=f\"Clusters {cluster1} vs {cluster2}\", marker='o')\n",
    "\n",
    "    plt.axhline(y=0.05, color=\"r\", linestyle=\"--\", label=\"Significance Threshold (p=0.05)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(1e-6, 1)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"p-value (log scale)\")\n",
    "    plt.title(f\"P-values for Differential Expression - {gene_of_interest}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"p_values_all_clusters_{gene_of_interest}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db14cfb-33c4-45cd-b677-cddb7a2d91d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Generate the statistical results for the comparison of subtrajectories (Stem Cell data)\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 3\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]\n",
    "\n",
    "#intermediate_t = []\n",
    "\n",
    "d_red= 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "subtraj_dir = os.path.join(output_dir, 'subtraj')\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(subtraj_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Compute_and_Plot_FoldChange_MeanDiff_PValues(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397dc0c3-5cb4-4653-b0a0-84bdc8c88d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Generate the statistical results for the comparison of subtrajectories (Stem Cell data)\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 3\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,2,3]\n",
    "\n",
    "#intermediate_t = []\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=6e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "subtraj_dir = os.path.join(result_dir, 'output', exp_memo, 'subtraj')\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(subtraj_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Compute_and_Plot_FoldChange_MeanDiff_PValues(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bcc5a-5ced-4c04-86b1-f308f1c9442f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the difference of means across subtrajectories (EMT data)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "genes_of_interest = gene_names  # List of all genes\n",
    "subtraj_dir = os.path.join(result_dir, \"output\", exp_memo, \"subtraj\")\n",
    "\n",
    "# Load all CSVs for fold change, mean difference, and p-values\n",
    "df_fc_list, df_md_list, df_pval_list = [], [], []\n",
    "\n",
    "for gene in genes_of_interest:\n",
    "    fc_file_path = os.path.join(subtraj_dir, f\"fold_change_nan_propagated_{gene}.csv\")\n",
    "    md_file_path = os.path.join(subtraj_dir, f\"mean_difference_{gene}.csv\")\n",
    "    pval_file_path = os.path.join(subtraj_dir, f\"p_values_{gene}.csv\")\n",
    "\n",
    "    if os.path.exists(fc_file_path):\n",
    "        df_fc = pd.read_csv(fc_file_path)\n",
    "        df_fc[\"Gene\"] = gene  \n",
    "        df_fc_list.append(df_fc)\n",
    "\n",
    "    if os.path.exists(md_file_path):\n",
    "        df_md = pd.read_csv(md_file_path)\n",
    "        df_md[\"Gene\"] = gene  \n",
    "        df_md_list.append(df_md)\n",
    "\n",
    "    if os.path.exists(pval_file_path):\n",
    "        df_pval = pd.read_csv(pval_file_path)\n",
    "        df_pval[\"Gene\"] = gene  \n",
    "        df_pval_list.append(df_pval)\n",
    "\n",
    "# Merge all DataFrames\n",
    "df_fc_all = pd.concat(df_fc_list, ignore_index=True)\n",
    "df_md_all = pd.concat(df_md_list, ignore_index=True)\n",
    "df_pval_all = pd.concat(df_pval_list, ignore_index=True)\n",
    "\n",
    "# Merge p-values into fold change and mean difference DataFrames\n",
    "df_fc_all = df_fc_all.merge(df_pval_all, on=[\"Time\", \"Cluster 1\", \"Cluster 2\", \"Gene\"], how=\"left\")\n",
    "df_md_all = df_md_all.merge(df_pval_all, on=[\"Time\", \"Cluster 1\", \"Cluster 2\", \"Gene\"], how=\"left\")\n",
    "\n",
    "# **Rescale Time by dividing by 50**\n",
    "df_fc_all[\"Time\"] = df_fc_all[\"Time\"] / 50\n",
    "df_md_all[\"Time\"] = df_md_all[\"Time\"] / 50\n",
    "\n",
    "# Define thresholds\n",
    "pos_threshold_fc = 5.0  \n",
    "neg_threshold_fc = -5.0  \n",
    "pos_threshold_md = 0.11  \n",
    "neg_threshold_md = -0.15  \n",
    "p_value_threshold = 1e-3  # Significance threshold\n",
    "\n",
    "# **Identify genes passing p-value threshold**\n",
    "significant_genes = df_pval_all.groupby(\"Gene\")[\"p-value\"].apply(lambda x: x.min(skipna=True) < p_value_threshold)\n",
    "significant_genes = significant_genes[significant_genes].index  # Only genes passing p-value\n",
    "\n",
    "# **Filter fold-change and mean-difference genes, but retain all passing p-value**\n",
    "def filter_significant_genes(df, metric_col, threshold_high, threshold_low):\n",
    "    \"\"\"Find genes that exceed thresholds (colored) and others that stay gray (but pass p-value).\"\"\"\n",
    "    gene_criteria = df.groupby(\"Gene\")[metric_col].apply(lambda x: ((x > threshold_high) | (x < threshold_low)).any())\n",
    "    \n",
    "    highlighted_genes = gene_criteria[gene_criteria].index  # Genes exceeding threshold\n",
    "    retained_genes = significant_genes.intersection(gene_criteria.index)  # Only keep genes passing p-value\n",
    "    \n",
    "    return retained_genes, highlighted_genes  # Return both full set and colored ones\n",
    "\n",
    "# **Apply filtering**\n",
    "all_genes_fc, highlighted_genes_fc = filter_significant_genes(df_fc_all, \"Log2 Fold Change\", pos_threshold_fc, neg_threshold_fc)\n",
    "all_genes_md, highlighted_genes_md = filter_significant_genes(df_md_all, \"Mean Difference\", pos_threshold_md, neg_threshold_md)\n",
    "\n",
    "# Compute per-gene normalization\n",
    "df_md_all[\"Normalized Mean Difference\"] = df_md_all.groupby(\"Gene\")[\"Mean Difference\"].transform(lambda x: x / x.abs().max())\n",
    "\n",
    "# Identify genes with significant normalized mean difference changes\n",
    "gene_change_norm = df_md_all.groupby(\"Gene\")[\"Normalized Mean Difference\"].apply(lambda x: x.max() - x.min())\n",
    "all_genes_norm = significant_genes.intersection(gene_change_norm.index)\n",
    "highlighted_genes_norm = gene_change_norm[gene_change_norm > 0.45].index\n",
    "\n",
    "# Define colormaps\n",
    "cmap1 = plt.colormaps[\"tab20b\"]\n",
    "cmap2 = plt.colormaps[\"tab20c\"]\n",
    "cmap3 = plt.colormaps[\"Set1\"]\n",
    "cmap4 = plt.colormaps[\"Set3\"]\n",
    "cmap5 = plt.colormaps[\"Paired\"]\n",
    "\n",
    "# Generate color lists\n",
    "color_list = (\n",
    "    [cmap3(i) for i in range(min(9, len(cmap3.colors)))] +  \n",
    "    [cmap4(i) for i in range(min(12, len(cmap4.colors)))] +  \n",
    "    [cmap1(i) for i in range(20)] +\n",
    "    [cmap2(i) for i in range(20)] +\n",
    "    [cmap5(i) for i in range(min(12, len(cmap5.colors)))] +  \n",
    "    list(plt.cm.hsv(np.linspace(0, 1, 15)))  \n",
    ")\n",
    "\n",
    "# Assign colors\n",
    "all_highlighted_genes = sorted(set(highlighted_genes_fc).union(set(highlighted_genes_md)).union(set(highlighted_genes_norm)))\n",
    "gene_colors = {gene: color_list[i % len(color_list)] for i, gene in enumerate(all_highlighted_genes)}\n",
    "\n",
    "# Group by cluster pairs and plot\n",
    "for (cluster1, cluster2), group_fc in df_fc_all.groupby([\"Cluster 1\", \"Cluster 2\"]):\n",
    "\n",
    "    # **(1) Fold Change Plot**\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    legend_handles = []\n",
    "\n",
    "    for gene in all_genes_fc:\n",
    "        sub_group = group_fc[group_fc[\"Gene\"] == gene]\n",
    "        if gene in highlighted_genes_fc:\n",
    "            color = gene_colors.get(gene)\n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "        else:\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "\n",
    "        line, = ax.plot(sub_group[\"Time\"], sub_group[\"Log2 Fold Change\"], marker=\"o\", markersize=4, linestyle=linestyle, color=color, alpha=alpha_value, label=gene)\n",
    "        if gene in highlighted_genes_fc:\n",
    "            legend_handles.append(line)\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Log2 Fold Change\")\n",
    "    ax.set_title(f\"Fold Change Over Time (Cluster {cluster1} vs {cluster2})\")\n",
    "\n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title=\"Significant Genes\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"fold_change_comparison_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # **(2) Mean Difference Plot**\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    legend_handles = []\n",
    "    group_md = df_md_all[(df_md_all[\"Cluster 1\"] == cluster1) & (df_md_all[\"Cluster 2\"] == cluster2)]\n",
    "\n",
    "    for gene in all_genes_md:\n",
    "        sub_group = group_md[group_md[\"Gene\"] == gene]\n",
    "        if gene in highlighted_genes_md:\n",
    "            color = gene_colors.get(gene)\n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "        else:\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "\n",
    "        line, = ax.plot(sub_group[\"Time\"], sub_group[\"Mean Difference\"], marker=\"o\", markersize=4, linestyle=linestyle, color=color, alpha=alpha_value, label=gene)\n",
    "        if gene in highlighted_genes_md:\n",
    "            legend_handles.append(line)\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Mean Difference\")\n",
    "    ax.set_title(f\"Mean Difference Over Time (Cluster {cluster1} vs {cluster2})\")\n",
    "\n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title=\"Significant Genes\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"mean_difference_comparison_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # **(3) Normalized Mean Difference Plot**\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    legend_handles = []\n",
    "    \n",
    "    # **Filter data for the given cluster pair**\n",
    "    group_md = df_md_all[(df_md_all[\"Cluster 1\"] == cluster1) & (df_md_all[\"Cluster 2\"] == cluster2)]\n",
    "    \n",
    "    # **Only keep genes passing the p-value threshold**\n",
    "    genes_to_plot = significant_genes.intersection(group_md[\"Gene\"].unique())\n",
    "    \n",
    "    # **Get Last Time Point Values**\n",
    "    last_time_values = group_md[group_md[\"Time\"] == group_md[\"Time\"].max()].set_index(\"Gene\")[\"Normalized Mean Difference\"]\n",
    "    \n",
    "    # **Categorize Highlighted Genes into Four Groups Based on Their Last Time Point Values**\n",
    "    group_green = [gene for gene in highlighted_genes_norm if gene in last_time_values and 1.0 >= last_time_values[gene] > 0.8]\n",
    "    group_red = [gene for gene in highlighted_genes_norm if gene in last_time_values and 0.8 >= last_time_values[gene] > -0.3]\n",
    "    group_blue = [gene for gene in highlighted_genes_norm if gene in last_time_values and -0.3 >= last_time_values[gene] > -0.8]\n",
    "    group_orange = [gene for gene in highlighted_genes_norm if gene in last_time_values and -0.8 >= last_time_values[gene] >= -1.0]\n",
    "    \n",
    "    # **Define function to get only bright colors from a colormap (skip dark colors)**\n",
    "    def get_bright_colormap(cmap_name, num_colors):\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        if num_colors == 1:\n",
    "            return [cmap(0.75)]  # Single color case, select a bright shade\n",
    "        return [cmap(0.5 + (i / (2 * (num_colors - 1)))) for i in range(num_colors)]  # Use only upper 50% of colormap\n",
    "    \n",
    "    # **Generate Bright Color Maps for Each Group**\n",
    "    colors_green = get_bright_colormap(\"Greens\", max(len(group_green), 1))\n",
    "    colors_blue = get_bright_colormap(\"Purples\", max(len(group_blue), 1))\n",
    "    colors_red = get_bright_colormap(\"Reds\", max(len(group_red), 1))\n",
    "    colors_orange = get_bright_colormap(\"Oranges\", max(len(group_orange), 1))\n",
    "    \n",
    "    # **Assign Colors to Highlighted Genes Based on Group**\n",
    "    color_mapped_genes = {}\n",
    "    \n",
    "    for i, gene in enumerate(group_green):\n",
    "        color_mapped_genes[gene] = colors_green[i]\n",
    "    for i, gene in enumerate(group_blue):\n",
    "        color_mapped_genes[gene] = colors_blue[i]\n",
    "    for i, gene in enumerate(group_red):\n",
    "        color_mapped_genes[gene] = colors_red[i]\n",
    "    for i, gene in enumerate(group_orange):\n",
    "        color_mapped_genes[gene] = colors_orange[i]\n",
    "    \n",
    "    # **Plot Data (Exclude group_green and group_red)**\n",
    "    for gene in genes_to_plot:  # **Only plot genes that satisfy p-value threshold**\n",
    "        if gene in group_green or gene in group_orange:  # Skip plotting genes in group_green and group_red\n",
    "            continue  \n",
    "    \n",
    "        sub_group = group_md[group_md[\"Gene\"] == gene]\n",
    "    \n",
    "        if gene in highlighted_genes_norm:  # **Highlight genes that satisfy both p-value & change criteria**\n",
    "            color = color_mapped_genes.get(gene, \"black\")\n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "            line, = ax.plot(sub_group[\"Time\"], sub_group[\"Normalized Mean Difference\"], \n",
    "                            marker=\"o\", markersize=4, linestyle=linestyle, \n",
    "                            color=color, alpha=alpha_value, label=gene)\n",
    "            legend_handles.append((line, gene))  # Save for grouped legend\n",
    "        else:  # **Gray for genes that satisfy p-value but not change threshold**\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "            ax.plot(sub_group[\"Time\"], sub_group[\"Normalized Mean Difference\"], \n",
    "                    marker=\"o\", markersize=4, linestyle=linestyle, \n",
    "                    color=color, alpha=alpha_value, label=gene)\n",
    "\n",
    "    # **Labeling and Formatting**\n",
    "    ax.set_xlabel(\"Time\", fontsize=30)\n",
    "    ax.set_ylabel(\"Normalized Mean Difference\", fontsize=30)\n",
    "    ax.set_title(f\"Trajectory {cluster1+1} vs {cluster2+1}\", fontsize=30)\n",
    "    ax.tick_params(axis=\"both\", labelsize=30)\n",
    "    \n",
    "    # **Save Main Plot Without Legend**\n",
    "    plot_output_path = os.path.join(subtraj_dir, f\"normalized_mean_difference_cluster_{cluster1}_vs_{cluster2}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"âœ… Normalized Mean Difference plot saved: {plot_output_path}\")\n",
    "    \n",
    "    # **Create and Save Separate Legend**\n",
    "    if legend_handles:\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(10, 10))  # Wider figure to fit grouped layout\n",
    "        ax_legend.axis(\"off\")\n",
    "    \n",
    "        # **Group Legend by Color**\n",
    "        grouped_legend_handles = []\n",
    "        \n",
    "        for gene_list, title in zip(\n",
    "            #[group_green, group_blue, group_red, group_orange],\n",
    "            [group_red, group_blue],\n",
    "            [\"\", \"\", \"\", \"\"]  # No explicit labels, but groups remain visually separate\n",
    "        ):\n",
    "            if gene_list:\n",
    "                handles = [mpatches.Patch(color=color_mapped_genes[gene], label=gene) for gene in gene_list]\n",
    "                grouped_legend_handles.append(handles)\n",
    "    \n",
    "        # **Flatten List of Legends**\n",
    "        flattened_handles = [item for sublist in grouped_legend_handles for item in sublist]\n",
    "    \n",
    "        # **Plot the Grouped Legend**\n",
    "        ax_legend.legend(\n",
    "            handles=flattened_handles, \n",
    "            title=\"Significant Genes\", \n",
    "            loc=\"center\", \n",
    "            fontsize=26, \n",
    "            title_fontsize=26, \n",
    "            frameon=True, \n",
    "            ncol=min(len(flattened_handles), 2)  # Adjust to avoid overflow\n",
    "        )\n",
    "    \n",
    "        # **Save Legend as Separate PNG**\n",
    "        legend_output_path = os.path.join(subtraj_dir, f\"legend_normalized_mean_difference_cluster_{cluster1}_vs_{cluster2}.png\")\n",
    "        plt.savefig(legend_output_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"âœ… Legend saved separately: {legend_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018fb79-ebd3-4832-ad86-7469331dba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the difference of means across subtrajectories (Stem cell data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Define function to get evenly spaced colors from a colormap\n",
    "def get_colormap_colors(cmap_name, num_colors):\n",
    "    if num_colors == 1:\n",
    "        return [plt.get_cmap(cmap_name)(0.5)]  # Select the middle of the colormap\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    return [cmap(i / (num_colors - 1)) for i in range(num_colors)]\n",
    "\n",
    "# Define paths\n",
    "genes_of_interest = gene_names  \n",
    "subtraj_dir = os.path.join(result_dir, \"output\", exp_memo, \"subtraj\")\n",
    "\n",
    "# Load all CSVs for fold change, mean difference, and p-values\n",
    "df_fc_list, df_md_list, df_pval_list = [], [], []\n",
    "\n",
    "for gene in genes_of_interest:\n",
    "    fc_file_path = os.path.join(subtraj_dir, f\"fold_change_nan_propagated_{gene}.csv\")\n",
    "    md_file_path = os.path.join(subtraj_dir, f\"mean_difference_{gene}.csv\")\n",
    "    pval_file_path = os.path.join(subtraj_dir, f\"p_values_{gene}.csv\")\n",
    "\n",
    "    if os.path.exists(fc_file_path):\n",
    "        df_fc = pd.read_csv(fc_file_path)\n",
    "        df_fc[\"Gene\"] = gene  \n",
    "        df_fc_list.append(df_fc)\n",
    "\n",
    "    if os.path.exists(md_file_path):\n",
    "        df_md = pd.read_csv(md_file_path)\n",
    "        df_md[\"Gene\"] = gene  \n",
    "        df_md_list.append(df_md)\n",
    "\n",
    "    if os.path.exists(pval_file_path):\n",
    "        df_pval = pd.read_csv(pval_file_path)\n",
    "        df_pval[\"Gene\"] = gene  \n",
    "        df_pval_list.append(df_pval)\n",
    "\n",
    "# Merge all DataFrames\n",
    "df_fc_all = pd.concat(df_fc_list, ignore_index=True)\n",
    "df_md_all = pd.concat(df_md_list, ignore_index=True)\n",
    "df_pval_all = pd.concat(df_pval_list, ignore_index=True)\n",
    "\n",
    "# Merge p-values into fold change and mean difference DataFrames\n",
    "df_fc_all = df_fc_all.merge(df_pval_all, on=[\"Time\", \"Cluster 1\", \"Cluster 2\", \"Gene\"], how=\"left\")\n",
    "df_md_all = df_md_all.merge(df_pval_all, on=[\"Time\", \"Cluster 1\", \"Cluster 2\", \"Gene\"], how=\"left\")\n",
    "\n",
    "# **Rescale Time**\n",
    "df_fc_all[\"Time\"] = df_fc_all[\"Time\"] / 50\n",
    "df_md_all[\"Time\"] = df_md_all[\"Time\"] / 50\n",
    "\n",
    "# Define thresholds\n",
    "pos_threshold_fc = 5.0  \n",
    "neg_threshold_fc = -5.0  \n",
    "pos_threshold_md = 0.11  \n",
    "neg_threshold_md = -0.15  \n",
    "p_value_threshold = 1e-4  \n",
    "\n",
    "# **Identify significant genes based on p-value threshold**\n",
    "significant_genes = df_pval_all.groupby(\"Gene\")[\"p-value\"].apply(lambda x: x.min(skipna=True) < p_value_threshold)\n",
    "significant_genes = significant_genes[significant_genes].index  \n",
    "\n",
    "# Compute per-gene normalization\n",
    "df_md_all[\"Normalized Mean Difference\"] = df_md_all.groupby(\"Gene\")[\"Mean Difference\"].transform(lambda x: x / x.abs().max())\n",
    "\n",
    "# **Filter fold-change and mean-difference genes, but retain all passing p-value**\n",
    "def filter_significant_genes(df, metric_col, threshold_high, threshold_low):\n",
    "    \"\"\"Find genes that exceed thresholds (colored) and others that stay gray (but pass p-value).\"\"\"\n",
    "    gene_criteria = df.groupby(\"Gene\")[metric_col].apply(lambda x: ((x > threshold_high) | (x < threshold_low)).any())\n",
    "    \n",
    "    highlighted_genes = gene_criteria[gene_criteria].index  # Genes exceeding threshold\n",
    "    retained_genes = significant_genes.intersection(gene_criteria.index)  # Only keep genes passing p-value\n",
    "    \n",
    "    return retained_genes, highlighted_genes  # Return both full set and colored ones\n",
    "\n",
    "# **Apply filtering**\n",
    "all_genes_fc, highlighted_genes_fc = filter_significant_genes(df_fc_all, \"Log2 Fold Change\", pos_threshold_fc, neg_threshold_fc)\n",
    "all_genes_md, highlighted_genes_md = filter_significant_genes(df_md_all, \"Mean Difference\", pos_threshold_md, neg_threshold_md)\n",
    "\n",
    "# Identify genes with significant normalized mean difference changes\n",
    "gene_max_diff = df_md_all.groupby(\"Gene\")[\"Normalized Mean Difference\"].apply(lambda x: x.max() - x.min())\n",
    "\n",
    "# **Determine which genes to highlight**\n",
    "highlighted_genes_norm = significant_genes.intersection(gene_max_diff[gene_max_diff > 0.62].index)\n",
    "gray_genes = significant_genes.difference(highlighted_genes_norm)  # Genes passing p-value but not gene_max_diff\n",
    "\n",
    "# **Split Highlighted Genes into Positive and Negative Groups**\n",
    "positive_genes = []\n",
    "negative_genes = []\n",
    "\n",
    "for gene in highlighted_genes_norm:\n",
    "    initial_value = df_md_all[df_md_all[\"Gene\"] == gene][\"Normalized Mean Difference\"].iloc[0]  \n",
    "    if initial_value >= 0:\n",
    "        positive_genes.append(gene)\n",
    "    else:\n",
    "        negative_genes.append(gene)\n",
    "\n",
    "\n",
    "\n",
    "# **Assign Colors Using Custom Colormaps (Avoiding White)**\n",
    "num_positive = max(len(positive_genes), 1)  \n",
    "num_negative = max(len(negative_genes), 1)  \n",
    "\n",
    "# Load colormaps\n",
    "cmap_reds = plt.get_cmap(\"Reds\")  # Red to White\n",
    "cmap_blues = plt.get_cmap(\"Blues_r\")  # Blue to White (Reversed)\n",
    "\n",
    "# Clip the colormap range to avoid very light colors (too close to white)\n",
    "colors_positive = [cmap_reds(0.3 + 0.7 * (i / (num_positive - 1))) for i in range(num_positive)]  # Avoid very light red\n",
    "colors_negative = [cmap_blues(0.1 + 0.6 * (i / max(1, num_negative - 1))) for i in range(num_negative)]\n",
    "\n",
    "gene_colors_scaled = {}\n",
    "\n",
    "for i, gene in enumerate(positive_genes):\n",
    "    gene_colors_scaled[gene] = colors_positive[i % len(colors_positive)]\n",
    "\n",
    "for i, gene in enumerate(negative_genes):\n",
    "    gene_colors_scaled[gene] = colors_negative[i % len(colors_negative)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Separate Legends for Positive & Negative Genes**\n",
    "legend_handles_pos = [mpatches.Patch(color=gene_colors_scaled[gene], label=gene) for gene in positive_genes]\n",
    "legend_handles_neg = [mpatches.Patch(color=gene_colors_scaled[gene], label=gene) for gene in negative_genes]\n",
    "\n",
    "# **Save Separate Legend for Positive Genes**\n",
    "if legend_handles_pos:\n",
    "    fig_legend_pos, ax_legend_pos = plt.subplots(figsize=(6, max(1, len(legend_handles_pos) // 2)))\n",
    "    ax_legend_pos.axis(\"off\")\n",
    "    ax_legend_pos.legend(handles=legend_handles_pos, loc=\"center\", title=\"Positive  Difference\",\n",
    "                         title_fontsize=36, fontsize=36, frameon=True, ncol=1)\n",
    "    plt.savefig(os.path.join(subtraj_dir, \"legend_positive_genes.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# **Save Separate Legend for Negative Genes**\n",
    "if legend_handles_neg:\n",
    "    fig_legend_neg, ax_legend_neg = plt.subplots(figsize=(6, max(1, len(legend_handles_neg) // 2)))\n",
    "    ax_legend_neg.axis(\"off\")\n",
    "    ax_legend_neg.legend(handles=legend_handles_neg, loc=\"center\", title=\"Negative Difference\",\n",
    "                         title_fontsize=36, fontsize=36, frameon=True, ncol=1)\n",
    "    plt.savefig(os.path.join(subtraj_dir, \"legend_negative_genes.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# **Generate Main Plot (Without Legend)**\n",
    "for (cluster1, cluster2), sub_group in df_md_all.groupby([\"Cluster 1\", \"Cluster 2\"]):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    for gene in significant_genes:  # Only genes passing p-value threshold are plotted\n",
    "        gene_data = sub_group[sub_group[\"Gene\"] == gene]\n",
    "\n",
    "        if gene in highlighted_genes_norm:\n",
    "            color = gene_colors_scaled.get(gene, \"gray\")  \n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "        else:  # Genes passing p-value but NOT max diff\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "\n",
    "        ax.plot(\n",
    "            gene_data[\"Time\"], gene_data[\"Normalized Mean Difference\"], \n",
    "            marker=\"o\", markersize=3, linestyle=linestyle, color=color, alpha=alpha_value\n",
    "        )\n",
    "\n",
    "    # Labels and Title\n",
    "    ax.set_xlabel(\"Time\", fontsize=30)\n",
    "    ax.set_ylabel(\"Normalized Mean Difference\", fontsize=30)\n",
    "    ax.set_title(f\"Trajectory {cluster1 + 1} vs {cluster2 + 1}\", fontsize=30)\n",
    "    ax.tick_params(axis=\"both\", labelsize=30)\n",
    "\n",
    "    # Save Plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"normalized_mean_difference_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "     # **(1) Fold Change Plot**\n",
    "    group_fc = df_fc_all[(df_fc_all[\"Cluster 1\"] == cluster1) & (df_fc_all[\"Cluster 2\"] == cluster2)]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    legend_handles = []\n",
    "\n",
    "    for gene in all_genes_fc:\n",
    "        sub_group = group_fc[group_fc[\"Gene\"] == gene]\n",
    "        if gene in highlighted_genes_fc:\n",
    "            color = gene_colors.get(gene)\n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "        else:\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "\n",
    "        line, = ax.plot(sub_group[\"Time\"], sub_group[\"Log2 Fold Change\"], marker=\"o\", markersize=4, linestyle=linestyle, color=color, alpha=alpha_value, label=gene)\n",
    "        if gene in highlighted_genes_fc:\n",
    "            legend_handles.append(line)\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Time\", fontsize = 30)\n",
    "    ax.set_ylabel(\"Log2 Fold Change\", fontsize = 30)\n",
    "    ax.set_title(f\"Fold Change Over Time (Cluster {cluster1} vs {cluster2})\", fontsize = 30)\n",
    "\n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title=\"Significant Genes\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"fold_change_comparison_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # **(2) Mean Difference Plot**\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    legend_handles = []\n",
    "    group_md = df_md_all[(df_md_all[\"Cluster 1\"] == cluster1) & (df_md_all[\"Cluster 2\"] == cluster2)]\n",
    "\n",
    "    for gene in all_genes_md:\n",
    "        sub_group = group_md[group_md[\"Gene\"] == gene]\n",
    "        if gene in highlighted_genes_md:\n",
    "            color = gene_colors.get(gene)\n",
    "            alpha_value, linestyle = 1.0, \"-\"\n",
    "        else:\n",
    "            color = \"gray\"\n",
    "            alpha_value, linestyle = 0.2, \"--\"\n",
    "\n",
    "        line, = ax.plot(sub_group[\"Time\"], sub_group[\"Mean Difference\"], marker=\"o\", markersize=4, linestyle=linestyle, color=color, alpha=alpha_value, label=gene)\n",
    "        if gene in highlighted_genes_md:\n",
    "            legend_handles.append(line)\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Time\", fontsize = 30)\n",
    "    ax.set_ylabel(\"Mean Difference\", fontsize = 30)\n",
    "    ax.set_title(f\"Mean Difference Over Time (Cluster {cluster1} vs {cluster2})\", fontsize = 30)\n",
    "\n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title=\"Significant Genes\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(subtraj_dir, f\"mean_difference_comparison_cluster_{cluster1}_vs_{cluster2}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(\"âœ… Main plots saved.\")\n",
    "print(\"âœ… Separate legend for positive genes saved as 'legend_positive_genes.png'.\")\n",
    "print(\"âœ… Separate legend for negative genes saved as 'legend_negative_genes.png'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e91c1-8384-48ec-aea1-8ae7c30dac89",
   "metadata": {},
   "source": [
    "## Plot single gene dynamics for every single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13105277-7e64-4d36-ae15-12605d76473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for EMT data, Time [0 , 2, 4]\n",
    "## Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_EMT(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    #subtrajectory_colors = ['red', 'blue', 'brown']\n",
    "    subtrajectory_colors = ['green']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"gray\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 â†’ 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Individual_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cellâ€™s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # âœ… Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 â†’ 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory {label_mapping[label]}'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 2, 4])  # Ensure correct positions\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # âœ… REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # âœ… Assign correct color\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0,2, 4])  \n",
    "    ax1.set_xticklabels([0, 2, 4], fontsize=32)\n",
    "    ax1.tick_params(axis='y', labelsize=32)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=32)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=32)\n",
    "    ax1.set_title(f'Single Cell {gene_of_interest} Expression Dynamics', fontsize=32)\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # ðŸŽ¨ **Redefine `legend_patches` to Include a Green Bar**\n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"green\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Gene dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b2723-a5e8-4bbf-8422-c6fd8ca4e1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for EMT data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]\n",
    "#intermediate_t = [4]\n",
    "\n",
    "d_red= 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_EMT(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafe22a-1a0c-46c9-a09b-eae74c2dabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for EMT data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Individual_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Individual_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54ca85-d12d-43c9-9c62-15c180451c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for Stem Cell data, Time [0 , 1,  2, 3,  4]\n",
    "## Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_mESC(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day2]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=40)\n",
    "    kmeans.fit(last_day_reduced)\n",
    "    last_day_labels = kmeans.labels_\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "\n",
    "    #mask = last_day_labels == 0\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    #subtrajectory_colors = ['red', 'blue']\n",
    "    subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    #violin_colors = [\"#3cb44b\", \"#f58231\", \"#3cb44b\", \"#f58231\", \"#3cb44b\"]  # Green, Orange, Purple\n",
    "    violin_colors = [\"black\", \"gray\", \"black\", \"gray\", \"black\"] \n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 â†’ 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Individual_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cellâ€™s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # âœ… Use the **subtrajectory colors**\n",
    "                    alpha=0.7, linewidth=1.0 \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 â†’ 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory {label_mapping[label]}'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 1, 2, 3, 4])  # Ensure correct positions\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # âœ… REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # âœ… Assign correct color\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 1, 2, 3, 4])  \n",
    "    ax1.set_xticklabels([0, 1, 2, 3, 4], fontsize=35)\n",
    "    ax1.tick_params(axis='y', labelsize=35)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=35)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=35)\n",
    "    ax1.set_title(f'Single Cell {gene_of_interest} Expression Dynamics', fontsize=35)\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # ðŸŽ¨ **Redefine `legend_patches` to Include a Green Bar**\n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Gene dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1837c38-e9e5-47f5-bd9b-6b0aa482ccd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for Stem cell data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,2,3]\n",
    "#intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_mESC(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359e0ac-232e-4445-8511-4ea756a22b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for stem cell data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Individual_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Individual_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdef54-19ec-43fc-b1b2-7360af7749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for breast cancer cell line's data, Time [0 , 4]\n",
    "# Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_NDPR(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_deviation.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    real_cell_types = np.array(cell_ids_by_day[day2])\n",
    "    #unique_cell_types = np.unique(real_cell_types)\n",
    "    unique_cell_types = unique_labels\n",
    "    #subtrajectory_colors = list(sns.color_palette(\"tab20\", len(unique_cell_types)))\n",
    "    #subtrajectory_colors = ['green', 'orange', 'purple', 'blue', 'red', 'brown']\n",
    "    subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 â†’ 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cellâ€™s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # âœ… Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 â†’ 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory of {label_mapping[label]} phenotypic shift'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 4])  # Ensure correct positions\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # âœ… REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # âœ… Assign correct color\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 4])  \n",
    "    #ax1.set_xticklabels([0, 4], fontsize=32)\n",
    "    ax1.set_xticklabels([\"Pre-treatment\", \"Post-treatment\"], fontsize=46)\n",
    "    ax1.tick_params(axis='y', labelsize=46)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=46)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=46)\n",
    "    ax1.set_title(f'{gene_of_interest}', fontsize=46)\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # ðŸŽ¨ **Redefine `legend_patches` to Include a Green Bar**\n",
    "    \n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Hallmark dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec9d17-841f-4ee3-9018-6305aa22a610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for breast cancer cell line data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "#intermediate_t = [1,2,3]\n",
    "intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_NDPR(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba050fa6-7fd2-4b57-8290-047b682e1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf - Breast cancer cell line data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674b512-09e6-4e75-835f-72b1be73082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for clinical data, Time [0 , 4]\n",
    "## Plot gene dynamis for each trajectory (deviation cell types)\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_clinical(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_deviation.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    real_cell_types = np.array(cell_ids_by_day[day2])\n",
    "    #unique_cell_types = np.unique(real_cell_types)\n",
    "    unique_cell_types = unique_labels\n",
    "    #subtrajectory_colors = list(sns.color_palette(\"tab20\", len(unique_cell_types)))\n",
    "    subtrajectory_colors = ['green', 'orange', 'purple', 'blue', 'red', 'brown']\n",
    "    #subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 â†’ 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cellâ€™s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # âœ… Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 â†’ 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory of {label_mapping[label]} phenotypic shift'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 4])  # Ensure correct positions\n",
    "    \n",
    "    # ðŸŽ» **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # âœ… REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # âœ… Assign correct color\n",
    "            alpha=0.8,  # âœ… MAKE TRANSPARENT\n",
    "            zorder=3  # âœ… BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # ðŸ›  **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 4])  \n",
    "    #ax1.set_xticklabels([0, 4], fontsize=32)\n",
    "    ax1.set_xticklabels([\"Pre-treatment\", \"Post-treatment\"], fontsize=46)\n",
    "    ax1.tick_params(axis='y', labelsize=46)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=46)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=46)\n",
    "    ax1.set_title(f'{gene_of_interest}', fontsize=46)\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # ðŸŽ¨ **Redefine `legend_patches` to Include a Green Bar**\n",
    "    \n",
    "    #legend_patches = [\n",
    "    #    mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "    #                  label=\"Hallmark dynamics of each single cell\")\n",
    "    #]\n",
    "\n",
    "    label_descriptions = {\n",
    "    \"low\": \"Trajectory of low phenotypic shift\",\n",
    "    \"medium\": \"Trajectory of medium phenotypic shift\",\n",
    "    \"high\": \"Trajectory of high phenotypic shift\"}\n",
    "\n",
    "\n",
    "    # Thicker lines using `linewidth`\n",
    "    legend_patches = [\n",
    "        mlines.Line2D(\n",
    "            [], [], color=color, linestyle='-', linewidth=3,  # â† thicker line here\n",
    "            markersize=10,\n",
    "            label=f\"{label_descriptions.get(ctype, '')}\"\n",
    "        )\n",
    "        for ctype, color in zip(unique_cell_types, subtrajectory_colors)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # ðŸŽ¨ **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\")\n",
    "    ]\n",
    "    \n",
    "    # ðŸŽ¨ **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278d60e-2645-4946-afb9-bfc64663642a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for clinical data\n",
    "\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "#intermediate_t = [1,2,3]\n",
    "intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_clinical(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e18839-42db-4b04-83bd-d05800cbf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf - clincial data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbdc0b-7311-4b87-8c3b-1bd0f7ba27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantify the errors by using W2 distance (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different.\n",
    "\n",
    "    Parameters:\n",
    "    - source_t: Start time point (not included in the plot)\n",
    "    - target_t: End time point (not included in the plot)\n",
    "    - optimal_k: Number of clusters for KMeans\n",
    "    - gene_of_interest: The gene whose expression is analyzed\n",
    "    - index: Step size for trajectory extraction\n",
    "    - max_i: Maximum index for trajectory extraction\n",
    "    - intermediate_t: List of intermediate time points (defaults to [1] if not provided)\n",
    "    - d_red: Dimensionality reduction method\n",
    "    - random_state: Random seed\n",
    "    - exp_memo: Experiment identifier\n",
    "    - num_permutations: Number of permutations for significance testing.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2 distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure intermediate_t has a default value\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute the observed W2 distance\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2_distances = []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  # Shuffle data\n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2 = wasserstein_distance(perm_test_sample, perm_pred_sample)\n",
    "            permuted_w2_distances.append(permuted_w2)\n",
    "\n",
    "        # Compute p-value (proportion of permuted distances â‰¥ observed W2)\n",
    "        p_value = np.mean(np.array(permuted_w2_distances) >= observed_w2)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2,\n",
    "            \"Permutation Mean W2\": np.mean(permuted_w2_distances),\n",
    "            \"Permutation Std W2\": np.std(permuted_w2_distances),\n",
    "            \"p-value\": p_value\n",
    "        }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Permutation Test Summary ---\")\n",
    "    for time, results in permutation_results.items():\n",
    "        print(f\"Time {time}:\")\n",
    "        print(f\"  Observed W2: {results['Observed W2']:.4f}\")\n",
    "        print(f\"  Mean Permutation W2: {results['Permutation Mean W2']:.4f} Â± {results['Permutation Std W2']:.4f}\")\n",
    "        print(f\"  p-value: {results['p-value']:.4f}\")\n",
    "        if results[\"p-value\"] < 0.05:\n",
    "            print(\"  ** Significant Difference (Reject Null Hypothesis) **\")\n",
    "        else:\n",
    "            print(\"  No Significant Difference (Cannot Reject Null Hypothesis)\")\n",
    "\n",
    "    return permutation_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843bf8b-a5c9-4fa3-943c-2f647a9426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics but No Sinkhorn (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def maximum_mean_discrepancy(X, Y, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) between two distributions using an RBF kernel.\n",
    "    \"\"\"\n",
    "    K_xx = rbf_kernel(X[:, None], X[:, None], gamma=gamma)\n",
    "    K_yy = rbf_kernel(Y[:, None], Y[:, None], gamma=gamma)\n",
    "    K_xy = rbf_kernel(X[:, None], Y[:, None], gamma=gamma)\n",
    "\n",
    "    return K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000, mmd_gamma=1.0):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different using W2 and MMD.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2, MMD distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute observed metrics\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        observed_mmd = maximum_mean_discrepancy(test_vals, predicted_vals, gamma=mmd_gamma)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2, permuted_mmd = [], []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  \n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2.append(wasserstein_distance(perm_test_sample, perm_pred_sample))\n",
    "            permuted_mmd.append(maximum_mean_discrepancy(perm_test_sample, perm_pred_sample, gamma=mmd_gamma))\n",
    "\n",
    "        # Compute p-values\n",
    "        p_w2 = np.mean(np.array(permuted_w2) >= observed_w2)\n",
    "        p_mmd = np.mean(np.array(permuted_mmd) >= observed_mmd)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2, \"p_W2\": p_w2,\n",
    "            \"Observed MMD\": observed_mmd, \"p_MMD\": p_mmd\n",
    "        }\n",
    "\n",
    "    return permutation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3d5c8-69ed-464f-85b8-8f7d53d31c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics With Sinkhorn (permutation test)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from geomloss import SamplesLoss  # Sinkhorn divergence\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with entropy regularization.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "\n",
    "def permutation_test_sinkhorn(test_vals, pred_vals, num_permutations=1000, epsilon=0.5):\n",
    "    \"\"\"Permutation test for Sinkhorn divergence.\"\"\"\n",
    "    observed_stat = sinkhorn_divergence(test_vals, pred_vals, epsilon)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        perm_stat = sinkhorn_divergence(perm_test_sample, perm_pred_sample, epsilon)\n",
    "        permuted_stats.append(perm_stat)\n",
    "\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Sinkhorn(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                  intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                  num_permutations=1000, sinkhorn_epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with correct scaling and permutation test.\"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping method not available.\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correct scaling\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    predicted_data = []\n",
    "    for snapshot_idx in scaled_intermediate_indices:\n",
    "        if snapshot_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[snapshot_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        predicted_vals = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_data.append(predicted_vals)\n",
    "\n",
    "    results = []\n",
    "    for time, test_vals, pred_vals in zip(intermediate_t, test_data, predicted_data):\n",
    "\n",
    "        # Match sample sizes\n",
    "        min_size = min(len(test_vals), len(pred_vals))\n",
    "        test_vals, pred_vals = resample(test_vals, n_samples=min_size, random_state=42), \\\n",
    "                               resample(pred_vals, n_samples=min_size, random_state=42)\n",
    "\n",
    "        # Compute Sinkhorn and permutation test\n",
    "        sinkhorn_stat, p_sinkhorn = permutation_test_sinkhorn(\n",
    "            test_vals, pred_vals, num_permutations, sinkhorn_epsilon\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"Sinkhorn Divergence\": sinkhorn_stat,\n",
    "            \"Sinkhorn p-value\": p_sinkhorn\n",
    "        })\n",
    "\n",
    "    output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(output_dir, f\"Sinkhorn_metrics_{gene_of_interest}.csv\")\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd419e-09fe-4e6a-bd6b-cd0ddc14afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 1\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean Â± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, and Jensen-Shannon).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    intermediate_only_points = intermediate_t  \n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "        \n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db53770-527b-4c6e-9212-b23285fb8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 3\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean Â± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, JS, and Mean Comparison).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correctly scale the intermediate time points\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions using scaled indices\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "\n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Mean and Standard Deviation Comparison**\n",
    "        mean_test = np.mean(test_vals)\n",
    "        mean_pred = np.mean(predicted_vals)\n",
    "        std_test = np.std(test_vals)\n",
    "        std_pred = np.std(predicted_vals)\n",
    "        mean_diff = mean_test - mean_pred\n",
    "        std_diff = std_test - std_pred\n",
    "\n",
    "        # Perform a t-test to compare means\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Test\": mean_test,\n",
    "            \"Mean Predicted\": mean_pred,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Test\": std_test,\n",
    "            \"Standard Deviation Predicted\": std_pred,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b6e27-ef58-47a8-8ee5-76b66bc842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Permuation by statistical tests (permutation test) - stem cell data\n",
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p = np.clip(p, 1e-10, None)\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    min_size = min(len(X), len(Y))\n",
    "    return resample(X, n_samples=min_size, random_state=42), resample(Y, n_samples=min_size, random_state=42)\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence using PyTorch and geomloss.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "        permuted_stats.append(stat_func(perm_test_sample, perm_pred_sample))\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"âŒ PCA method not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    scaling_factor = len(X1_trpts) / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    kde_test_data = [pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t]\n",
    "\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            continue\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    metric_results = []\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)\n",
    "\n",
    "        hist_test = np.histogram(test_vals, bins=50, density=True)[0]\n",
    "        hist_pred = np.histogram(predicted_vals, bins=50, density=True)[0]\n",
    "\n",
    "        tv = total_variation_distance(hist_test, hist_pred)\n",
    "        kl = kl_divergence(hist_test, hist_pred)\n",
    "        js = jensenshannon(hist_test, hist_pred)\n",
    "        w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        sink = sinkhorn_divergence(test_vals, predicted_vals)\n",
    "\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "        perm_w2, mean_perm_w2, std_perm_w2, p_w2 = permutation_test(wasserstein_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_sink, mean_perm_sink, std_perm_sink, p_sink = permutation_test(sinkhorn_divergence, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        mean_diff = np.mean(test_vals) - np.mean(predicted_vals)\n",
    "        std_diff = np.std(test_vals) - np.std(predicted_vals)\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"W2 Distance\": w2,\n",
    "            \"Permutation W2 Â± Std\": f\"{mean_perm_w2:.4f} Â± {std_perm_w2:.4f}\",\n",
    "            \"p-value W2\": p_w2,\n",
    "            \"Sinkhorn Distance\": sink,\n",
    "            \"Permutation Sinkhorn Â± Std\": f\"{mean_perm_sink:.4f} Â± {std_perm_sink:.4f}\",\n",
    "            \"p-value Sinkhorn\": p_sink,\n",
    "            \"Total Variation Distance\": tv,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ… Results saved to: {csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d023-eb24-44eb-b76b-07e018303b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = gene_names # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,3]  # Intermediate time points\n",
    "d_red = 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV Â± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL Â± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 Â± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn Â± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7eb23-a0b9-47c1-8458-c7d0d0a1761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Venn Diagram of null hypothesis results for Stem cell data\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"Sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Ensure consistent column names\n",
    "tv_col = \"Reject Null Hypothesis (TV)\"\n",
    "kl_col = \"Reject Null Hypothesis (KL)\"\n",
    "sinkhorn_col = \"Reject Null Hypothesis (Sinkhorn)\"\n",
    "\n",
    "# Identify unique time points\n",
    "time_points = sorted(tv_df[\"Time\"].unique())\n",
    "\n",
    "# Iterate over each time point\n",
    "for time_point in time_points:\n",
    "    # Filter DataFrames for the current time point\n",
    "    tv_time_df = tv_df[tv_df[\"Time\"] == time_point]\n",
    "    kl_time_df = kl_df[kl_df[\"Time\"] == time_point]\n",
    "    sinkhorn_time_df = sinkhorn_df[sinkhorn_df[\"Time\"] == time_point]\n",
    "\n",
    "    # Identify genes with \"No\" in each metric\n",
    "    genes_no_tv = set(tv_time_df.loc[tv_time_df[tv_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_kl = set(kl_time_df.loc[kl_time_df[kl_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_sinkhorn = set(sinkhorn_time_df.loc[sinkhorn_time_df[sinkhorn_col] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "    # Genes with at least one metric showing \"No\"\n",
    "    genes_at_least_one_no = genes_no_tv | genes_no_kl | genes_no_sinkhorn\n",
    "\n",
    "    # Genes with at least two metrics showing \"No\"\n",
    "    genes_at_least_two_no = (\n",
    "        (genes_no_tv & genes_no_kl) |\n",
    "        (genes_no_tv & genes_no_sinkhorn) |\n",
    "        (genes_no_kl & genes_no_sinkhorn)\n",
    "    )\n",
    "\n",
    "    # Genes with all three metrics showing \"No\"\n",
    "    genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "    # Prepare summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Criteria\": [\n",
    "            \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "        ],\n",
    "        \"Genes\": [\n",
    "            \", \".join(sorted(genes_at_least_one_no)),\n",
    "            \", \".join(sorted(genes_at_least_two_no)),\n",
    "            \", \".join(sorted(genes_all_three_no))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Print results for current time point\n",
    "    print(f\"\\nðŸ”¹ Summary of Genes by Null Hypothesis Rejection (Time {time_point}):\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary to CSV for current time point\n",
    "    summary_csv_path = os.path.join(output_dir, f\"genes_null_hypothesis_summary_time_{time_point}.csv\")\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ… Summary for Time {time_point} saved to {summary_csv_path}\")\n",
    "\n",
    "    # Create Venn diagram for current time point\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    venn = venn3(\n",
    "        [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "        set_labels=('TV', 'KL', 'Sinkhorn')\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Genes NOT Rejecting Null Hypothesis (Time {time_point})\", fontsize=16)\n",
    "\n",
    "    # Add summary annotations\n",
    "    x_pos = 0.6\n",
    "    y_pos = 0.6\n",
    "    step = 0.07\n",
    "\n",
    "    plt.text(x_pos, y_pos, f\"Genes in â‰¥1 metric: {len(genes_at_least_one_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - step, f\"Genes in â‰¥2 metrics: {len(genes_at_least_two_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_all_three_no)}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save Venn diagram\n",
    "    venn_path = os.path.join(output_dir, f\"genes_venn_diagram_time_{time_point}.png\")\n",
    "    plt.savefig(venn_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"âœ… Venn diagram for Time {time_point} saved to {venn_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f1e37-24d5-4466-ad0b-27793a954566",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = ['DSP'] # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]  # Intermediate time points\n",
    "d_red = 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV Â± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL Â± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 Â± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn Â± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc9409-fb78-4961-b9f2-9381d20689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining genes (preprocess)\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "csv_dir = os.path.join(result_dir, 'Sample 3')\n",
    "\n",
    "\n",
    "# List of prefixes\n",
    "prefixes = ['sinkhorn']\n",
    "\n",
    "# Mapping prefixes to their corresponding p-value column names\n",
    "pval_columns = {\n",
    "    'sinkhorn': 'p_Sinkhorn_1',\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through each prefix\n",
    "for prefix in prefixes:\n",
    "    # Use glob to find matching files\n",
    "    matching_files = glob.glob(os.path.join(csv_dir, f\"{prefix}*.csv\"))\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"No files found for prefix '{prefix}'\")\n",
    "        continue\n",
    "    \n",
    "    # Read and concatenate files\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in matching_files], ignore_index=True)\n",
    "\n",
    "    # Check if the p-value column exists\n",
    "    p_col = pval_columns[prefix]\n",
    "    if p_col in combined_df.columns:\n",
    "        combined_df[f'Reject Null Hypothesis ({prefix[:-1].upper()})'] = combined_df[p_col].apply(\n",
    "            lambda p: 'No' if p > 0.05 else 'Yes'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Column '{p_col}' not found in files for prefix '{prefix}'.\")\n",
    "    \n",
    "    # Save the combined dataframe to CSV\n",
    "    output_filename = os.path.join(csv_dir, f\"{prefix}_metric.csv\")\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"âœ… Combined {len(matching_files)} files into '{output_filename}' with hypothesis test results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f349b0a-a140-466f-a287-0c264f2808a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sample 1 (only one time point)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Identify genes with \"No\" in each metric\n",
    "genes_no_tv = set(tv_df.loc[tv_df[\"Reject Null Hypothesis (TV)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_kl = set(kl_df.loc[kl_df[\"Reject Null Hypothesis (KL)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_sinkhorn = set(sinkhorn_df.loc[sinkhorn_df[\"Reject Null Hypothesis (SINKHOR)\"] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "# Genes with at least one metric showing \"No\"\n",
    "genes_at_least_one_no = genes_no_tv.union(genes_no_kl).union(genes_no_sinkhorn)\n",
    "\n",
    "# Genes with at least two metrics showing \"No\"\n",
    "genes_at_least_two_no = (\n",
    "    (genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn)\n",
    ")\n",
    "\n",
    "# Genes with all three metrics showing \"No\"\n",
    "genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "# Prepare dataframes for easy viewing and saving\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Criteria\": [\n",
    "        \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "    ],\n",
    "    \"Genes\": [\n",
    "        \", \".join(sorted(genes_at_least_one_no)),\n",
    "        \", \".join(sorted(genes_at_least_two_no)),\n",
    "        \", \".join(sorted(genes_all_three_no))\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSummary of Genes by Null Hypothesis Rejection:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_csv_path = os.path.join(output_dir, \"genes_null_hypothesis_summary.csv\")\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary saved to {summary_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Venn diagram\n",
    "plt.figure(figsize=(12, 8))\n",
    "venn = venn3(\n",
    "    [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "    set_labels=('TV', 'KL', 'SINKHORN')\n",
    ")\n",
    "\n",
    "plt.title(\"Genes NOT Rejecting Null Hypothesis\", fontsize=16)\n",
    "\n",
    "# Additional summaries (positions adjusted)\n",
    "x_pos = 0.6  # Move further to the right\n",
    "y_pos = 0.6\n",
    "step = 0.07\n",
    "\n",
    "plt.text(x_pos, y_pos, f\"Genes in at least 1 metric: {len(genes_no_tv | genes_no_kl | genes_no_sinkhorn)}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - step, f\"Genes in at least 2 metrics: {len((genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn))}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_no_tv & genes_no_kl & genes_no_sinkhorn)}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "venn_path = os.path.join(output_dir, \"genes_venn_diagram.png\")\n",
    "plt.savefig(venn_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Venn diagram saved to {venn_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0eda51-abb4-453f-a59b-d0779d2ae090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing distribution plots based on the gene sets which have similar distributions\n",
    "\n",
    "## PDF combination for comparison distributions \n",
    "## save the gene expression dynamics png as pdf\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/KDE_Intermediate_Only_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = ['DSP', 'ENPP5', 'EPB41L5', 'KRTCAP3', 'MMP2', 'RAB25', 'SERINC2', 'TMEM45B']  # List of genes \n",
    "## Selected genes (no difference by TV)  ['AXL', 'HNMT', 'TMEM45B', 'SSH3', 'SHROOM3', 'PRSS22', 'SERINC2', 'EVPL', 'GALNT3', 'DSP', 'ELMO3', 'KRTCAP3', 'KRT19', 'C1orf116', 'CDS1', 'INADL']\n",
    "## Selected genes (no difference by TV and KL) ['HNMT', 'TMEM45B', 'SHROOM3', 'PRSS22', 'SERINC2', 'KRTCAP3', 'C1orf116', 'CDS1']  \n",
    "pdf_path = f\"{output_dir}/selected_gene_expression_KDE_Intermediate_Only(two metrics).pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64d61d-8af3-4348-be9d-e912ceb0f268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
