{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55cb0ad",
   "metadata": {},
   "source": [
    "# GPA for transportation of gene expression datasets\n",
    "\n",
    "This notebook aims to apply GPA (our baseline model) to recover the trajectory of an empirical (EMT) gene expression dataset with unknown dynamics.\n",
    "We first reduce the dimensionality of the original data (175) to an accessible dimension through PCA, then apply GPA in the latent space, and reconstruct the result to the original high-dimensional space.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "* 6 snapshots at different timepoints (day 0, 1, 2, 3, 4, 8)\n",
    "* samplesize: differ by days\n",
    "* dimension: 175\n",
    "* The trajectory will be visualized in 2D principal component axes\n",
    "\n",
    "## Functionality\n",
    "\n",
    "* Given source and target days, GPA transports the source dataset toward the target dataset from a deterministic particle dynamics for the gradient flow of (Lipschitz regularized or limited transportation speed) KL divergergence.\n",
    "$$D_{KL}^L(P\\|Q) = \\sup_{\\| \\nabla \\phi \\| \\leq L} \\left \\{\\mathbb{E}_P[\\phi] - \\log \\mathbb{E}_Q [\\exp(\\phi)] \\right \\}$$\n",
    "$$\\partial_t P + \\nabla \\cdot \\left(P v\\right) = \\partial_t P - \\nabla \\cdot \\left(P \\nabla \\phi \\right) = 0$$\n",
    "$$\\dot{X} = - \\nabla \\phi_t(X)$$\n",
    "\n",
    "* Snapshots at all the intermediate points will be highlighted by default, but it can be specified.\n",
    "* $W_2$ distance will be calculated between the particle trajectory and the snapshots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc474107",
   "metadata": {},
   "source": [
    "## Transport source to target using GPA\n",
    "* Put ```--dataset Transport_genes -N_dim (d_red) --label (day1) (day2) --reduction (dim_red_method) --exp_no (exp_memo)``` and additional GPA training parameters that you want to temporarily change for this experiment. \n",
    "* Default GPA parameters and hyper-parameters are listed in ```configs/Transport_genes-GPA_NN.yaml```.\n",
    "* Recommended tunable parameters depending on examples are ```--epochs``` for number of transportation steps, ```--lr_P``` for time step size for the ODE, ```--epochs_phi``` for number of gradient ascent updates for $\\phi$, ```--lr_phi``` gradient ascent stepsize for $\\phi$.\n",
    "* For the loss function, \n",
    "    * the default setting is the Lipschitz regularized KL divergence in Donsker-Varadhan (DV) formulation; ```--f KL --formulation DV -L 1.0``` \n",
    "    * But if KL divergence is not suitable for the loss, we can try another Lipschitz regularized $f$-divergence in Legendre transform (LT) formulation; <br> ex) ```--f alpha -alpha 2.0 --formulation LT -L 1.0```\n",
    "* Lipschitz constant $L$ is an algorithm level parameter. ```-L 1.0``` makes the algorithm stable in general. \n",
    "* `--` tag means required parameter, `-` tag means optional parameter defined in ```scripts/util/input_args.py```.\n",
    "* The results will be saved in ```assets/Transport_genes/```. Results include \n",
    "    * movie for the evolving particles in the designated 2 axes ```plot_axes``` and ```plot_axes_join```\n",
    "    * movie for the evolving vector field in the designated 2 axes ```plot_axes``` and ```plot_axes_join```\n",
    "    * trajectories of particles in time in pickle format.\n",
    "    \n",
    "The video of resulting GPA trajectory in the latent space is stored in ```assets/Transport_genes/``` directory as gif format file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein-2 distance\n",
    "import torch # for W2 calculation purpose\n",
    "try:\n",
    "    from geomloss import SamplesLoss\n",
    "except:\n",
    "    !pip3 install geomloss\n",
    "    from geomloss import SamplesLoss\n",
    "\n",
    "def W2(X, Y):\n",
    "    X = torch.from_numpy(X).type(torch.float32)\n",
    "    Y = torch.from_numpy(Y).type(torch.float32)\n",
    "\n",
    "    return SamplesLoss(loss='sinkhorn', p=2)(X, Y).numpy()\n",
    "\n",
    "\n",
    "# conversion from gpa iterations to algorithm time \n",
    "def calculate_time_steps(dt, iter_nos, physical_time=True):\n",
    "    if physical_time == True:\n",
    "        iter_nos_new = []\n",
    "        if type(dt) == list: # decaying or varying dt\n",
    "            for iter_no in iter_nos:\n",
    "                if iter_no == 0:\n",
    "                    iter_nos_new.append(0)\n",
    "                else:\n",
    "                    iter_nos_new.append(sum(dt[:iter_no]))\n",
    "        else: # constant dt\n",
    "            for iter_no in iter_nos:\n",
    "                iter_nos_new.append(dt*iter_no)\n",
    "        iter_nos = iter_nos_new\n",
    "    return iter_nos\n",
    "\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcf60c-2754-48db-a4be-e7624319646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply GPA\n",
    "def run(source_t, target_t, intermediate_t = [], \n",
    "        total_iter = 10000, termination_error = 0.01, termination_error_metric = 'KE',\n",
    "        d_red=32, exp_memo = '2'):\n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "    # GPA parameters\n",
    "    f, alpha, L = 'KL', None, 1.0\n",
    "    #f, alpha, L = 'alpha', 2.0, 1.0\n",
    "    epochs = total_iter # for sample run\n",
    "    lr_P = 0.0025\n",
    "    plot_axes = '0 1' # 2 axes for plotting in 2D\n",
    "    plot_axes_join = '0_1'\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "\n",
    "    result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "    if f == 'KL':\n",
    "        f_Lip = 'KL-Lipschitz_%.4f' % L\n",
    "    else:\n",
    "        f_Lip = 'alpha=%05.2f-Lipschitz_%.4f' % (alpha, L)\n",
    "    result_name = f_Lip + '-%d_%dtimes-%s_dim%d_%04d_%04d_00_%s' % \\\n",
    "    (day1, day2, dim_red_method, d_red, N_target, N_source, exp_memo)\n",
    "    print(result_name)\n",
    "    result_filepath = result_dir + result_name + '.pickle'\n",
    "    \n",
    "    \n",
    "    # run GPA\n",
    "    if os.path.exists(result_filepath): # load existing result\n",
    "       with open(result_filepath, \"rb\") as fr:\n",
    "            param, result = pk.load(fr)\n",
    "            X1_trpts = result['trajectories']\n",
    "    else:  # run GPA\n",
    "        if f == 'KL':\n",
    "            !python3 $main_dir/main.py --dataset Transport_genes -L $L --label $day1 $day2 \\\n",
    "            --reduction $dim_red_method -N_dim $d_red --exp_no $exp_memo --f $f \\\n",
    "            --formulation DV --epochs $epochs \\\n",
    "            -termination_error $termination_error -termination_error_metric $termination_error_metric\\\n",
    "            -plot_axes $plot_axes --lr_P $lr_P\n",
    "        else:\n",
    "            !python3 $main_dir/main.py --dataset Transport_genes -L $L --label $day1 $day2 \\\n",
    "            --reduction $dim_red_method -N_dim $d_red --exp_no $exp_memo --f $f -alpha $alpha \\\n",
    "            --formulation LT --epochs $epochs \\\n",
    "            -termination_error $termination_error -termination_error_metric $termination_error_metric\\\n",
    "            -plot_axes $plot_axes --lr_P $lr_P\n",
    "    \n",
    "    \n",
    "    #img_src = result_dir + result_name + '-%s-movie.gif' % plot_axes_join\n",
    "    #display(Image(filename = img_src))\n",
    "\n",
    "    \n",
    "    # read the result\n",
    "    with open(result_filepath, \"rb\") as fr:\n",
    "        param, result = pk.load(fr)\n",
    "        X1_trpts = result['trajectories']\n",
    "        \n",
    "    # epochs(=iteration counts) -> physical time\n",
    "    ts = np.arange(1, len(result['trajectories'])*param['save_iter']+1, param['save_iter'])\n",
    "    print(\"Number of epochs: \", len(result['trajectories'])*param['save_iter'], \", time step size: \", param['lr_P'])\n",
    "    pts = param['lr_P'] * ts\n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Visualization in the original space \n",
    "    img_src = result_dir + result_name + '-movie-original.gif' \n",
    "    if os.path.exists(img_src): # try loading saved movie\n",
    "        display(Image(filename = img_src))\n",
    "    else: # draw, save and load\n",
    "        fig, ax = plt.subplots()\n",
    "        ims = []\n",
    "        X1_vis = reducer.transform(mats[day1])\n",
    "        X2_vis = reducer.transform(mats[day2])\n",
    "        im = ax.scatter(X1_vis[:,0], X1_vis[:,1], color=colors[day1], \n",
    "                        alpha=1.0, s=0.7, zorder=10, label=f'day {day1}') # source in red\n",
    "        ax.scatter(X2_vis[:,0], X2_vis[:,1], color=colors[day2], \n",
    "                   alpha=0.7, s=0.7, zorder=7, label=f'day {day2}') # target in blue\n",
    "        for t in intermediate_t:\n",
    "            X1_intermediate_vis = reducer.transform(mats[t])\n",
    "            ax.scatter(X1_intermediate_vis[:,0], X1_intermediate_vis[:,1], color=colors[t], \n",
    "                        alpha=0.5, s=0.7, zorder=5, label=f'day {t}')\n",
    "        ax.scatter(vis_all_days[:,0], vis_all_days[:,1], color='lightgray', \n",
    "                   alpha=0.3, s=0.7, zorder=1) # backgrounds in gray\n",
    "        #leg = ax.legend(loc='upper right')\n",
    "        ttl = ax.text(0.5,1.05, \"t = %.3f\" % 0.0, \\\n",
    "                      bbox={'facecolor':'w', 'alpha':0.5, 'pad':5}, \\\n",
    "                      transform=ax.transAxes, ha=\"center\")\n",
    "        ims.append([im, ttl])#, leg])\n",
    "        for i, X1_trpt in enumerate(X1_trpts):  # trajectories\n",
    "            if np.isnan(X1_trpt).any():\n",
    "                break\n",
    "            X1_hat = pca.inverse_transform(X1_trpt)\n",
    "            X1_hat_vis = reducer.transform(X1_hat)\n",
    "            im = ax.scatter(X1_hat_vis[:,0], X1_hat_vis[:,1], color=colors[day1], \n",
    "                            alpha=1.0, s=0.7, zorder=10, label=f'day {day1}') # transported source \n",
    "            ax.scatter(X2_vis[:,0], X2_vis[:,1], color=colors[day2], \n",
    "                       alpha=0.7, s=0.7, zorder=7, label=f'day {day2}') # target\n",
    "            ax.scatter(X1_intermediate_vis[:,0], X1_intermediate_vis[:,1], color=colors[day1+1], \n",
    "                        alpha=0.5, s=0.7, zorder=5, label=f'day {day1+1}')\n",
    "            ax.scatter(vis_all_days[:,0], vis_all_days[:,1], color='lightgray', \n",
    "                       alpha=0.3, s=0.7, zorder=1) # backgrounds in gray\n",
    "            #leg = ax.legend(loc='upper right')\n",
    "            ttl = ax.text(0.5,1.05, \"t = %.3f\" % pts[i], \\\n",
    "                          bbox={'facecolor':'w', 'alpha':0.5, 'pad':5}, \\\n",
    "                          transform=ax.transAxes, ha=\"center\")\n",
    "            ims.append([im, ttl])#, leg])\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=200)\n",
    "        writergif = animation.PillowWriter(fps=3)\n",
    "        ani.save(img_src, writer=writergif)\n",
    "        plt.clf()\n",
    "        display(Image(filename = img_src))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88945bd-02aa-4534-bc79-0cf1b093a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synthetic data\n",
    "\n",
    "source_t, target_t = 0, 2\n",
    "run(source_t, target_t, intermediate_t = [], total_iter = 5000, \n",
    "    termination_error = 1e-4, termination_error_metric = 'f_Lip', # termination_error_metric from ['f_Lip', 'KE']\n",
    "    d_red=26, exp_memo='Data_4_T0_2_26d_lr_P_0.04_5000i_f_Lip_1e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56152165-a983-40d0-af05-4ff311e9f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem cell differentiaion data\n",
    "\n",
    "source_t, target_t = 0, 2\n",
    "run(source_t, target_t, intermediate_t = [], total_iter = 5000, \n",
    "    termination_error = 1e-4, termination_error_metric = 'f_Lip', # termination_error_metric from ['f_Lip', 'KE']\n",
    "    d_red=4, exp_memo='Data_0_T0_2_4d_lr_P_0.0025_5000i_f_Lip_1e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6eebc-a124-499e-b988-c33e8642235f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Clinical data\n",
    "\n",
    "source_t, target_t = 0, 4\n",
    "run(source_t, target_t, intermediate_t = [], total_iter = 5000, \n",
    "    termination_error = 2e-5, termination_error_metric = 'f_Lip', # termination_error_metric from ['f_Lip', 'KE']\n",
    "    d_red=2, exp_memo='Palbo_887_nofibroblast_malignant_Rgene_T0_4_2d_lr_P_0.0025_5000i_f_Lip_2e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ce004-96ce-4f19-8279-654814b36f35",
   "metadata": {},
   "source": [
    "## Time-dependent vectorfield\n",
    "\n",
    "* Step 1: For each subinterval of time, run original GPA from the cell above\n",
    "* Step 2: Train time-dependent vectorfield by running scripts/train_time_dep_vectorfields.py\n",
    "  e.g. python3 train_time_dep_vectorfields.py --dataset Transport_genes --ts 0 2 4 --files KL-Lipschitz_1.0000-0_2times-EMT_PCA_dim64_2381_2734_00_1.pickle KL-Lipschitz_1.0000-2_4times-EMT_PCA_dim64_1147_2381_00_1.pickle --hidden_units 64 64 64 64 --exp_memo test1\n",
    "* Step 3: Evaluate sample trajectory by running below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "def load_W(filename):\n",
    "    with open(filename, \"rb\") as fr:\n",
    "        W, b, p = pk.load(fr)\n",
    "\n",
    "    W = [tf.Variable(w,  dtype=tf.float32) for w in W]\n",
    "    b = [tf.Variable(b_,  dtype=tf.float32) for b_ in b]\n",
    "\n",
    "    return W, b, p\n",
    "\n",
    "def v(x, t, W, b):   # neural newtork for time-dependent vectorfield\n",
    "    num_layers = len(W)\n",
    "    activation_ftn = tf.nn.tanh\n",
    "        \n",
    "    h = tf.concat([x, t*tf.ones([x.shape[0], 1], dtype=tf.float32)], axis=1)\n",
    "    for l in range(0,num_layers-1):\n",
    "        h = activation_ftn(tf.add(tf.matmul(h, W[l]), b[l]))\n",
    "    out=tf.add(tf.matmul(h, W[-1]), b[-1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def time_integration(x0, T, dt):\n",
    "    x = tf.constant(x0, dtype=tf.float32)\n",
    "    xs = [x0]\n",
    "    for i in range(int(T/dt)):\n",
    "        vv = v(x, dt*i, W, b)\n",
    "        x += dt * vv\n",
    "        xs.append(x.numpy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def generate_animation(days, intermediate_days, X1_trpts, dt, physical_dt, img_src, d_red = 2, vs = None):\n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}    \n",
    "    \n",
    "    \n",
    "    for i, X1_trpt in enumerate(X1_trpts):  # trajectories\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        X1_trpt_vis = X1_trpt\n",
    "        \n",
    "        if type(vs) != type(None) and i < len(X1_trpts)-1:\n",
    "            X1_trpt_vis_next = X1_trpts[i+1]\n",
    "            vs_vis = (X1_trpt_vis_next-X1_trpt_vis) / dt\n",
    "            im = ax.quiver(X1_trpt_vis[:, 0], X1_trpt_vis[:, 1], vs_vis[:, 0], vs_vis[:, 1], \n",
    "                           width=0.003, headwidth=7, headlength=15, headaxislength=7, zorder=15)                            \n",
    "        else:\n",
    "            im = ax.scatter(X1_trpt_vis[:,0], X1_trpt_vis[:,1], color=colors[days[0]], \n",
    "                            alpha=1.0, s=0.7, zorder=10, label=f'day {days[0]}') # transported source \n",
    "            for t in days[1:]:\n",
    "                X2_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X2_vis[:,0], X2_vis[:,1], color=colors[t], \n",
    "                   alpha=1.0, s=0.7, zorder=5, label=f'day {t}') # target \n",
    "            \n",
    "            for t in intermediate_days:\n",
    "                X1_intermediate_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X1_intermediate_vis[:,0], X1_intermediate_vis[:,1], color='lightgray', \n",
    "                        alpha=0.3, s=0.7, zorder=1, label=f'day {t}')\n",
    "        #ax.set_xlim([-5,6])\n",
    "        #ax.set_ylim([-5,6])\n",
    "        \n",
    "        #leg = ax.legend(loc='upper right')\n",
    "        ttl = ax.text(0.5,1.05, \"t = %.3f\" % (physical_dt*i), \\\n",
    "                      bbox={'facecolor':'w', 'alpha':0.5, 'pad':5}, \\\n",
    "                      transform=ax.transAxes, ha=\"center\")\n",
    "        ims.append([im, ttl])#, leg])\n",
    "        \n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=200)\n",
    "    writergif = animation.PillowWriter(fps=3)\n",
    "    ani.save(img_src, writer=writergif)\n",
    "    plt.clf()\n",
    "    display(Image(filename = img_src))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3201aa-0747-44cf-837f-9310eef4786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for piecewise GPA (sample 3 - stem cell and sample 5 - synthetic)\n",
    "\n",
    "\n",
    "## Static plot function for piecewise GPA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_three_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#2ca02c',  # Green\n",
    "        middle_t: '#ff7f0e',  # Orange\n",
    "        intermediate_days[1]: '#8c564b',  # Brown\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax1.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=5, zorder = 1)\n",
    "\n",
    "    \n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=24)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=24)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax2.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "    # Create legend with smaller markers and tighter spacing\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles,\n",
    "        sorted_labels,\n",
    "        fontsize=20,         # Font size of text\n",
    "        loc='center',\n",
    "        ncol=len(sorted_labels),\n",
    "        markerscale=2,     # Scale down marker size in legend\n",
    "        handlelength=1.5,    # Length of the marker line\n",
    "        handletextpad=0.2    # Padding between marker and label text\n",
    "    )    \n",
    "\n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d6d3-f9f8-4757-b381-2442975fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (Sample 1 - EMT data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.0, alpha=0.75, s=10, zorder = 20,  label=f'Day {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=27)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=27)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=30)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=27)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "        \n",
    "    # Create legend with larger markers for scatter plots\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "        ncol=len(sorted_labels), markerscale=2)\n",
    "    \n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b79cf-56f9-44f8-b98e-77e37b64fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=20)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Proceed only if legend elements exist\n",
    "    if handles and labels:\n",
    "        # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "        time_labels = []\n",
    "        for label in labels:\n",
    "            try:\n",
    "                time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "                time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "            except ValueError:\n",
    "                time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "        # Sort legend by time values\n",
    "        time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "        sorted_labels = [item[1] for item in time_labels]\n",
    "        sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "        # **Increase marker size in legend**\n",
    "        for handle in sorted_handles:\n",
    "            if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "                handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "        # Create a separate figure for the legend\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "        ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "        # Create legend with larger markers for scatter plots\n",
    "        legend = ax_legend.legend(\n",
    "            sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "            ncol=len(sorted_labels), markerscale=6  # Increase scatter marker size\n",
    "        )\n",
    "    \n",
    "        # Save the legend separately\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "    else:\n",
    "        print(\"No legend elements found â€” skipping separate legend plot.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b995d3-f5f3-4e68-b20f-ec1800daae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data) - separate legend\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days, intermediate_days, X1_trpts, mats, d_red=26,\n",
    "    output_file_with_snapshots=None,\n",
    "    output_file_without_snapshots=None,\n",
    "    output_file_snapshots_only=None\n",
    "):\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    source_t, _, target_t = days[0], days[1], days[-1]\n",
    "\n",
    "    color_map = {\n",
    "        source_t: 'magenta',  # Blue\n",
    "        target_t: '#008080'   # Red\n",
    "    }\n",
    "\n",
    "    # --- Plot 1: WITH snapshots (No inline colorbar) ---\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder=1)\n",
    "\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax1.tick_params(axis='both', labelsize=32)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Save colorbar separately ---\n",
    "    fig_cb, ax_cb = plt.subplots(figsize=(10, 1))\n",
    "    cb = plt.colorbar(sm, cax=ax_cb, orientation='horizontal')\n",
    "    cb.set_ticks([0, 4])\n",
    "    cb.set_ticklabels([\"Pre-treatment\", \"Post-treatment\"])\n",
    "    cb.ax.tick_params(labelsize=24)\n",
    "    cb.set_label(\"Time\", fontsize=24)\n",
    "    cb_path = os.path.join(result_dir, \"trajectory_colorbar_only.png\")\n",
    "    fig_cb.savefig(cb_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_cb)\n",
    "    print(f\"Standalone colorbar saved to {cb_path}\")\n",
    "\n",
    "    # --- Plot 2: WITHOUT snapshots ---\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8, zorder=15, label='Pre-treatment')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8, zorder=10, label='Post-treatment')\n",
    "\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax2.tick_params(axis='both', labelsize=32)\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Legend (just for pre/post-treatment) ---\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(6, 2))\n",
    "        ax_legend.axis(\"off\")\n",
    "        ax_legend.legend(\n",
    "            handles, labels, fontsize=16, loc='center',\n",
    "            ncol=len(labels), markerscale=3, handletextpad=0.5\n",
    "        )\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close(fig_legend)\n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a373d4-d7ed-4142-8b91-9b5ad8633521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136033ed-955a-43c7-849c-6ee8c75c3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c365c-319c-4eff-bf24-67435d1f6137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6f201-8cf9-44a7-8e30-05d38a306d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594032d-8820-4228-b82d-0f576845ca33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e85a9-2644-4dae-a799-d814c319b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6faa1-cae0-45a1-ae38-eec202c26b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819b6a5-bd12-46e5-936a-4dc7f39da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (samples 1 - EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[2],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75def9f-4e12-4edb-98a5-648bb7d36b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (Sample 5 - synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636e70b-e41c-4e1b-977a-1684daa4e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (samples 3 - stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc9293-f90b-424c-988f-01d52bb13a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154d2ad-1e37-4b94-8b38-b70df9d4706e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a20c9-19ef-4817-b018-55ee0ed5174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85b29a-ff95-4834-86aa-eeaafb43ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21ad8-a556-4f35-86d4-49068a3a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_cell_types(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_cell_type_source=None, output_file_cell_type_target=None, output_file_cell_type_legend=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "\n",
    "    # Same PCA transformation and cell type extraction as before\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    cell_types_X1 = cell_types_by_day[source_t]\n",
    "    cell_types_X2 = cell_types_by_day[target_t]\n",
    "    unique_cell_types = np.unique(np.concatenate([cell_types_X1, cell_types_X2]))\n",
    "    cell_type_palette = dict(zip(unique_cell_types, sns.color_palette(\"tab20\", len(unique_cell_types))))\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 1: X1 colored, X2 gray (no legend)\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X1 == cell_type\n",
    "        ax1.scatter(X1_vis[idx, 0], X1_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax1.set_title(f\"Untreated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_source:\n",
    "        plt.savefig(output_file_cell_type_source, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 2: X2 colored, X1 gray (no legend)\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X2 == cell_type\n",
    "        ax2.scatter(X2_vis[idx, 0], X2_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax2.set_title(f\"Treated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_target:\n",
    "        plt.savefig(output_file_cell_type_target, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    # Use circle markers instead of patches for legend\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create circle markers for legend entries\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create figure and axis (just for the legend)\n",
    "    fig_leg, ax_leg = plt.subplots()\n",
    "    fig_leg.set_figwidth(8)  # Initial size; will be adjusted\n",
    "    fig_leg.set_figheight(6)\n",
    "    \n",
    "    # Hide axes\n",
    "    ax_leg.axis('off')\n",
    "    \n",
    "    # Add legend to axis (not directly to plt)\n",
    "    legend = ax_leg.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='center',\n",
    "        frameon=True,\n",
    "        fontsize=14,\n",
    "        ncol=1,\n",
    "        title='Cell Types',\n",
    "        title_fontsize=14,\n",
    "        borderpad=1\n",
    "    )\n",
    "    \n",
    "    # Resize the figure to tightly fit the legend\n",
    "    fig_leg.canvas.draw()\n",
    "    bbox = legend.get_window_extent().transformed(fig_leg.dpi_scale_trans.inverted())\n",
    "    fig_leg.set_size_inches(bbox.width + 0.5, bbox.height + 0.5)  # Add a little padding\n",
    "    \n",
    "    # Save only, no display\n",
    "    if output_file_cell_type_legend:\n",
    "        plt.savefig(output_file_cell_type_legend, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_leg)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baaf34-9a34-442b-aee1-171ebe303d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell types plots\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_dim20-f_Lip=5e-3-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 20\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_cell_type_source = f\"{result_dir}{exp_name}_cell_type_source.png\"\n",
    "output_file_cell_type_target = f\"{result_dir}{exp_name}_cell_type_target.png\"\n",
    "output_file_cell_type_legend = f\"{result_dir}{exp_name}_cell_type_legend.png\"\n",
    "\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_cell_types(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_cell_type_source=output_file_cell_type_source,\n",
    "    output_file_cell_type_target=output_file_cell_type_target,\n",
    "    output_file_cell_type_legend=output_file_cell_type_legend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbdc0b-7311-4b87-8c3b-1bd0f7ba27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantify the errors by using W2 distance (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different.\n",
    "\n",
    "    Parameters:\n",
    "    - source_t: Start time point (not included in the plot)\n",
    "    - target_t: End time point (not included in the plot)\n",
    "    - optimal_k: Number of clusters for KMeans\n",
    "    - gene_of_interest: The gene whose expression is analyzed\n",
    "    - index: Step size for trajectory extraction\n",
    "    - max_i: Maximum index for trajectory extraction\n",
    "    - intermediate_t: List of intermediate time points (defaults to [1] if not provided)\n",
    "    - d_red: Dimensionality reduction method\n",
    "    - random_state: Random seed\n",
    "    - exp_memo: Experiment identifier\n",
    "    - num_permutations: Number of permutations for significance testing.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2 distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure intermediate_t has a default value\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute the observed W2 distance\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2_distances = []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  # Shuffle data\n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2 = wasserstein_distance(perm_test_sample, perm_pred_sample)\n",
    "            permuted_w2_distances.append(permuted_w2)\n",
    "\n",
    "        # Compute p-value (proportion of permuted distances â‰¥ observed W2)\n",
    "        p_value = np.mean(np.array(permuted_w2_distances) >= observed_w2)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2,\n",
    "            \"Permutation Mean W2\": np.mean(permuted_w2_distances),\n",
    "            \"Permutation Std W2\": np.std(permuted_w2_distances),\n",
    "            \"p-value\": p_value\n",
    "        }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Permutation Test Summary ---\")\n",
    "    for time, results in permutation_results.items():\n",
    "        print(f\"Time {time}:\")\n",
    "        print(f\"  Observed W2: {results['Observed W2']:.4f}\")\n",
    "        print(f\"  Mean Permutation W2: {results['Permutation Mean W2']:.4f} Â± {results['Permutation Std W2']:.4f}\")\n",
    "        print(f\"  p-value: {results['p-value']:.4f}\")\n",
    "        if results[\"p-value\"] < 0.05:\n",
    "            print(\"  ** Significant Difference (Reject Null Hypothesis) **\")\n",
    "        else:\n",
    "            print(\"  No Significant Difference (Cannot Reject Null Hypothesis)\")\n",
    "\n",
    "    return permutation_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843bf8b-a5c9-4fa3-943c-2f647a9426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics but No Sinkhorn (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def maximum_mean_discrepancy(X, Y, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) between two distributions using an RBF kernel.\n",
    "    \"\"\"\n",
    "    K_xx = rbf_kernel(X[:, None], X[:, None], gamma=gamma)\n",
    "    K_yy = rbf_kernel(Y[:, None], Y[:, None], gamma=gamma)\n",
    "    K_xy = rbf_kernel(X[:, None], Y[:, None], gamma=gamma)\n",
    "\n",
    "    return K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000, mmd_gamma=1.0):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different using W2 and MMD.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2, MMD distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute observed metrics\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        observed_mmd = maximum_mean_discrepancy(test_vals, predicted_vals, gamma=mmd_gamma)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2, permuted_mmd = [], []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  \n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2.append(wasserstein_distance(perm_test_sample, perm_pred_sample))\n",
    "            permuted_mmd.append(maximum_mean_discrepancy(perm_test_sample, perm_pred_sample, gamma=mmd_gamma))\n",
    "\n",
    "        # Compute p-values\n",
    "        p_w2 = np.mean(np.array(permuted_w2) >= observed_w2)\n",
    "        p_mmd = np.mean(np.array(permuted_mmd) >= observed_mmd)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2, \"p_W2\": p_w2,\n",
    "            \"Observed MMD\": observed_mmd, \"p_MMD\": p_mmd\n",
    "        }\n",
    "\n",
    "    return permutation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3d5c8-69ed-464f-85b8-8f7d53d31c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics With Sinkhorn (permutation test)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from geomloss import SamplesLoss  # Sinkhorn divergence\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with entropy regularization.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "\n",
    "def permutation_test_sinkhorn(test_vals, pred_vals, num_permutations=1000, epsilon=0.5):\n",
    "    \"\"\"Permutation test for Sinkhorn divergence.\"\"\"\n",
    "    observed_stat = sinkhorn_divergence(test_vals, pred_vals, epsilon)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        perm_stat = sinkhorn_divergence(perm_test_sample, perm_pred_sample, epsilon)\n",
    "        permuted_stats.append(perm_stat)\n",
    "\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Sinkhorn(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                  intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                  num_permutations=1000, sinkhorn_epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with correct scaling and permutation test.\"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping method not available.\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correct scaling\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    predicted_data = []\n",
    "    for snapshot_idx in scaled_intermediate_indices:\n",
    "        if snapshot_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[snapshot_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        predicted_vals = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_data.append(predicted_vals)\n",
    "\n",
    "    results = []\n",
    "    for time, test_vals, pred_vals in zip(intermediate_t, test_data, predicted_data):\n",
    "\n",
    "        # Match sample sizes\n",
    "        min_size = min(len(test_vals), len(pred_vals))\n",
    "        test_vals, pred_vals = resample(test_vals, n_samples=min_size, random_state=42), \\\n",
    "                               resample(pred_vals, n_samples=min_size, random_state=42)\n",
    "\n",
    "        # Compute Sinkhorn and permutation test\n",
    "        sinkhorn_stat, p_sinkhorn = permutation_test_sinkhorn(\n",
    "            test_vals, pred_vals, num_permutations, sinkhorn_epsilon\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"Sinkhorn Divergence\": sinkhorn_stat,\n",
    "            \"Sinkhorn p-value\": p_sinkhorn\n",
    "        })\n",
    "\n",
    "    output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(output_dir, f\"Sinkhorn_metrics_{gene_of_interest}.csv\")\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd419e-09fe-4e6a-bd6b-cd0ddc14afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 1\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean Â± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, and Jensen-Shannon).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    intermediate_only_points = intermediate_t  \n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "        \n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db53770-527b-4c6e-9212-b23285fb8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 3\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean Â± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, JS, and Mean Comparison).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correctly scale the intermediate time points\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions using scaled indices\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "\n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Mean and Standard Deviation Comparison**\n",
    "        mean_test = np.mean(test_vals)\n",
    "        mean_pred = np.mean(predicted_vals)\n",
    "        std_test = np.std(test_vals)\n",
    "        std_pred = np.std(predicted_vals)\n",
    "        mean_diff = mean_test - mean_pred\n",
    "        std_diff = std_test - std_pred\n",
    "\n",
    "        # Perform a t-test to compare means\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Test\": mean_test,\n",
    "            \"Mean Predicted\": mean_pred,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Test\": std_test,\n",
    "            \"Standard Deviation Predicted\": std_pred,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b6e27-ef58-47a8-8ee5-76b66bc842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Permuation by statistical tests (permutation test) - stem cell data\n",
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p = np.clip(p, 1e-10, None)\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    min_size = min(len(X), len(Y))\n",
    "    return resample(X, n_samples=min_size, random_state=42), resample(Y, n_samples=min_size, random_state=42)\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence using PyTorch and geomloss.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "        permuted_stats.append(stat_func(perm_test_sample, perm_pred_sample))\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"âŒ PCA method not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    scaling_factor = len(X1_trpts) / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    kde_test_data = [pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t]\n",
    "\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            continue\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    metric_results = []\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)\n",
    "\n",
    "        hist_test = np.histogram(test_vals, bins=50, density=True)[0]\n",
    "        hist_pred = np.histogram(predicted_vals, bins=50, density=True)[0]\n",
    "\n",
    "        tv = total_variation_distance(hist_test, hist_pred)\n",
    "        kl = kl_divergence(hist_test, hist_pred)\n",
    "        js = jensenshannon(hist_test, hist_pred)\n",
    "        w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        sink = sinkhorn_divergence(test_vals, predicted_vals)\n",
    "\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "        perm_w2, mean_perm_w2, std_perm_w2, p_w2 = permutation_test(wasserstein_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_sink, mean_perm_sink, std_perm_sink, p_sink = permutation_test(sinkhorn_divergence, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        mean_diff = np.mean(test_vals) - np.mean(predicted_vals)\n",
    "        std_diff = np.std(test_vals) - np.std(predicted_vals)\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"W2 Distance\": w2,\n",
    "            \"Permutation W2 Â± Std\": f\"{mean_perm_w2:.4f} Â± {std_perm_w2:.4f}\",\n",
    "            \"p-value W2\": p_w2,\n",
    "            \"Sinkhorn Distance\": sink,\n",
    "            \"Permutation Sinkhorn Â± Std\": f\"{mean_perm_sink:.4f} Â± {std_perm_sink:.4f}\",\n",
    "            \"p-value Sinkhorn\": p_sink,\n",
    "            \"Total Variation Distance\": tv,\n",
    "            \"Permutation TV Â± Std\": f\"{mean_perm_tv:.4f} Â± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl,\n",
    "            \"Permutation KL Â± Std\": f\"{mean_perm_kl:.4f} Â± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js,\n",
    "            \"Permutation JS Â± Std\": f\"{mean_perm_js:.4f} Â± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ… Results saved to: {csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d023-eb24-44eb-b76b-07e018303b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = gene_names # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,3]  # Intermediate time points\n",
    "d_red = 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV Â± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL Â± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 Â± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn Â± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7eb23-a0b9-47c1-8458-c7d0d0a1761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Venn Diagram of null hypothesis results for Stem cell data\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"Sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Ensure consistent column names\n",
    "tv_col = \"Reject Null Hypothesis (TV)\"\n",
    "kl_col = \"Reject Null Hypothesis (KL)\"\n",
    "sinkhorn_col = \"Reject Null Hypothesis (Sinkhorn)\"\n",
    "\n",
    "# Identify unique time points\n",
    "time_points = sorted(tv_df[\"Time\"].unique())\n",
    "\n",
    "# Iterate over each time point\n",
    "for time_point in time_points:\n",
    "    # Filter DataFrames for the current time point\n",
    "    tv_time_df = tv_df[tv_df[\"Time\"] == time_point]\n",
    "    kl_time_df = kl_df[kl_df[\"Time\"] == time_point]\n",
    "    sinkhorn_time_df = sinkhorn_df[sinkhorn_df[\"Time\"] == time_point]\n",
    "\n",
    "    # Identify genes with \"No\" in each metric\n",
    "    genes_no_tv = set(tv_time_df.loc[tv_time_df[tv_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_kl = set(kl_time_df.loc[kl_time_df[kl_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_sinkhorn = set(sinkhorn_time_df.loc[sinkhorn_time_df[sinkhorn_col] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "    # Genes with at least one metric showing \"No\"\n",
    "    genes_at_least_one_no = genes_no_tv | genes_no_kl | genes_no_sinkhorn\n",
    "\n",
    "    # Genes with at least two metrics showing \"No\"\n",
    "    genes_at_least_two_no = (\n",
    "        (genes_no_tv & genes_no_kl) |\n",
    "        (genes_no_tv & genes_no_sinkhorn) |\n",
    "        (genes_no_kl & genes_no_sinkhorn)\n",
    "    )\n",
    "\n",
    "    # Genes with all three metrics showing \"No\"\n",
    "    genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "    # Prepare summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Criteria\": [\n",
    "            \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "        ],\n",
    "        \"Genes\": [\n",
    "            \", \".join(sorted(genes_at_least_one_no)),\n",
    "            \", \".join(sorted(genes_at_least_two_no)),\n",
    "            \", \".join(sorted(genes_all_three_no))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Print results for current time point\n",
    "    print(f\"\\nðŸ”¹ Summary of Genes by Null Hypothesis Rejection (Time {time_point}):\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary to CSV for current time point\n",
    "    summary_csv_path = os.path.join(output_dir, f\"genes_null_hypothesis_summary_time_{time_point}.csv\")\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ… Summary for Time {time_point} saved to {summary_csv_path}\")\n",
    "\n",
    "    # Create Venn diagram for current time point\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    venn = venn3(\n",
    "        [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "        set_labels=('TV', 'KL', 'Sinkhorn')\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Genes NOT Rejecting Null Hypothesis (Time {time_point})\", fontsize=16)\n",
    "\n",
    "    # Add summary annotations\n",
    "    x_pos = 0.6\n",
    "    y_pos = 0.6\n",
    "    step = 0.07\n",
    "\n",
    "    plt.text(x_pos, y_pos, f\"Genes in â‰¥1 metric: {len(genes_at_least_one_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - step, f\"Genes in â‰¥2 metrics: {len(genes_at_least_two_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_all_three_no)}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save Venn diagram\n",
    "    venn_path = os.path.join(output_dir, f\"genes_venn_diagram_time_{time_point}.png\")\n",
    "    plt.savefig(venn_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"âœ… Venn diagram for Time {time_point} saved to {venn_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f1e37-24d5-4466-ad0b-27793a954566",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = ['DSP'] # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]  # Intermediate time points\n",
    "d_red = 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV Â± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL Â± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 Â± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn Â± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc9409-fb78-4961-b9f2-9381d20689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining genes (preprocess)\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "csv_dir = os.path.join(result_dir, 'Sample 3')\n",
    "\n",
    "\n",
    "# List of prefixes\n",
    "prefixes = ['sinkhorn']\n",
    "\n",
    "# Mapping prefixes to their corresponding p-value column names\n",
    "pval_columns = {\n",
    "    'sinkhorn': 'p_Sinkhorn_1',\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through each prefix\n",
    "for prefix in prefixes:\n",
    "    # Use glob to find matching files\n",
    "    matching_files = glob.glob(os.path.join(csv_dir, f\"{prefix}*.csv\"))\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"No files found for prefix '{prefix}'\")\n",
    "        continue\n",
    "    \n",
    "    # Read and concatenate files\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in matching_files], ignore_index=True)\n",
    "\n",
    "    # Check if the p-value column exists\n",
    "    p_col = pval_columns[prefix]\n",
    "    if p_col in combined_df.columns:\n",
    "        combined_df[f'Reject Null Hypothesis ({prefix[:-1].upper()})'] = combined_df[p_col].apply(\n",
    "            lambda p: 'No' if p > 0.05 else 'Yes'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Column '{p_col}' not found in files for prefix '{prefix}'.\")\n",
    "    \n",
    "    # Save the combined dataframe to CSV\n",
    "    output_filename = os.path.join(csv_dir, f\"{prefix}_metric.csv\")\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"âœ… Combined {len(matching_files)} files into '{output_filename}' with hypothesis test results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f349b0a-a140-466f-a287-0c264f2808a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sample 1 (only one time point)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Identify genes with \"No\" in each metric\n",
    "genes_no_tv = set(tv_df.loc[tv_df[\"Reject Null Hypothesis (TV)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_kl = set(kl_df.loc[kl_df[\"Reject Null Hypothesis (KL)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_sinkhorn = set(sinkhorn_df.loc[sinkhorn_df[\"Reject Null Hypothesis (SINKHOR)\"] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "# Genes with at least one metric showing \"No\"\n",
    "genes_at_least_one_no = genes_no_tv.union(genes_no_kl).union(genes_no_sinkhorn)\n",
    "\n",
    "# Genes with at least two metrics showing \"No\"\n",
    "genes_at_least_two_no = (\n",
    "    (genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn)\n",
    ")\n",
    "\n",
    "# Genes with all three metrics showing \"No\"\n",
    "genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "# Prepare dataframes for easy viewing and saving\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Criteria\": [\n",
    "        \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "    ],\n",
    "    \"Genes\": [\n",
    "        \", \".join(sorted(genes_at_least_one_no)),\n",
    "        \", \".join(sorted(genes_at_least_two_no)),\n",
    "        \", \".join(sorted(genes_all_three_no))\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSummary of Genes by Null Hypothesis Rejection:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_csv_path = os.path.join(output_dir, \"genes_null_hypothesis_summary.csv\")\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary saved to {summary_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Venn diagram\n",
    "plt.figure(figsize=(12, 8))\n",
    "venn = venn3(\n",
    "    [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "    set_labels=('TV', 'KL', 'SINKHORN')\n",
    ")\n",
    "\n",
    "plt.title(\"Genes NOT Rejecting Null Hypothesis\", fontsize=16)\n",
    "\n",
    "# Additional summaries (positions adjusted)\n",
    "x_pos = 0.6  # Move further to the right\n",
    "y_pos = 0.6\n",
    "step = 0.07\n",
    "\n",
    "plt.text(x_pos, y_pos, f\"Genes in at least 1 metric: {len(genes_no_tv | genes_no_kl | genes_no_sinkhorn)}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - step, f\"Genes in at least 2 metrics: {len((genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn))}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_no_tv & genes_no_kl & genes_no_sinkhorn)}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "venn_path = os.path.join(output_dir, \"genes_venn_diagram.png\")\n",
    "plt.savefig(venn_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Venn diagram saved to {venn_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0eda51-abb4-453f-a59b-d0779d2ae090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing distribution plots based on the gene sets which have similar distributions\n",
    "\n",
    "## PDF combination for comparison distributions \n",
    "## save the gene expression dynamics png as pdf\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/KDE_Intermediate_Only_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"âœ… PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = ['DSP', 'ENPP5', 'EPB41L5', 'KRTCAP3', 'MMP2', 'RAB25', 'SERINC2', 'TMEM45B']  # List of genes \n",
    "## Selected genes (no difference by TV)  ['AXL', 'HNMT', 'TMEM45B', 'SSH3', 'SHROOM3', 'PRSS22', 'SERINC2', 'EVPL', 'GALNT3', 'DSP', 'ELMO3', 'KRTCAP3', 'KRT19', 'C1orf116', 'CDS1', 'INADL']\n",
    "## Selected genes (no difference by TV and KL) ['HNMT', 'TMEM45B', 'SHROOM3', 'PRSS22', 'SERINC2', 'KRTCAP3', 'C1orf116', 'CDS1']  \n",
    "pdf_path = f\"{output_dir}/selected_gene_expression_KDE_Intermediate_Only(two metrics).pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64d61d-8af3-4348-be9d-e912ceb0f268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
