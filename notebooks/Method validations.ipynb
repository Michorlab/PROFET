{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55cb0ad",
   "metadata": {},
   "source": [
    "# GPA for transportation of gene expression datasets\n",
    "\n",
    "This notebook aims to apply GPA (our baseline model) to recover the trajectory of an empirical (EMT) gene expression dataset with unknown dynamics.\n",
    "We first reduce the dimensionality of the original data (175) to an accessible dimension through PCA, then apply GPA in the latent space, and reconstruct the result to the original high-dimensional space.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "* 6 snapshots at different timepoints (day 0, 1, 2, 3, 4, 8)\n",
    "* samplesize: differ by days\n",
    "* dimension: 175\n",
    "* The trajectory will be visualized in 2D principal component axes\n",
    "\n",
    "## Functionality\n",
    "\n",
    "* Given source and target days, GPA transports the source dataset toward the target dataset from a deterministic particle dynamics for the gradient flow of (Lipschitz regularized or limited transportation speed) KL divergergence.\n",
    "$$D_{KL}^L(P\\|Q) = \\sup_{\\| \\nabla \\phi \\| \\leq L} \\left \\{\\mathbb{E}_P[\\phi] - \\log \\mathbb{E}_Q [\\exp(\\phi)] \\right \\}$$\n",
    "$$\\partial_t P + \\nabla \\cdot \\left(P v\\right) = \\partial_t P - \\nabla \\cdot \\left(P \\nabla \\phi \\right) = 0$$\n",
    "$$\\dot{X} = - \\nabla \\phi_t(X)$$\n",
    "\n",
    "* Snapshots at all the intermediate points will be highlighted by default, but it can be specified.\n",
    "* $W_2$ distance will be calculated between the particle trajectory and the snapshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "def load_W(filename):\n",
    "    with open(filename, \"rb\") as fr:\n",
    "        W, b, p = pk.load(fr)\n",
    "\n",
    "    W = [tf.Variable(w,  dtype=tf.float32) for w in W]\n",
    "    b = [tf.Variable(b_,  dtype=tf.float32) for b_ in b]\n",
    "\n",
    "    return W, b, p\n",
    "\n",
    "def v(x, t, W, b):   # neural newtork for time-dependent vectorfield\n",
    "    num_layers = len(W)\n",
    "    activation_ftn = tf.nn.tanh\n",
    "        \n",
    "    h = tf.concat([x, t*tf.ones([x.shape[0], 1], dtype=tf.float32)], axis=1)\n",
    "    for l in range(0,num_layers-1):\n",
    "        h = activation_ftn(tf.add(tf.matmul(h, W[l]), b[l]))\n",
    "    out=tf.add(tf.matmul(h, W[-1]), b[-1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def time_integration(x0, T, dt):\n",
    "    x = tf.constant(x0, dtype=tf.float32)\n",
    "    xs = [x0]\n",
    "    for i in range(int(T/dt)):\n",
    "        vv = v(x, dt*i, W, b)\n",
    "        x += dt * vv\n",
    "        xs.append(x.numpy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def generate_animation(days, intermediate_days, X1_trpts, dt, physical_dt, img_src, d_red = 2, vs = None):\n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}    \n",
    "    \n",
    "    \n",
    "    for i, X1_trpt in enumerate(X1_trpts):  # trajectories\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        X1_trpt_vis = X1_trpt\n",
    "        \n",
    "        if type(vs) != type(None) and i < len(X1_trpts)-1:\n",
    "            X1_trpt_vis_next = X1_trpts[i+1]\n",
    "            vs_vis = (X1_trpt_vis_next-X1_trpt_vis) / dt\n",
    "            im = ax.quiver(X1_trpt_vis[:, 0], X1_trpt_vis[:, 1], vs_vis[:, 0], vs_vis[:, 1], \n",
    "                           width=0.003, headwidth=7, headlength=15, headaxislength=7, zorder=15)                            \n",
    "        else:\n",
    "            im = ax.scatter(X1_trpt_vis[:,0], X1_trpt_vis[:,1], color=colors[days[0]], \n",
    "                            alpha=1.0, s=0.7, zorder=10, label=f'day {days[0]}') # transported source \n",
    "            for t in days[1:]:\n",
    "                X2_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X2_vis[:,0], X2_vis[:,1], color=colors[t], \n",
    "                   alpha=1.0, s=0.7, zorder=5, label=f'day {t}') # target \n",
    "            \n",
    "            for t in intermediate_days:\n",
    "                X1_intermediate_vis = pca.transform(mats[t])\n",
    "                ax.scatter(X1_intermediate_vis[:,0], X1_intermediate_vis[:,1], color='lightgray', \n",
    "                        alpha=0.3, s=0.7, zorder=1, label=f'day {t}')\n",
    "        #ax.set_xlim([-5,6])\n",
    "        #ax.set_ylim([-5,6])\n",
    "        \n",
    "        #leg = ax.legend(loc='upper right')\n",
    "        ttl = ax.text(0.5,1.05, \"t = %.3f\" % (physical_dt*i), \\\n",
    "                      bbox={'facecolor':'w', 'alpha':0.5, 'pad':5}, \\\n",
    "                      transform=ax.transAxes, ha=\"center\")\n",
    "        ims.append([im, ttl])#, leg])\n",
    "        \n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=200)\n",
    "    writergif = animation.PillowWriter(fps=3)\n",
    "    ani.save(img_src, writer=writergif)\n",
    "    plt.clf()\n",
    "    display(Image(filename = img_src))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3201aa-0747-44cf-837f-9310eef4786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for piecewise GPA (sample 3 - stem cell and sample 5 - synthetic)\n",
    "\n",
    "\n",
    "## Static plot function for piecewise GPA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_three_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#2ca02c',  # Green\n",
    "        middle_t: '#ff7f0e',  # Orange\n",
    "        intermediate_days[1]: '#8c564b',  # Brown\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax1.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=5, zorder = 1)\n",
    "\n",
    "    \n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=24)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=24)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=12, zorder = 10, label=f'Time {source_t} (Training Data)')\n",
    "    ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=12, zorder = 10, label=f'Time {middle_t} (Training Data)')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=12, zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax2.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.2, alpha=1.0, s=15, zorder = 20,  label=f'Time {t} (Test Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 24)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 24)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=24)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "    # Create legend with smaller markers and tighter spacing\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles,\n",
    "        sorted_labels,\n",
    "        fontsize=20,         # Font size of text\n",
    "        loc='center',\n",
    "        ncol=len(sorted_labels),\n",
    "        markerscale=2,     # Scale down marker size in legend\n",
    "        handlelength=1.5,    # Length of the marker line\n",
    "        handletextpad=0.2    # Padding between marker and label text\n",
    "    )    \n",
    "\n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d6d3-f9f8-4757-b381-2442975fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (Sample 1 - EMT data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "    # Plot intermediate time points\n",
    "    for t in intermediate_days:\n",
    "        X_intermediate_vis = pca.transform(mats[t])\n",
    "        ax1.scatter(X_intermediate_vis[:, 0], X_intermediate_vis[:, 1], color=color_map[t], facecolors='none', edgecolors=color_map[t], linewidths=1.0, alpha=0.75, s=10, zorder = 20,  label=f'Day {t} (Test Data)')\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=27)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=27)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=30)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 27)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 27)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=27)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "    time_labels = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "            time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "        except ValueError:\n",
    "            time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "    # Sort legend by time values\n",
    "    time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "    sorted_labels = [item[1] for item in time_labels]\n",
    "    sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "    # **Increase marker size in legend**\n",
    "    for handle in sorted_handles:\n",
    "        if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "            handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "    ax_legend.axis(\"off\")  # Remove axes\n",
    "        \n",
    "    # Create legend with larger markers for scatter plots\n",
    "    legend = ax_legend.legend(\n",
    "        sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "        ncol=len(sorted_labels), markerscale=2)\n",
    "    \n",
    "    # Save the legend separately\n",
    "    legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "    fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "    print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b79cf-56f9-44f8-b98e-77e37b64fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_with_snapshots=None, output_file_without_snapshots=None, output_file_snapshots_only=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    # Define color gradient for snapshots\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis  # Can change to \"plasma\", \"inferno\", etc.\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "\n",
    "    # Create a normalization object for the color mapping\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # Needed for color bar\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot source, intermediates, and target\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    #Xm_vis = pca.transform(mats[middle_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    #ax1.scatter(X1_vis[:, 0], X1_vis[:, 1], facecolors='none', edgecolors=color_map[source_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {source_t}')\n",
    "    #ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], facecolors='none', edgecolors=color_map[target_t], linewidths=0.5, alpha=1.0, s=20, zorder=10, label=f'Time {target_t}')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot snapshots from X1_trpts with a color gradient\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder = 1)\n",
    "\n",
    "    # Add a small color bar inside the plot\n",
    "    cax = ax1.inset_axes([1.02, 0.2, 0.03, 0.6])  # [x, y, width, height] (relative position)\n",
    "    \n",
    "    # Create the colorbar with increased size\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    \n",
    "    # Set manual tick positions\n",
    "    cbar.set_ticks(np.linspace(0, 4, 5))  # Ensures ticks at 0, 1, 2, 3, 4\n",
    "    \n",
    "    # Optional: Explicitly set tick labels if needed\n",
    "    cbar.set_ticklabels([0, 1, 2, 3, 4])  \n",
    "    \n",
    "    # Increase colorbar label font size\n",
    "    cbar.set_label(\"Time\", fontsize=20)  \n",
    "    \n",
    "    # Increase colorbar tick font size\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    # Adjust colorbar thickness\n",
    "    #cbar.ax.set_aspect(20)  # Increase aspect ratio to make it thicker\n",
    "   \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax1.legend(loc='upper right', fontsize= 24)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # **Plot 2: Without Snapshots**\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot only source, intermediates, and target\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8,  zorder = 15, label=f'Time {source_t} (Training Data)')\n",
    "    #ax2.scatter(Xm_vis[:, 0], Xm_vis[:, 1], color=color_map[middle_t], alpha=1.0, s=10,  zorder = 10, label=f'Time {middle_t}')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8,  zorder = 10, label=f'Time {target_t} (Training Data)')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize = 20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize = 20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=20)  # Increase tick sizes\n",
    "    #ax2.legend(loc='upper right', fontsize='small')\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    # Save or show the plot\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract legend elements\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    \n",
    "    # Proceed only if legend elements exist\n",
    "    if handles and labels:\n",
    "        # Extract numeric values from \"Time X (Input Data)\" and \"Time X (Test Data)\"\n",
    "        time_labels = []\n",
    "        for label in labels:\n",
    "            try:\n",
    "                time_value = int(label.split(\" \")[1])  # Extract the numerical value after \"Time\"\n",
    "                time_labels.append((time_value, label))  # Store (time, label) pairs\n",
    "            except ValueError:\n",
    "                time_labels.append((float('inf'), label))  # Place non-time labels at the end\n",
    "    \n",
    "        # Sort legend by time values\n",
    "        time_labels.sort(key=lambda x: x[0])  # Sort by the extracted numeric value\n",
    "        sorted_labels = [item[1] for item in time_labels]\n",
    "        sorted_handles = [handles[labels.index(label)] for label in sorted_labels]\n",
    "    \n",
    "        # **Increase marker size in legend**\n",
    "        for handle in sorted_handles:\n",
    "            if isinstance(handle, plt.Line2D):  # Ensure we're modifying scatter markers\n",
    "                handle.set_markersize(30)  # Adjust marker size\n",
    "    \n",
    "        # Create a separate figure for the legend\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Adjust size as needed\n",
    "        ax_legend.axis(\"off\")  # Remove axes\n",
    "    \n",
    "        # Create legend with larger markers for scatter plots\n",
    "        legend = ax_legend.legend(\n",
    "            sorted_handles, sorted_labels, fontsize=20, loc='center',\n",
    "            ncol=len(sorted_labels), markerscale=6  # Increase scatter marker size\n",
    "        )\n",
    "    \n",
    "        # Save the legend separately\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig_legend)  # Close the legend figure\n",
    "    \n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "    else:\n",
    "        print(\"No legend elements found — skipping separate legend plot.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b995d3-f5f3-4e68-b20f-ec1800daae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static plot function for simple GPA (NDPR and Clinical data) - separate legend\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "def generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days, intermediate_days, X1_trpts, mats, d_red=26,\n",
    "    output_file_with_snapshots=None,\n",
    "    output_file_without_snapshots=None,\n",
    "    output_file_snapshots_only=None\n",
    "):\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    colormap = cm.viridis\n",
    "    snapshot_colors = [colormap(i / num_snapshots) for i in range(num_snapshots)]\n",
    "\n",
    "    # Rescale time values for the color bar\n",
    "    time_values = np.linspace(0, physical_dt * num_snapshots, num_snapshots)\n",
    "    norm = mcolors.Normalize(vmin=time_values.min(), vmax=time_values.max())\n",
    "    sm = cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    source_t, _, target_t = days[0], days[1], days[-1]\n",
    "\n",
    "    color_map = {\n",
    "        source_t: 'magenta',  # Blue\n",
    "        target_t: '#008080'   # Red\n",
    "    }\n",
    "\n",
    "    # --- Plot 1: WITH snapshots (No inline colorbar) ---\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "\n",
    "    for i, X1_trpt in enumerate(X1_trpts):\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        X1_hat_vis = X1_trpt\n",
    "        ax1.scatter(X1_hat_vis[:, 0], X1_hat_vis[:, 1], color=snapshot_colors[i], alpha=0.75, s=2, zorder=1)\n",
    "\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax1.tick_params(axis='both', labelsize=32)\n",
    "    ax1.set_title(\"\")\n",
    "\n",
    "    if output_file_with_snapshots:\n",
    "        plt.savefig(output_file_with_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITH snapshots saved to {output_file_with_snapshots}\")\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Save colorbar separately ---\n",
    "    fig_cb, ax_cb = plt.subplots(figsize=(10, 1))\n",
    "    cb = plt.colorbar(sm, cax=ax_cb, orientation='horizontal')\n",
    "    cb.set_ticks([0, 4])\n",
    "    cb.set_ticklabels([\"Pre-treatment\", \"Post-treatment\"])\n",
    "    cb.ax.tick_params(labelsize=24)\n",
    "    cb.set_label(\"Time\", fontsize=24)\n",
    "    cb_path = os.path.join(result_dir, \"trajectory_colorbar_only.png\")\n",
    "    fig_cb.savefig(cb_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_cb)\n",
    "    print(f\"Standalone colorbar saved to {cb_path}\")\n",
    "\n",
    "    # --- Plot 2: WITHOUT snapshots ---\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color=color_map[source_t], alpha=1.0, s=8, zorder=15, label='Pre-treatment')\n",
    "    ax2.scatter(X2_vis[:, 0], X2_vis[:, 1], color=color_map[target_t], alpha=1.0, s=8, zorder=10, label='Post-treatment')\n",
    "\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=32)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=32)\n",
    "    ax2.tick_params(axis='both', labelsize=32)\n",
    "    ax2.set_title(\"\")\n",
    "\n",
    "    if output_file_without_snapshots:\n",
    "        plt.savefig(output_file_without_snapshots, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Static trajectory plot WITHOUT snapshots saved to {output_file_without_snapshots}\")\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # --- Legend (just for pre/post-treatment) ---\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig_legend, ax_legend = plt.subplots(figsize=(6, 2))\n",
    "        ax_legend.axis(\"off\")\n",
    "        ax_legend.legend(\n",
    "            handles, labels, fontsize=16, loc='center',\n",
    "            ncol=len(labels), markerscale=3, handletextpad=0.5\n",
    "        )\n",
    "        legend_path = os.path.join(result_dir, \"legend_only.png\")\n",
    "        fig_legend.savefig(legend_path, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close(fig_legend)\n",
    "        print(f\"Legend saved separately at: {legend_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a373d4-d7ed-4142-8b91-9b5ad8633521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136033ed-955a-43c7-849c-6ee8c75c3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two pieces GPA (Synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0,2,4], [1,3], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c365c-319c-4eff-bf24-67435d1f6137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 2, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6f201-8cf9-44a7-8e30-05d38a306d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594032d-8820-4228-b82d-0f576845ca33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One piece of GPA (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e85a9-2644-4dae-a799-d814c319b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6faa1-cae0-45a1-ae38-eec202c26b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One piece of GPA (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "img_src = result_dir + exp_name + '-movie-original.gif' \n",
    "img_src1 = result_dir + exp_name + '-movie-original-with-arrows.gif'\n",
    "if os.path.exists(img_src): # try loading saved movie\n",
    "    display(Image(filename = img_src))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src, d_red = d_red)\n",
    "if os.path.exists(img_src1): # try loading saved movie\n",
    "    display(Image(filename = img_src1))\n",
    "else:\n",
    "    generate_animation([0, 4], [], X1_trpts, dt, physical_dt, img_src1, d_red = d_red, vs = \"vecotorfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819b6a5-bd12-46e5-936a-4dc7f39da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (samples 1 - EMT data)\n",
    "\n",
    "exp_name = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 8\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[2],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75def9f-4e12-4edb-98a5-648bb7d36b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Full trajectories on static plot (Sample 5 - synthetic data)\n",
    "\n",
    "exp_name = 'f_Lip=5e-2-t_size=50-network=64_64_64_26d' #'times_10_particles_200_3'\n",
    "d_red = 26\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636e70b-e41c-4e1b-977a-1684daa4e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (samples 3 - stem cell data)\n",
    "\n",
    "exp_name = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_three_timepoints(\n",
    "    days=[0, 2, 4],\n",
    "    intermediate_days=[1, 3],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc9293-f90b-424c-988f-01d52bb13a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (NDPR data)\n",
    "\n",
    "exp_name = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154d2ad-1e37-4b94-8b38-b70df9d4706e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (PA3)\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a20c9-19ef-4817-b018-55ee0ed5174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 862)\n",
    "\n",
    "exp_name = 'Palbo_862_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85b29a-ff95-4834-86aa-eeaafb43ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full trajectories on static plot (Patient 887)\n",
    "\n",
    "exp_name = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 2\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_with_snapshots = f\"{result_dir}{exp_name}_static_trajectory_with_snapshots_circle.png\"\n",
    "output_file_without_snapshots = f\"{result_dir}{exp_name}_static_trajectory_without_snapshots.png\"\n",
    "output_file_snapshots_only = f\"{result_dir}{exp_name}_static_trajectory_snapshots_only.png\"\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_two_timepoints_no_middle_legend(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_with_snapshots=output_file_with_snapshots,\n",
    "    output_file_without_snapshots=output_file_without_snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21ad8-a556-4f35-86d4-49068a3a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_static_trajectory_plots_cell_types(days, intermediate_days, X1_trpts, mats, d_red=26, output_file_cell_type_source=None, output_file_cell_type_target=None, output_file_cell_type_legend=None):\n",
    "    \"\"\"\n",
    "    Generate two static trajectory plots:\n",
    "    1. With snapshots from X1_trpts using a color gradient.\n",
    "    2. Without snapshots, showing only main time points.\n",
    "    \"\"\"\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "\n",
    "    source_t, middle_t, target_t = days[0], days[1], days[-1]\n",
    "    \n",
    "    # Define colors for time points\n",
    "    color_map = {\n",
    "        source_t: '#1f77b4',  # Blue\n",
    "        #intermediate_days[0]: '#ff7f0e',  # Orange\n",
    "        target_t: '#d62728'  # Red\n",
    "    }\n",
    "\n",
    "    # **Plot 1: With Snapshots**\n",
    "\n",
    "    # Same PCA transformation and cell type extraction as before\n",
    "    X1_vis = pca.transform(mats[source_t])\n",
    "    X2_vis = pca.transform(mats[target_t])\n",
    "    cell_types_X1 = cell_types_by_day[source_t]\n",
    "    cell_types_X2 = cell_types_by_day[target_t]\n",
    "    unique_cell_types = np.unique(np.concatenate([cell_types_X1, cell_types_X2]))\n",
    "    cell_type_palette = dict(zip(unique_cell_types, sns.color_palette(\"tab20\", len(unique_cell_types))))\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 1: X1 colored, X2 gray (no legend)\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    ax1.scatter(X2_vis[:, 0], X2_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X1 == cell_type\n",
    "        ax1.scatter(X1_vis[idx, 0], X1_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax1.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax1.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax1.set_title(f\"Untreated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_source:\n",
    "        plt.savefig(output_file_cell_type_source, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # -----------------------\n",
    "    # Plot 2: X2 colored, X1 gray (no legend)\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    ax2.scatter(X1_vis[:, 0], X1_vis[:, 1], color='lightgray', alpha=0.5, s=8)\n",
    "    for cell_type in unique_cell_types:\n",
    "        idx = cell_types_X2 == cell_type\n",
    "        ax2.scatter(X2_vis[idx, 0], X2_vis[idx, 1], \n",
    "                    color=cell_type_palette[cell_type], s=8, alpha=1.0)\n",
    "    ax2.set_xlabel(\"PC 1\", fontsize=20)\n",
    "    ax2.set_ylabel(\"PC 2\", fontsize=20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax2.set_title(f\"Treated Samples colored by Cell Type\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    if output_file_cell_type_target:\n",
    "        plt.savefig(output_file_cell_type_target, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    # Use circle markers instead of patches for legend\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create circle markers for legend entries\n",
    "    legend_elements = [\n",
    "        mlines.Line2D(\n",
    "            [], [], marker='o', color='w',\n",
    "            markerfacecolor=cell_type_palette[cell_type],\n",
    "            markersize=8, label=cell_type\n",
    "        )\n",
    "        for cell_type in unique_cell_types\n",
    "    ]\n",
    "    \n",
    "    # Create figure and axis (just for the legend)\n",
    "    fig_leg, ax_leg = plt.subplots()\n",
    "    fig_leg.set_figwidth(8)  # Initial size; will be adjusted\n",
    "    fig_leg.set_figheight(6)\n",
    "    \n",
    "    # Hide axes\n",
    "    ax_leg.axis('off')\n",
    "    \n",
    "    # Add legend to axis (not directly to plt)\n",
    "    legend = ax_leg.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='center',\n",
    "        frameon=True,\n",
    "        fontsize=14,\n",
    "        ncol=1,\n",
    "        title='Cell Types',\n",
    "        title_fontsize=14,\n",
    "        borderpad=1\n",
    "    )\n",
    "    \n",
    "    # Resize the figure to tightly fit the legend\n",
    "    fig_leg.canvas.draw()\n",
    "    bbox = legend.get_window_extent().transformed(fig_leg.dpi_scale_trans.inverted())\n",
    "    fig_leg.set_size_inches(bbox.width + 0.5, bbox.height + 0.5)  # Add a little padding\n",
    "    \n",
    "    # Save only, no display\n",
    "    if output_file_cell_type_legend:\n",
    "        plt.savefig(output_file_cell_type_legend, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_leg)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baaf34-9a34-442b-aee1-171ebe303d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell types plots\n",
    "\n",
    "exp_name = 'Palbo_BMC_nofibroblast_malignant_dim20-f_Lip=5e-3-t_size=50-network=64_64_64' #'times_10_particles_200_3'\n",
    "d_red = 20\n",
    "filename = result_dir + exp_name + \".pickle\"\n",
    "W, b, p = load_W(filename)\n",
    "\n",
    "# load PCA\n",
    "if dim_red_method == 'EMT_PCA':\n",
    "    pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "elif dim_red_method == 'PCA':\n",
    "    pca_filename = \"pca_%d.pkl\" % d_red\n",
    "else:\n",
    "    print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "    [pca] = pk.load(fr)\n",
    "\n",
    "dt = p['numerical_ts'][-1]/200\n",
    "X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "\n",
    "physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_cell_type_source = f\"{result_dir}{exp_name}_cell_type_source.png\"\n",
    "output_file_cell_type_target = f\"{result_dir}{exp_name}_cell_type_target.png\"\n",
    "output_file_cell_type_legend = f\"{result_dir}{exp_name}_cell_type_legend.png\"\n",
    "\n",
    "\n",
    "# Generate both plots\n",
    "generate_static_trajectory_plots_cell_types(\n",
    "    days=[0, 4],\n",
    "    intermediate_days=[ ],\n",
    "    X1_trpts=X1_trpts,\n",
    "    mats=mats,\n",
    "    d_red=d_red,\n",
    "    output_file_cell_type_source=output_file_cell_type_source,\n",
    "    output_file_cell_type_target=output_file_cell_type_target,\n",
    "    output_file_cell_type_legend=output_file_cell_type_legend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13105277-7e64-4d36-ae15-12605d76473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for EMT data, Time [0 , 2, 4]\n",
    "## Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_EMT(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    #subtrajectory_colors = ['red', 'blue', 'brown']\n",
    "    subtrajectory_colors = ['green']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"gray\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 → 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Individual_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cell’s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # ✅ Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 → 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory {label_mapping[label]}'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 2, 4])  # Ensure correct positions\n",
    "    \n",
    "    # 🎻 **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # ✅ REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # ✅ Assign correct color\n",
    "            alpha=0.8,  # ✅ MAKE TRANSPARENT\n",
    "            zorder=3  # ✅ BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # 🛠 **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0,2, 4])  \n",
    "    ax1.set_xticklabels([0, 2, 4], fontsize=32)\n",
    "    ax1.tick_params(axis='y', labelsize=32)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=32)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=32)\n",
    "    ax1.set_title(f'Single Cell {gene_of_interest} Expression Dynamics', fontsize=32)\n",
    "\n",
    "\n",
    "    # 🎨 **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # 🎨 **Redefine `legend_patches` to Include a Green Bar**\n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"green\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Gene dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "    # 🎨 **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # 🎨 **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b2723-a5e8-4bbf-8422-c6fd8ca4e1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for EMT data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]\n",
    "#intermediate_t = [4]\n",
    "\n",
    "d_red= 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_EMT(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafe22a-1a0c-46c9-a09b-eae74c2dabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for EMT data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Individual_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Individual_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54ca85-d12d-43c9-9c62-15c180451c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for Stem Cell data, Time [0 , 1,  2, 3,  4]\n",
    "## Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_mESC(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    last_day = mats[day2]\n",
    "\n",
    "    last_day_reduced = pca.transform(last_day).astype(np.float32)\n",
    "    \n",
    "    # Perform KMeans clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=40)\n",
    "    kmeans.fit(last_day_reduced)\n",
    "    last_day_labels = kmeans.labels_\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_clusters.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "\n",
    "    #mask = last_day_labels == 0\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    #subtrajectory_colors = ['red', 'blue']\n",
    "    subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    #violin_colors = [\"#3cb44b\", \"#f58231\", \"#3cb44b\", \"#f58231\", \"#3cb44b\"]  # Green, Orange, Purple\n",
    "    violin_colors = [\"black\", \"gray\", \"black\", \"gray\", \"black\"] \n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 → 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Individual_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cell’s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # ✅ Use the **subtrajectory colors**\n",
    "                    alpha=0.7, linewidth=1.0 \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 → 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory {label_mapping[label]}'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 1, 2, 3, 4])  # Ensure correct positions\n",
    "    \n",
    "    # 🎻 **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # ✅ REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # ✅ Assign correct color\n",
    "            alpha=0.8,  # ✅ MAKE TRANSPARENT\n",
    "            zorder=3  # ✅ BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # 🛠 **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 1, 2, 3, 4])  \n",
    "    ax1.set_xticklabels([0, 1, 2, 3, 4], fontsize=35)\n",
    "    ax1.tick_params(axis='y', labelsize=35)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=35)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=35)\n",
    "    ax1.set_title(f'Single Cell {gene_of_interest} Expression Dynamics', fontsize=35)\n",
    "\n",
    "\n",
    "    # 🎨 **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # 🎨 **Redefine `legend_patches` to Include a Green Bar**\n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Gene dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "    # 🎨 **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\"),\n",
    "        mpatches.Patch(color=\"gray\", label=\"Test Data\")\n",
    "    ]\n",
    "    \n",
    "    # 🎨 **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1837c38-e9e5-47f5-bd9b-6b0aa482ccd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for Stem cell data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,2,3]\n",
    "#intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_mESC(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359e0ac-232e-4445-8511-4ea756a22b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf for stem cell data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Individual_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Individual_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdef54-19ec-43fc-b1b2-7360af7749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for breast cancer cell line's data, Time [0 , 4]\n",
    "# Plot gene dynamis for each trajectory\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_NDPR(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_deviation.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    real_cell_types = np.array(cell_ids_by_day[day2])\n",
    "    #unique_cell_types = np.unique(real_cell_types)\n",
    "    unique_cell_types = unique_labels\n",
    "    #subtrajectory_colors = list(sns.color_palette(\"tab20\", len(unique_cell_types)))\n",
    "    #subtrajectory_colors = ['green', 'orange', 'purple', 'blue', 'red', 'brown']\n",
    "    subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 → 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cell’s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # ✅ Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 → 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory of {label_mapping[label]} phenotypic shift'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 4])  # Ensure correct positions\n",
    "    \n",
    "    # 🎻 **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # ✅ REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # ✅ Assign correct color\n",
    "            alpha=0.8,  # ✅ MAKE TRANSPARENT\n",
    "            zorder=3  # ✅ BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # 🛠 **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 4])  \n",
    "    #ax1.set_xticklabels([0, 4], fontsize=32)\n",
    "    ax1.set_xticklabels([\"Pre-treatment\", \"Post-treatment\"], fontsize=46)\n",
    "    ax1.tick_params(axis='y', labelsize=46)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=46)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=46)\n",
    "    ax1.set_title(f'{gene_of_interest}', fontsize=46)\n",
    "\n",
    "\n",
    "    # 🎨 **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # 🎨 **Redefine `legend_patches` to Include a Green Bar**\n",
    "    \n",
    "    legend_patches = [\n",
    "        mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "                      label=\"Hallmark dynamics of each single cell\")\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 🎨 **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\")\n",
    "    ]\n",
    "    \n",
    "    # 🎨 **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec9d17-841f-4ee3-9018-6305aa22a610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for breast cancer cell line data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "#intermediate_t = [1,2,3]\n",
    "intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_NDPR(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba050fa6-7fd2-4b57-8290-047b682e1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf - Breast cancer cell line data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"Palbo_NDPR_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674b512-09e6-4e75-835f-72b1be73082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## This is for clinical data, Time [0 , 4]\n",
    "## Plot gene dynamis for each trajectory (deviation cell types)\n",
    "\n",
    "import seaborn as sns  # Required for violin plots\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "## Subtrajectroies defined by source\n",
    "def Average_gene_dynamics_whole_saveonly_single_trajectory_clinical(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                              intermediate_t = [1], \n",
    "                              d_red=2, random_state=42, exp_memo = '2'):\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "    \n",
    "    # load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = \"emt_pca_%d.pkl\" % d_red\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = \"pca_%d.pkl\" % d_red\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "    \n",
    "    with open(data_dir + pca_filename,\"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "    \n",
    "    dt = p['numerical_ts'][-1]/200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T = p['numerical_ts'][-1], dt = dt)\n",
    "    \n",
    "    physical_dt = dt * p['ts'][-1] / p['numerical_ts'][-1]\n",
    "    \n",
    "    intermediate_t = np.array(intermediate_t)\n",
    "    \n",
    "    if len(intermediate_t) == 0:\n",
    "        intermediate_t = range(source_t+1, target_t)\n",
    "        \n",
    "    # data parameters\n",
    "    day1, day2 = source_t, target_t\n",
    "\n",
    "\n",
    "    # --------\n",
    "    N_source = N_samples_cls[day1]\n",
    "    N_target = N_samples_cls[day2]\n",
    "        \n",
    "\n",
    "    X1_trpt = X1_trpts[-1]\n",
    "    \n",
    "    \n",
    "    contrast_colors = [\n",
    "    '#1f77b4',  # blue\n",
    "    '#2ca02c',  # green\n",
    "    '#ff7f0e',  # orange\n",
    "    '#8c564b',  # brown\n",
    "    '#d62728',  # red \n",
    "    '#9467bd'  # purple (to be used for index 8)\n",
    "    ]\n",
    "\n",
    "    # Create a color mapping for the specific indices\n",
    "    colors = {0: contrast_colors[0], 1: contrast_colors[1], 2: contrast_colors[2], 3: contrast_colors[3], 4: contrast_colors[4], 8: contrast_colors[5]}\n",
    "\n",
    "    \n",
    "    # Step 1: Perform clustering analysis on the last day's cell states from mats\n",
    "    \n",
    "    # Load previously saved cluster labels\n",
    "    cluster_save_path = f\"{result_dir}{exp_memo}_X1_hat_deviation.csv\"\n",
    "    if not os.path.exists(cluster_save_path):\n",
    "        raise FileNotFoundError(f\"Cluster labels file not found: {cluster_save_path}\")\n",
    "    \n",
    "    df_clusters = pd.read_csv(cluster_save_path)\n",
    "    X1_hat_labels = df_clusters[\"Cluster_Label\"].values  # Load saved labels\n",
    "\n",
    "    # Print the number of unique labels in last_day_labels\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    print(f\"Number of unique labels in X1_hat_labels: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Define a function to create colors for the subgroups using a predefined set of colors\n",
    "    def get_subgroup_colors(labels, colors):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(colors) < len(unique_labels):\n",
    "            raise ValueError(\"Not enough colors for the number of unique labels.\")\n",
    "        subgroup_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        return subgroup_colors\n",
    "\n",
    "    # Define specific sets of colors for the blue and red subgroups\n",
    "    blue_colors = ['#1f77b4', '#878ceb', '#104E8B', '#87CEEB', '#4682B4', '#6495ED', '#5F9EA0']  # Add more shades of blue as needed\n",
    "    red_colors = ['#d62728',  '#eb8787', '#FF4500', '#DC143C', '#FF6347', '#B22222', '#8B0000']  # Add more shades of red as needed\n",
    "    light_red_colors = ['#f99fa1', '#ffb1b1', '#ffaf86', '#f48585', '#ffb5a5', '#ff9c9c', '#ff5f5f']\n",
    "    \n",
    "    # Get the subgroup colors based on the labels\n",
    "    subgroup_colors_blue = get_subgroup_colors(X1_hat_labels, blue_colors)\n",
    "    subgroup_colors_red = get_subgroup_colors(X1_hat_labels, red_colors)\n",
    "    \n",
    "    \n",
    "    # Extract the gene index for the gene of interest\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    \n",
    "    # Extract gene expression values from mats[day1], intermediate time points, and mats[day2]\n",
    "    X1_vis_pca = pca.transform(mats[source_t])\n",
    "    X1_vis_i_pca = pca.inverse_transform(X1_vis_pca)\n",
    "    X2_vis_pca = pca.transform(mats[target_t])\n",
    "    X2_vis_i_pca = pca.inverse_transform(X2_vis_pca)\n",
    "\n",
    "    gene_expression_X1 = X1_vis_i_pca[:, gene_index]\n",
    "    gene_expression_X2 = X2_vis_i_pca[:, gene_index]\n",
    "\n",
    "    gene_expression_intermediates = []\n",
    "    for t in intermediate_t:\n",
    "        X1_intermediate_vis_pca = pca.transform(mats[t])\n",
    "        X1_intermediate_vis_i_pca = pca.inverse_transform(X1_intermediate_vis_pca)\n",
    "        gene_expression_intermediates.append(X1_intermediate_vis_i_pca[:, gene_index])\n",
    "\n",
    "    # Extract gene expression values from X1_trpts based on the given condition\n",
    "    \n",
    "    gene_expression_X1_trpts = np.concatenate([pca.inverse_transform(X1_trpt)[:, gene_index] for i, X1_trpt in enumerate(X1_trpts) if i % index == 0 and i <= max_i])\n",
    "    \n",
    "    # Combine all gene expression values\n",
    "    all_gene_expression_values = np.concatenate([gene_expression_X1, *gene_expression_intermediates, gene_expression_X2, gene_expression_X1_trpts])\n",
    "\n",
    "    gene_expression_X1_normalized = gene_expression_X1\n",
    "    gene_expression_intermediates_normalized = gene_expression_intermediates\n",
    "    gene_expression_X2_normalized = gene_expression_X2\n",
    "    gene_expression_X1_trpts_normalized = gene_expression_X1_trpts\n",
    "    \n",
    "    vmin = all_gene_expression_values.min()\n",
    "    vmax = all_gene_expression_values.max()\n",
    "    \n",
    "    # Plot dynamics for X1_trpts with subgroup colors\n",
    "    indices = range(len(X1_trpts))\n",
    "\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "\n",
    "    \n",
    "    # (1) Plot the averaged gene expressions across X1_trpt at each time point with confidence intervals\n",
    "    \n",
    "    # Compute the average gene expression and confidence intervals\n",
    "    avg_gene_expressions = []\n",
    "    ci_gene_expressions = []\n",
    "    \n",
    "    # Reset normalized gene expression values for X1_trpts\n",
    "    all_gene_expression_values_normalized_X1 = gene_expression_X1_trpts_normalized\n",
    "    \n",
    "    # Use indices with the specified step size defined by `index`\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    \n",
    "    # Iterate through indices to compute averages and confidence intervals\n",
    "    for i in indices:\n",
    "        if i > max_i:  # Apply truncation based on max_i\n",
    "            break\n",
    "        X1_trpt = X1_trpts[i]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Inverse transform the current trajectory\n",
    "        X1_hat = pca.inverse_transform(X1_trpt)\n",
    "    \n",
    "        # Extract gene expression values for the current step\n",
    "        gene_expression_values = all_gene_expression_values_normalized_X1[:len(X1_hat)]\n",
    "        all_gene_expression_values_normalized_X1 = all_gene_expression_values_normalized_X1[len(X1_hat):]  # Update to exclude used values\n",
    "    \n",
    "        # Compute average and confidence interval\n",
    "        avg_gene_expressions.append(np.mean(gene_expression_values))\n",
    "        ci = stats.sem(gene_expression_values) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_values) - 1)\n",
    "        ci_gene_expressions.append(ci)\n",
    "    \n",
    "    # Process intermediate time points\n",
    "    intermediate_avg_expressions = []\n",
    "    intermediate_ci_expressions = []\n",
    "    intermediate_indices = []\n",
    "\n",
    "\n",
    "    for idx, t in enumerate(intermediate_t):\n",
    "        gene_expression_intermediate = gene_expression_intermediates_normalized[idx]\n",
    "        intermediate_avg_expressions.append(np.mean(gene_expression_intermediate))\n",
    "        ci = stats.sem(gene_expression_intermediate) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_intermediate) - 1)\n",
    "        intermediate_ci_expressions.append(ci)\n",
    "    \n",
    "        # Rescale the intermediate time points to align with `index`\n",
    "        shifted_value_1 = intermediate_t - 1\n",
    "        shifted_value_2 = intermediate_t[0] - 1\n",
    "        shifted_t_1 = t - shifted_value_1\n",
    "        shifted_t_2 = t - shifted_value_2\n",
    "        time_index = int((float(shifted_t_2) / (float(max(shifted_t_1)) + 1)) * len(indices))\n",
    "        intermediate_indices.append(time_index)\n",
    "\n",
    "    \n",
    "    # Include first and last time points\n",
    "    all_avg_expressions = [np.mean(gene_expression_X1_normalized)] + intermediate_avg_expressions + [np.mean(gene_expression_X2_normalized)]\n",
    "    all_ci_expressions = [\n",
    "        stats.sem(gene_expression_X1_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X1_normalized) - 1)\n",
    "    ] + intermediate_ci_expressions + [\n",
    "        stats.sem(gene_expression_X2_normalized) * stats.t.ppf((1 + 0.95) / 2., len(gene_expression_X2_normalized) - 1)\n",
    "    ]\n",
    "\n",
    "        \n",
    "    all_indices = [0] + intermediate_indices + [len(indices)]\n",
    "    combined_indices = sorted([day1] + intermediate_t.tolist() + [day2])\n",
    "\n",
    "    print(combined_indices)\n",
    "\n",
    "    \n",
    "    # Ensure extended_indices align with avg_gene_expressions\n",
    "    extended_indices = np.array([x * index for x in range(len(avg_gene_expressions))])\n",
    "    \n",
    "    # Ensure all_indices and extended_indices are NumPy arrays\n",
    "    combined_indices = np.array(combined_indices)\n",
    "    extended_indices = np.array(extended_indices)\n",
    "    \n",
    "    # Linearly rescale all_indices to be equally distributed in extended_indices\n",
    "    rescaled_indices = np.interp(\n",
    "        combined_indices,  # Original indices\n",
    "        [combined_indices[0], combined_indices[-1]],  # Range of all_indices\n",
    "        [extended_indices[0], extended_indices[-1]]  # Range of extended_indices\n",
    "    )\n",
    "\n",
    "    # Define the filename for saving the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # (1) **Assign Labels for Subgroups Based on Step 1**\n",
    "\n",
    "    \n",
    "    # Define **subtrajectory colors** (for cell trajectories)\n",
    "    real_cell_types = np.array(cell_ids_by_day[day2])\n",
    "    #unique_cell_types = np.unique(real_cell_types)\n",
    "    unique_cell_types = unique_labels\n",
    "    #subtrajectory_colors = list(sns.color_palette(\"tab20\", len(unique_cell_types)))\n",
    "    subtrajectory_colors = ['green', 'orange', 'purple', 'blue', 'red', 'brown']\n",
    "    #subtrajectory_colors = ['violet']\n",
    "    \n",
    "    # Define **violin plot colors** for the three time points\n",
    "    violin_colors = [\"black\", \"black\"]  # Green, Orange, Purple\n",
    "    \n",
    "    # Map each subgroup label to a **trajectory color** and shift labels from 0,1 → 1,2\n",
    "    unique_labels = np.unique(X1_hat_labels)\n",
    "    subgroup_color_map = {label: subtrajectory_colors[i % len(subtrajectory_colors)] for i, label in enumerate(unique_labels)}\n",
    "    label_mapping = {old_label: new_label + 1 for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Define filename for saving\n",
    "    subgroup_output_file = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene_of_interest}.png\"\n",
    "    \n",
    "    # (2) **Create Figure**\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # (3) **Ensure Proper x-axis Scaling**\n",
    "    num_points = len(indices)\n",
    "    x_positions = np.linspace(0, 4, num_points)  # Scale to match `[0, 2, 4]`\n",
    "    \n",
    "    # (4) **Extract Cell Trajectories for Each Gene**\n",
    "    cell_trajectories = {cell_idx: [] for cell_idx in range(X1_trpts[0].shape[0])}\n",
    "    \n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "    \n",
    "        # Extract **expression values of the gene of interest** from each cell at this time point\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "    \n",
    "        # Append the expression value at this time to each cell’s trajectory\n",
    "        for cell_idx, expr_value in enumerate(gene_expression_values):\n",
    "            cell_trajectories[cell_idx].append(expr_value)\n",
    "    \n",
    "    # (5) **Plot Individual Trajectories per Subgroup**\n",
    "    legend_patches = []  # Store legend handles\n",
    "    for label in unique_labels:\n",
    "        first_plotted = False  # Track if we added a legend entry for this subgroup\n",
    "        \n",
    "        for cell_idx, traj in cell_trajectories.items():\n",
    "            if len(traj) != len(x_positions):\n",
    "                continue  # Ensure trajectories align with time points\n",
    "    \n",
    "            if X1_hat_labels[cell_idx] == label:  # Match subgroup label from step 1\n",
    "                ax1.plot(\n",
    "                    x_positions, traj,  \n",
    "                    color=subgroup_color_map[label],  # ✅ Use the **subtrajectory colors**\n",
    "                    alpha=0.1, linewidth=0.8  \n",
    "                )\n",
    "                \n",
    "                # Add a single legend entry for each subgroup (renaming from 0,1 → 1,2)\n",
    "                if not first_plotted:\n",
    "                    legend_patches.append(mpatches.Patch(color=subgroup_color_map[label], label=f'Trajectory of {label_mapping[label]} phenotypic shift'))\n",
    "                    first_plotted = True\n",
    "    \n",
    "    # (6) **Ensure Violin Plots are at `[0, 2, 4]` & Appear in Front**\n",
    "    violin_data = [\n",
    "        gene_expression_X1_normalized,\n",
    "        *gene_expression_intermediates_normalized,\n",
    "        gene_expression_X2_normalized\n",
    "    ]\n",
    "    \n",
    "    violin_x_positions = np.array([0, 4])  # Ensure correct positions\n",
    "    \n",
    "    # 🎻 **Plot Violin Plots with Correct Colors and Transparency**\n",
    "    for i, (x_pos, data) in enumerate(zip(violin_x_positions, violin_data)):\n",
    "        violin_parts = sns.violinplot(\n",
    "            data=[data],  \n",
    "            ax=ax1,\n",
    "            inner=None,  # ✅ REMOVE QUARTILE LINES\n",
    "            linewidth=1.2,\n",
    "            width=0.7,\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            color=violin_colors[i],  # ✅ Assign correct color\n",
    "            alpha=0.8,  # ✅ MAKE TRANSPARENT\n",
    "            zorder=3  # ✅ BRINGS VIOLINS TO THE FRONT\n",
    "        )\n",
    "        \n",
    "        # **Manually Adjust X-Position of Each Violin**\n",
    "        for violin in ax1.collections[-1:]:  # Only adjust the last added violin\n",
    "            for path in violin.get_paths():\n",
    "                path.vertices[:, 0] += x_pos - path.vertices[:, 0].mean()  \n",
    "    \n",
    "    # **Expand x-axis limits to prevent cutting off last violin plot**\n",
    "    ax1.set_xlim(-0.5, 4.5)  \n",
    "    \n",
    "    # 🛠 **Fix x-axis labels and ensure proper alignment**\n",
    "    ax1.set_xticks([0, 4])  \n",
    "    #ax1.set_xticklabels([0, 4], fontsize=32)\n",
    "    ax1.set_xticklabels([\"Pre-treatment\", \"Post-treatment\"], fontsize=46)\n",
    "    ax1.tick_params(axis='y', labelsize=46)\n",
    "    \n",
    "    ax1.set_xlabel('Time', fontsize=46)\n",
    "    ax1.set_ylabel('Gene Expression', fontsize=46)\n",
    "    ax1.set_title(f'{gene_of_interest}', fontsize=46)\n",
    "\n",
    "\n",
    "    # 🎨 **Save the main figure without a legend**\n",
    "    plt.savefig(subgroup_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # 🎨 **Redefine `legend_patches` to Include a Green Bar**\n",
    "    \n",
    "    #legend_patches = [\n",
    "    #    mlines.Line2D([], [], color=\"violet\", linestyle=\"-\", linewidth=3, \n",
    "    #                  label=\"Hallmark dynamics of each single cell\")\n",
    "    #]\n",
    "\n",
    "    label_descriptions = {\n",
    "    \"low\": \"Trajectory of low phenotypic shift\",\n",
    "    \"medium\": \"Trajectory of medium phenotypic shift\",\n",
    "    \"high\": \"Trajectory of high phenotypic shift\"}\n",
    "\n",
    "\n",
    "    # Thicker lines using `linewidth`\n",
    "    legend_patches = [\n",
    "        mlines.Line2D(\n",
    "            [], [], color=color, linestyle='-', linewidth=3,  # ← thicker line here\n",
    "            markersize=10,\n",
    "            label=f\"{label_descriptions.get(ctype, '')}\"\n",
    "        )\n",
    "        for ctype, color in zip(unique_cell_types, subtrajectory_colors)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 🎨 **Violin Plot Legend**\n",
    "    violin_legend_patches = [\n",
    "        mpatches.Patch(color=\"black\", label=\"Input Data\")\n",
    "    ]\n",
    "    \n",
    "    # 🎨 **Create Separate Legend Figure (HORIZONTAL LAYOUT)**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(10, 2))  # Wider aspect ratio for horizontal layout\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Combine both legends**\n",
    "    combined_legend = legend_patches + violin_legend_patches\n",
    "    \n",
    "    ax_legend.legend(\n",
    "        handles=combined_legend,\n",
    "        loc=\"center\", fontsize=24, title=\"\",\n",
    "        title_fontsize=24, ncol=len(combined_legend),  # Horizontal layout\n",
    "        frameon=True, handletextpad=2, columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # Save the separate legend\n",
    "    legend_output_file = subgroup_output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278d60e-2645-4946-afb9-bfc64663642a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results for clinical data\n",
    "\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "#intermediate_t = [1,2,3]\n",
    "intermediate_t = [4]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Average_gene_dynamics_whole_saveonly_single_trajectory_clinical(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e18839-42db-4b04-83bd-d05800cbf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the gene expression dynamics png as pdf - clincial data\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"Palbo_887_nofibroblast_malignant_Rgene_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes\n",
    "pdf_path = f\"{output_dir}/Celltypes_deviated_trajectories_violin_plot.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa61a0-2826-4027-8c06-829f9e456b80",
   "metadata": {},
   "source": [
    "## Comparsion of Predicted and Test distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99604f-3cc5-4347-b37f-e09dd501cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of single genes comparions (Intermediate time points only) - Stem Cell data\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def Compare_Distribution_Trajectories_Intermediate_mESC(\n",
    "    source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "    intermediate_t=None, d_red=2, random_state=42, exp_memo='2'):\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Extract gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    # Correct snapshot extraction based on scaling\n",
    "    snapshots_per_day = len(X1_trpts) / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(day * snapshots_per_day) for day in intermediate_t]\n",
    "\n",
    "    kde_predicted_data = []\n",
    "    for idx in scaled_intermediate_indices:\n",
    "        if idx >= len(X1_trpts):\n",
    "            idx = len(X1_trpts) - 1\n",
    "        gene_expr_predicted = pca.inverse_transform(X1_trpts[idx])[:, gene_index]\n",
    "        kde_predicted_data.append(gene_expr_predicted)\n",
    "\n",
    "    # Visualization setup\n",
    "    num_plots = len(intermediate_t)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 5), sharey=True)\n",
    "\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    #test_data_colors = [\"#2ca02c\", \"#8c564b\"] #Sample 3\n",
    "    #predicted_colors = [\"#1b6420\", \"#5c3930\"] #Sample 3\n",
    "\n",
    "    test_data_colors = [\"#2ca02c\", \"#8c564b\", \"#3cb44b\"]  #Sample 1\n",
    "    predicted_colors = [\"#1b6420\", \"#5c3930\", \"#228B22\"]  #Sample 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    # Initialize list to store legend handles per intermediate time\n",
    "    legend_patches_list = []\n",
    "    \n",
    "    # Generate KDE plots\n",
    "    for i, (ax, t, test_vals, pred_vals) in enumerate(zip(axes, intermediate_t, kde_test_data, kde_predicted_data)):\n",
    "    \n",
    "        all_vals = np.concatenate([test_vals, pred_vals])\n",
    "        x_min, x_max = np.min(all_vals), np.max(all_vals)\n",
    "        x_margin = (x_max - x_min) * 0.2\n",
    "        x_range = np.linspace(x_min - x_margin, x_max + x_margin, 300)\n",
    "    \n",
    "        kde_test = gaussian_kde(test_vals)\n",
    "        kde_pred = gaussian_kde(pred_vals)\n",
    "    \n",
    "        test_density = kde_test(x_range)\n",
    "        pred_density = kde_pred(x_range)\n",
    "    \n",
    "        y_max = max(test_density.max(), pred_density.max()) * 2\n",
    "    \n",
    "        ax.fill_between(x_range, test_density, color=test_data_colors[i % len(test_data_colors)], alpha=0.5)\n",
    "        ax.plot(x_range, test_density, color=test_data_colors[i % len(test_data_colors)], linewidth=2)\n",
    "    \n",
    "        ax.fill_between(x_range, pred_density, color=predicted_colors[i % len(predicted_colors)], alpha=0.5)\n",
    "        ax.plot(x_range, pred_density, color=predicted_colors[i % len(predicted_colors)], linestyle=\"dashed\", linewidth=2)\n",
    "    \n",
    "        ax.set_title(f\"Time {t}\", fontsize=26)\n",
    "        ax.set_xlabel(\"Gene Expression\", fontsize=26)\n",
    "        ax.set_ylim(0, y_max)\n",
    "        ax.set_ylabel(\"Density\", fontsize=26)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "    \n",
    "        plt.suptitle(f\"KDE for {gene_of_interest}\", fontsize=26)\n",
    "    \n",
    "\n",
    "        # **Legend Entry for This Time Point**\n",
    "        legend_patches_list.append([\n",
    "            # **Test Data: Dashed Line**\n",
    "            mlines.Line2D([], [], color=test_data_colors[i % len(test_data_colors)], linestyle=\"solid\", linewidth=3,\n",
    "                          label=f\"Test Data time {t}\"),\n",
    "            \n",
    "            # **Predicted Data: Solid Line**\n",
    "            mlines.Line2D([], [], color=predicted_colors[i % len(predicted_colors)], linestyle=\"dashed\", linewidth=3,\n",
    "                          label=f\"Predicted time {t}\")\n",
    "        ])\n",
    "        \n",
    "    # **Save KDE plot (without legend)**\n",
    "    output_file = f\"{output_dir}/KDE_Intermediate_Only_updated_{gene_of_interest}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"KDE plot saved: {output_file}\")\n",
    "    \n",
    "    # **(2) Create a Separate Figure for the Legend**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(len(intermediate_t) * 3, 2))  # Adjust width dynamically\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Flatten legend handles into a single row-style list**\n",
    "    flattened_legend_patches = []\n",
    "    for group in legend_patches_list:\n",
    "        for entry in group:\n",
    "            flattened_legend_patches.append(entry)\n",
    "    \n",
    "    # **Create a Row-Style Legend with Box + Line for Each Entry**\n",
    "    ax_legend.legend(\n",
    "        handles=flattened_legend_patches,\n",
    "        loc=\"center\",\n",
    "        fontsize=22,\n",
    "        ncol=2,  # Ensures (Test, Predicted) pairs stay together\n",
    "        frameon=True,\n",
    "        handletextpad=1.5,\n",
    "        columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # **Save the separate legend**\n",
    "    legend_output_file = output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Legend plot saved separately at: {legend_output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf2247-0e7b-421f-8c24-d227c2d57042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot restuls for Stem cell data\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,3]\n",
    "#intermediate_t = [2]\n",
    "\n",
    "d_red= 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Compare_Distribution_Trajectories_Intermediate_mESC(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9b872-6f7a-4804-8cca-c92a10a35fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF combination for comparison distributions  (Stem cell data)\n",
    "## save the gene expression dynamics png as pdf\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/KDE_Intermediate_Only_updated_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes \n",
    "## Selected genes (no difference by TV)  ['AXL', 'HNMT', 'TMEM45B', 'SSH3', 'SHROOM3', 'PRSS22', 'SERINC2', 'EVPL', 'GALNT3', 'DSP', 'ELMO3', 'KRTCAP3', 'KRT19', 'C1orf116', 'CDS1', 'INADL']\n",
    "## Selected genes (no difference by TV and KL) ['HNMT', 'TMEM45B', 'SHROOM3', 'PRSS22', 'SERINC2', 'KRTCAP3', 'C1orf116', 'CDS1']  \n",
    "pdf_path = f\"{output_dir}/combined_gene_expression_KDE_Intermediate_Only_updated.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b886b-b547-4ab4-94d9-15f3d7862e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of single genes comparions (Intermediate time points only) - EMT data\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def Compare_Distribution_Trajectories_Intermediate_EMT(\n",
    "    source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "    intermediate_t=None, d_red=2, random_state=42, exp_memo='2'):\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Extract gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    # Correct snapshot extraction based on scaling\n",
    "    snapshots_per_day = len(X1_trpts) / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(day * snapshots_per_day) for day in intermediate_t]\n",
    "\n",
    "    kde_predicted_data = []\n",
    "    for idx in scaled_intermediate_indices:\n",
    "        if idx >= len(X1_trpts):\n",
    "            idx = len(X1_trpts) - 1\n",
    "        gene_expr_predicted = pca.inverse_transform(X1_trpts[idx])[:, gene_index]\n",
    "        kde_predicted_data.append(gene_expr_predicted)\n",
    "\n",
    "    # Visualization setup\n",
    "    num_plots = len(intermediate_t)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 5), sharey=True)\n",
    "\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    #test_data_colors = [\"#2ca02c\", \"#8c564b\"] #Sample 3\n",
    "    #predicted_colors = [\"#1b6420\", \"#5c3930\"] #Sample 3\n",
    "\n",
    "    test_data_colors = [\"#f58231\", \"#911eb4\", \"#3cb44b\"]  #Sample 1\n",
    "    predicted_colors = [\"#D2691E\", \"#800080\", \"#228B22\"]  #Sample 1\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    # Initialize list to store legend handles per intermediate time\n",
    "    legend_patches_list = []\n",
    "    \n",
    "    # Generate KDE plots\n",
    "    for i, (ax, t, test_vals, pred_vals) in enumerate(zip(axes, intermediate_t, kde_test_data, kde_predicted_data)):\n",
    "    \n",
    "        all_vals = np.concatenate([test_vals, pred_vals])\n",
    "        x_min, x_max = np.min(all_vals), np.max(all_vals)\n",
    "        x_margin = (x_max - x_min) * 0.2\n",
    "        x_range = np.linspace(x_min - x_margin, x_max + x_margin, 300)\n",
    "    \n",
    "        kde_test = gaussian_kde(test_vals)\n",
    "        kde_pred = gaussian_kde(pred_vals)\n",
    "    \n",
    "        test_density = kde_test(x_range)\n",
    "        pred_density = kde_pred(x_range)\n",
    "    \n",
    "        y_max = max(test_density.max(), pred_density.max()) * 2\n",
    "    \n",
    "        ax.fill_between(x_range, test_density, color=test_data_colors[i % len(test_data_colors)], alpha=0.5)\n",
    "        ax.plot(x_range, test_density, color=test_data_colors[i % len(test_data_colors)], linewidth=2)\n",
    "    \n",
    "        ax.fill_between(x_range, pred_density, color=predicted_colors[i % len(predicted_colors)], alpha=0.5)\n",
    "        ax.plot(x_range, pred_density, color=predicted_colors[i % len(predicted_colors)], linestyle=\"dashed\", linewidth=2)\n",
    "    \n",
    "        ax.set_title(f\"Day {t}\", fontsize=26)\n",
    "        ax.set_xlabel(\"Gene Expression\", fontsize=26)\n",
    "        ax.set_ylim(0, y_max)\n",
    "        ax.set_ylabel(\"Density\", fontsize=26)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=26)\n",
    "    \n",
    "        plt.suptitle(f\"KDE for {gene_of_interest}\", fontsize=26)\n",
    "    \n",
    "\n",
    "        # **Legend Entry for This Time Point**\n",
    "        legend_patches_list.append([\n",
    "            # **Test Data: Dashed Line**\n",
    "            mlines.Line2D([], [], color=test_data_colors[i % len(test_data_colors)], linestyle=\"solid\", linewidth=3,\n",
    "                          label=f\"Test Data day {t}\"),\n",
    "            \n",
    "            # **Predicted Data: Solid Line**\n",
    "            mlines.Line2D([], [], color=predicted_colors[i % len(predicted_colors)], linestyle=\"dashed\", linewidth=3,\n",
    "                          label=f\"Predicted day {t}\")\n",
    "        ])\n",
    "        \n",
    "    # **Save KDE plot (without legend)**\n",
    "    output_file = f\"{output_dir}/KDE_Intermediate_Only_updated_{gene_of_interest}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"KDE plot saved: {output_file}\")\n",
    "    \n",
    "    # **(2) Create a Separate Figure for the Legend**\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(len(intermediate_t) * 3, 2))  # Adjust width dynamically\n",
    "    ax_legend.axis(\"off\")  # Hide axes\n",
    "    \n",
    "    # **Flatten legend handles into a single row-style list**\n",
    "    flattened_legend_patches = []\n",
    "    for group in legend_patches_list:\n",
    "        for entry in group:\n",
    "            flattened_legend_patches.append(entry)\n",
    "    \n",
    "    # **Create a Row-Style Legend with Box + Line for Each Entry**\n",
    "    ax_legend.legend(\n",
    "        handles=flattened_legend_patches,\n",
    "        loc=\"center\",\n",
    "        fontsize=22,\n",
    "        ncol=2,  # Ensures (Test, Predicted) pairs stay together\n",
    "        frameon=True,\n",
    "        handletextpad=1.5,\n",
    "        columnspacing=2\n",
    "    )\n",
    "    \n",
    "    # **Save the separate legend**\n",
    "    legend_output_file = output_file.replace(\".png\", \"_legend.png\")\n",
    "    plt.savefig(legend_output_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Legend plot saved separately at: {legend_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8ca21-c1a9-493e-aa8d-c2f772825ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot restuls for EMT data\n",
    "\n",
    "\n",
    "genes_of_interest = gene_names # NANOG, SOX2\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "#intermediate_t = [1,3]\n",
    "intermediate_t = [2]\n",
    "\n",
    "d_red= 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Call the function with the current gene\n",
    "        Compare_Distribution_Trajectories_Intermediate_EMT(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c7110-d4d8-4149-9e5c-eef52a792875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF combination for comparison distributions  (EMT data)\n",
    "## save the gene expression dynamics png as pdf\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/KDE_Intermediate_Only_updated_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = gene_names  # List of genes \n",
    "## Selected genes (no difference by TV)  ['AXL', 'HNMT', 'TMEM45B', 'SSH3', 'SHROOM3', 'PRSS22', 'SERINC2', 'EVPL', 'GALNT3', 'DSP', 'ELMO3', 'KRTCAP3', 'KRT19', 'C1orf116', 'CDS1', 'INADL']\n",
    "## Selected genes (no difference by TV and KL) ['HNMT', 'TMEM45B', 'SHROOM3', 'PRSS22', 'SERINC2', 'KRTCAP3', 'C1orf116', 'CDS1']  \n",
    "pdf_path = f\"{output_dir}/combined_gene_expression_KDE_Intermediate_Only_updated.pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225af0a-f8b9-4238-a54e-64520854af4e",
   "metadata": {},
   "source": [
    "## Permutation test for predicted distributions to test data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbdc0b-7311-4b87-8c3b-1bd0f7ba27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantify the errors by using W2 distance (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different.\n",
    "\n",
    "    Parameters:\n",
    "    - source_t: Start time point (not included in the plot)\n",
    "    - target_t: End time point (not included in the plot)\n",
    "    - optimal_k: Number of clusters for KMeans\n",
    "    - gene_of_interest: The gene whose expression is analyzed\n",
    "    - index: Step size for trajectory extraction\n",
    "    - max_i: Maximum index for trajectory extraction\n",
    "    - intermediate_t: List of intermediate time points (defaults to [1] if not provided)\n",
    "    - d_red: Dimensionality reduction method\n",
    "    - random_state: Random seed\n",
    "    - exp_memo: Experiment identifier\n",
    "    - num_permutations: Number of permutations for significance testing.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2 distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure intermediate_t has a default value\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute the observed W2 distance\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2_distances = []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  # Shuffle data\n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2 = wasserstein_distance(perm_test_sample, perm_pred_sample)\n",
    "            permuted_w2_distances.append(permuted_w2)\n",
    "\n",
    "        # Compute p-value (proportion of permuted distances ≥ observed W2)\n",
    "        p_value = np.mean(np.array(permuted_w2_distances) >= observed_w2)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2,\n",
    "            \"Permutation Mean W2\": np.mean(permuted_w2_distances),\n",
    "            \"Permutation Std W2\": np.std(permuted_w2_distances),\n",
    "            \"p-value\": p_value\n",
    "        }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Permutation Test Summary ---\")\n",
    "    for time, results in permutation_results.items():\n",
    "        print(f\"Time {time}:\")\n",
    "        print(f\"  Observed W2: {results['Observed W2']:.4f}\")\n",
    "        print(f\"  Mean Permutation W2: {results['Permutation Mean W2']:.4f} ± {results['Permutation Std W2']:.4f}\")\n",
    "        print(f\"  p-value: {results['p-value']:.4f}\")\n",
    "        if results[\"p-value\"] < 0.05:\n",
    "            print(\"  ** Significant Difference (Reject Null Hypothesis) **\")\n",
    "        else:\n",
    "            print(\"  No Significant Difference (Cannot Reject Null Hypothesis)\")\n",
    "\n",
    "    return permutation_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843bf8b-a5c9-4fa3-943c-2f647a9426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics but No Sinkhorn (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def maximum_mean_discrepancy(X, Y, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) between two distributions using an RBF kernel.\n",
    "    \"\"\"\n",
    "    K_xx = rbf_kernel(X[:, None], X[:, None], gamma=gamma)\n",
    "    K_yy = rbf_kernel(Y[:, None], Y[:, None], gamma=gamma)\n",
    "    K_xy = rbf_kernel(X[:, None], Y[:, None], gamma=gamma)\n",
    "\n",
    "    return K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()\n",
    "\n",
    "def Compare_Distribution_Permutation_Test(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                          intermediate_t=None, d_red=2, random_state=42, exp_memo='2', \n",
    "                                          num_permutations=1000, mmd_gamma=1.0):\n",
    "    \"\"\"\n",
    "    Performs a permutation test to evaluate whether the predicted and test gene expression \n",
    "    distributions are significantly different using W2 and MMD.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing W2, MMD distances, permutation test results, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method and dimension is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Define intermediate time points\n",
    "    intermediate_only_points = intermediate_t  \n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract gene expression values for test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract trajectory-based predicted distributions at each intermediate time point\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "\n",
    "        # Extract gene expression at this time step\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "\n",
    "        # Assign to the corresponding intermediate time point\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    permutation_results = {}\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # Compute observed metrics\n",
    "        observed_w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        observed_mmd = maximum_mean_discrepancy(test_vals, predicted_vals, gamma=mmd_gamma)\n",
    "\n",
    "        # Perform permutation test\n",
    "        combined_vals = np.concatenate([test_vals, predicted_vals])\n",
    "        permuted_w2, permuted_mmd = [], []\n",
    "\n",
    "        for _ in range(num_permutations):\n",
    "            np.random.shuffle(combined_vals)  \n",
    "            perm_test_sample = combined_vals[:len(test_vals)]\n",
    "            perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "            permuted_w2.append(wasserstein_distance(perm_test_sample, perm_pred_sample))\n",
    "            permuted_mmd.append(maximum_mean_discrepancy(perm_test_sample, perm_pred_sample, gamma=mmd_gamma))\n",
    "\n",
    "        # Compute p-values\n",
    "        p_w2 = np.mean(np.array(permuted_w2) >= observed_w2)\n",
    "        p_mmd = np.mean(np.array(permuted_mmd) >= observed_mmd)\n",
    "\n",
    "        # Store results\n",
    "        permutation_results[time] = {\n",
    "            \"Observed W2\": observed_w2, \"p_W2\": p_w2,\n",
    "            \"Observed MMD\": observed_mmd, \"p_MMD\": p_mmd\n",
    "        }\n",
    "\n",
    "    return permutation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3d5c8-69ed-464f-85b8-8f7d53d31c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full metrics With Sinkhorn (permutation test)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from geomloss import SamplesLoss  # Sinkhorn divergence\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with entropy regularization.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "\n",
    "def permutation_test_sinkhorn(test_vals, pred_vals, num_permutations=1000, epsilon=0.5):\n",
    "    \"\"\"Permutation test for Sinkhorn divergence.\"\"\"\n",
    "    observed_stat = sinkhorn_divergence(test_vals, pred_vals, epsilon)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        perm_stat = sinkhorn_divergence(perm_test_sample, perm_pred_sample, epsilon)\n",
    "        permuted_stats.append(perm_stat)\n",
    "\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Sinkhorn(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                  intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                  num_permutations=1000, sinkhorn_epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence with correct scaling and permutation test.\"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"PCA mapping method not available.\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correct scaling\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    predicted_data = []\n",
    "    for snapshot_idx in scaled_intermediate_indices:\n",
    "        if snapshot_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[snapshot_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        predicted_vals = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_data.append(predicted_vals)\n",
    "\n",
    "    results = []\n",
    "    for time, test_vals, pred_vals in zip(intermediate_t, test_data, predicted_data):\n",
    "\n",
    "        # Match sample sizes\n",
    "        min_size = min(len(test_vals), len(pred_vals))\n",
    "        test_vals, pred_vals = resample(test_vals, n_samples=min_size, random_state=42), \\\n",
    "                               resample(pred_vals, n_samples=min_size, random_state=42)\n",
    "\n",
    "        # Compute Sinkhorn and permutation test\n",
    "        sinkhorn_stat, p_sinkhorn = permutation_test_sinkhorn(\n",
    "            test_vals, pred_vals, num_permutations, sinkhorn_epsilon\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"Sinkhorn Divergence\": sinkhorn_stat,\n",
    "            \"Sinkhorn p-value\": p_sinkhorn\n",
    "        })\n",
    "\n",
    "    output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(output_dir, f\"Sinkhorn_metrics_{gene_of_interest}.csv\")\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd419e-09fe-4e6a-bd6b-cd0ddc14afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 1\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean ± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, and Jensen-Shannon).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    intermediate_only_points = intermediate_t  \n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_only_points\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions\n",
    "    predicted_distributions = {t: [] for t in intermediate_only_points}\n",
    "    indices = range(0, len(X1_trpts), index)\n",
    "\n",
    "    for i, time_idx in enumerate(indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        if i < len(intermediate_only_points):  \n",
    "            predicted_distributions[intermediate_only_points[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_only_points]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_only_points):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "        \n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV ± Std\": f\"{mean_perm_tv:.4f} ± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL ± Std\": f\"{mean_perm_kl:.4f} ± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS ± Std\": f\"{mean_perm_js:.4f} ± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db53770-527b-4c6e-9212-b23285fb8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample 3\n",
    "\n",
    "## Permuation by other statistical tests (permutation test)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample  # Resampling for matching sizes\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    \"\"\"Compute Total Variation (TV) distance between two probability distributions.\"\"\"\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler (KL) divergence.\"\"\"\n",
    "    p = np.clip(p, 1e-10, None)  # Avoid zero division\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    \"\"\"Resample the larger array to match the size of the smaller one.\"\"\"\n",
    "    min_size = min(len(X), len(Y))\n",
    "    X_resampled = resample(X, n_samples=min_size, replace=False, random_state=42)\n",
    "    Y_resampled = resample(Y, n_samples=min_size, replace=False, random_state=42)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    \"\"\"Perform a permutation test for a given statistic and return mean ± std.\"\"\"\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "\n",
    "        permuted_stat = stat_func(perm_test_sample, perm_pred_sample)\n",
    "        permuted_stats.append(permuted_stat)\n",
    "\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    \"\"\"\n",
    "    Computes multiple statistical metrics (KS test, TV, KL, JS, and Mean Comparison).\n",
    "    Includes permutation tests to assess significance.\n",
    "    Saves results to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing statistics and permutation test p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    # Load PCA\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"PCA mapping for the reduction method is not available\")\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    # Correctly scale the intermediate time points\n",
    "    num_snapshots = len(X1_trpts)\n",
    "    scaling_factor = num_snapshots / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    # Extract the gene index\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "\n",
    "    # Extract test data distributions\n",
    "    kde_test_data = [\n",
    "        pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t\n",
    "    ]\n",
    "\n",
    "    # Extract predicted distributions using scaled indices\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            break\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            break\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    # Convert trajectory distributions into a list format\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    # Store results\n",
    "    metric_results = []\n",
    "\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "\n",
    "        # **Ensure test and predicted values have the same size**\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        # **Compute different statistics**\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)  # Kolmogorov-Smirnov test\n",
    "\n",
    "        # Compute distribution distances\n",
    "        tv_distance = total_variation_distance(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        kl_div = kl_divergence(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "        js_div = jensenshannon(np.histogram(test_vals, bins=50, density=True)[0],\n",
    "                               np.histogram(predicted_vals, bins=50, density=True)[0])\n",
    "\n",
    "        # **Permutation Tests**\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        # **Mean and Standard Deviation Comparison**\n",
    "        mean_test = np.mean(test_vals)\n",
    "        mean_pred = np.mean(predicted_vals)\n",
    "        std_test = np.std(test_vals)\n",
    "        std_pred = np.std(predicted_vals)\n",
    "        mean_diff = mean_test - mean_pred\n",
    "        std_diff = std_test - std_pred\n",
    "\n",
    "        # Perform a t-test to compare means\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        # **Store results**\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"Total Variation Distance\": tv_distance,\n",
    "            \"Permutation TV ± Std\": f\"{mean_perm_tv:.4f} ± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl_div,\n",
    "            \"Permutation KL ± Std\": f\"{mean_perm_kl:.4f} ± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js_div,\n",
    "            \"Permutation JS ± Std\": f\"{mean_perm_js:.4f} ± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Test\": mean_test,\n",
    "            \"Mean Predicted\": mean_pred,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Test\": std_test,\n",
    "            \"Standard Deviation Predicted\": std_pred,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save as CSV\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        output_csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return metric_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b6e27-ef58-47a8-8ee5-76b66bc842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Permuation by statistical tests (permutation test) - stem cell data\n",
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ttest_ind\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p = np.clip(p, 1e-10, None)\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "def match_sample_sizes(X, Y):\n",
    "    min_size = min(len(X), len(Y))\n",
    "    return resample(X, n_samples=min_size, random_state=42), resample(Y, n_samples=min_size, random_state=42)\n",
    "\n",
    "def sinkhorn_divergence(X, Y, epsilon=0.5):\n",
    "    \"\"\"Compute Sinkhorn divergence using PyTorch and geomloss.\"\"\"\n",
    "    X = torch.from_numpy(X.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(Y.reshape(-1, 1)).float()\n",
    "\n",
    "    min_samples = min(X.shape[0], Y.shape[0])\n",
    "    X, Y = X[:min_samples], Y[:min_samples]\n",
    "\n",
    "    sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=epsilon)\n",
    "    return (\n",
    "        sinkhorn_loss(X, Y).item()\n",
    "        - 0.5 * sinkhorn_loss(X, X).item()\n",
    "        - 0.5 * sinkhorn_loss(Y, Y).item()\n",
    "    )\n",
    "\n",
    "def permutation_test(stat_func, test_vals, pred_vals, num_permutations=1000):\n",
    "    observed_stat = stat_func(test_vals, pred_vals)\n",
    "    combined_vals = np.concatenate([test_vals, pred_vals])\n",
    "    permuted_stats = []\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined_vals)\n",
    "        perm_test_sample = combined_vals[:len(test_vals)]\n",
    "        perm_pred_sample = combined_vals[len(test_vals):]\n",
    "        permuted_stats.append(stat_func(perm_test_sample, perm_pred_sample))\n",
    "    mean_perm = np.mean(permuted_stats)\n",
    "    std_perm = np.std(permuted_stats)\n",
    "    p_value = np.mean(np.array(permuted_stats) >= observed_stat)\n",
    "    return observed_stat, mean_perm, std_perm, p_value\n",
    "\n",
    "def Compare_Distribution_Statistics(source_t, target_t, optimal_k, gene_of_interest, index, max_i,\n",
    "                                    intermediate_t=None, d_red=2, random_state=42, exp_memo='2',\n",
    "                                    num_permutations=1000, save_csv=True):\n",
    "    if intermediate_t is None:\n",
    "        intermediate_t = [1]\n",
    "\n",
    "    filename = result_dir + exp_memo + \".pickle\"\n",
    "    W, b, p = load_W(filename)\n",
    "\n",
    "    if dim_red_method == 'EMT_PCA':\n",
    "        pca_filename = f\"emt_pca_{d_red}.pkl\"\n",
    "    elif dim_red_method == 'PCA':\n",
    "        pca_filename = f\"pca_{d_red}.pkl\"\n",
    "    else:\n",
    "        print(\"❌ PCA method not available\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + pca_filename, \"rb\") as fr:\n",
    "        [pca] = pk.load(fr)\n",
    "\n",
    "    dt = p['numerical_ts'][-1] / 200\n",
    "    X1_trpts = time_integration(pca.transform(mats[0]), T=p['numerical_ts'][-1], dt=dt)\n",
    "\n",
    "    scaling_factor = len(X1_trpts) / (target_t - source_t)\n",
    "    scaled_intermediate_indices = [int(t * scaling_factor) for t in intermediate_t]\n",
    "\n",
    "    gene_index = df_reduced_emt.columns.get_loc(gene_of_interest) - 1\n",
    "    kde_test_data = [pca.inverse_transform(pca.transform(mats[t]))[:, gene_index] for t in intermediate_t]\n",
    "\n",
    "    predicted_distributions = {t: [] for t in intermediate_t}\n",
    "    for i, time_idx in enumerate(scaled_intermediate_indices):\n",
    "        if time_idx > max_i:\n",
    "            continue\n",
    "        X1_trpt = X1_trpts[time_idx]\n",
    "        if np.isnan(X1_trpt).any():\n",
    "            continue\n",
    "        gene_expression_values = pca.inverse_transform(X1_trpt)[:, gene_index]\n",
    "        predicted_distributions[intermediate_t[i]].extend(gene_expression_values)\n",
    "\n",
    "    kde_predicted_data = [np.array(predicted_distributions[t]) for t in intermediate_t]\n",
    "\n",
    "    metric_results = []\n",
    "    for i, time in enumerate(intermediate_t):\n",
    "        test_vals = kde_test_data[i]\n",
    "        predicted_vals = kde_predicted_data[i]\n",
    "        test_vals, predicted_vals = match_sample_sizes(test_vals, predicted_vals)\n",
    "\n",
    "        ks_stat, ks_pval = ks_2samp(test_vals, predicted_vals)\n",
    "\n",
    "        hist_test = np.histogram(test_vals, bins=50, density=True)[0]\n",
    "        hist_pred = np.histogram(predicted_vals, bins=50, density=True)[0]\n",
    "\n",
    "        tv = total_variation_distance(hist_test, hist_pred)\n",
    "        kl = kl_divergence(hist_test, hist_pred)\n",
    "        js = jensenshannon(hist_test, hist_pred)\n",
    "        w2 = wasserstein_distance(test_vals, predicted_vals)\n",
    "        sink = sinkhorn_divergence(test_vals, predicted_vals)\n",
    "\n",
    "        perm_tv, mean_perm_tv, std_perm_tv, p_tv = permutation_test(total_variation_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_kl, mean_perm_kl, std_perm_kl, p_kl = permutation_test(kl_divergence, test_vals, predicted_vals, num_permutations)\n",
    "        perm_js, mean_perm_js, std_perm_js, p_js = permutation_test(jensenshannon, test_vals, predicted_vals, num_permutations)\n",
    "        perm_w2, mean_perm_w2, std_perm_w2, p_w2 = permutation_test(wasserstein_distance, test_vals, predicted_vals, num_permutations)\n",
    "        perm_sink, mean_perm_sink, std_perm_sink, p_sink = permutation_test(sinkhorn_divergence, test_vals, predicted_vals, num_permutations)\n",
    "\n",
    "        mean_diff = np.mean(test_vals) - np.mean(predicted_vals)\n",
    "        std_diff = np.std(test_vals) - np.std(predicted_vals)\n",
    "        t_stat, t_pval = ttest_ind(test_vals, predicted_vals, equal_var=False)\n",
    "\n",
    "        metric_results.append({\n",
    "            \"Time\": time,\n",
    "            \"Gene\": gene_of_interest,\n",
    "            \"KS Test Statistic\": ks_stat,\n",
    "            \"KS p-value\": ks_pval,\n",
    "            \"W2 Distance\": w2,\n",
    "            \"Permutation W2 ± Std\": f\"{mean_perm_w2:.4f} ± {std_perm_w2:.4f}\",\n",
    "            \"p-value W2\": p_w2,\n",
    "            \"Sinkhorn Distance\": sink,\n",
    "            \"Permutation Sinkhorn ± Std\": f\"{mean_perm_sink:.4f} ± {std_perm_sink:.4f}\",\n",
    "            \"p-value Sinkhorn\": p_sink,\n",
    "            \"Total Variation Distance\": tv,\n",
    "            \"Permutation TV ± Std\": f\"{mean_perm_tv:.4f} ± {std_perm_tv:.4f}\",\n",
    "            \"p-value TV\": p_tv,\n",
    "            \"KL Divergence\": kl,\n",
    "            \"Permutation KL ± Std\": f\"{mean_perm_kl:.4f} ± {std_perm_kl:.4f}\",\n",
    "            \"p-value KL\": p_kl,\n",
    "            \"Jensen-Shannon Divergence\": js,\n",
    "            \"Permutation JS ± Std\": f\"{mean_perm_js:.4f} ± {std_perm_js:.4f}\",\n",
    "            \"p-value JS\": p_js,\n",
    "            \"Mean Difference\": mean_diff,\n",
    "            \"Standard Deviation Difference\": std_diff,\n",
    "            \"T-test Statistic\": t_stat,\n",
    "            \"T-test p-value\": t_pval\n",
    "        })\n",
    "\n",
    "    if save_csv:\n",
    "        output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_results = pd.DataFrame(metric_results)\n",
    "        csv_path = os.path.join(output_dir, f\"statistical_metrics_{gene_of_interest}.csv\")\n",
    "        df_results.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Results saved to: {csv_path}\")\n",
    "\n",
    "    return metric_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d023-eb24-44eb-b76b-07e018303b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = gene_names # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [1,3]  # Intermediate time points\n",
    "d_red = 2\n",
    "random_state = 40\n",
    "exp_memo = 'EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV ± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL ± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 ± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn ± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7eb23-a0b9-47c1-8458-c7d0d0a1761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Venn Diagram of null hypothesis results for Stem cell data\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "exp_memo = \"EMT_dim2-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"Sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Ensure consistent column names\n",
    "tv_col = \"Reject Null Hypothesis (TV)\"\n",
    "kl_col = \"Reject Null Hypothesis (KL)\"\n",
    "sinkhorn_col = \"Reject Null Hypothesis (Sinkhorn)\"\n",
    "\n",
    "# Identify unique time points\n",
    "time_points = sorted(tv_df[\"Time\"].unique())\n",
    "\n",
    "# Iterate over each time point\n",
    "for time_point in time_points:\n",
    "    # Filter DataFrames for the current time point\n",
    "    tv_time_df = tv_df[tv_df[\"Time\"] == time_point]\n",
    "    kl_time_df = kl_df[kl_df[\"Time\"] == time_point]\n",
    "    sinkhorn_time_df = sinkhorn_df[sinkhorn_df[\"Time\"] == time_point]\n",
    "\n",
    "    # Identify genes with \"No\" in each metric\n",
    "    genes_no_tv = set(tv_time_df.loc[tv_time_df[tv_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_kl = set(kl_time_df.loc[kl_time_df[kl_col] == \"No\", \"Gene\"].unique())\n",
    "    genes_no_sinkhorn = set(sinkhorn_time_df.loc[sinkhorn_time_df[sinkhorn_col] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "    # Genes with at least one metric showing \"No\"\n",
    "    genes_at_least_one_no = genes_no_tv | genes_no_kl | genes_no_sinkhorn\n",
    "\n",
    "    # Genes with at least two metrics showing \"No\"\n",
    "    genes_at_least_two_no = (\n",
    "        (genes_no_tv & genes_no_kl) |\n",
    "        (genes_no_tv & genes_no_sinkhorn) |\n",
    "        (genes_no_kl & genes_no_sinkhorn)\n",
    "    )\n",
    "\n",
    "    # Genes with all three metrics showing \"No\"\n",
    "    genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "    # Prepare summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Criteria\": [\n",
    "            \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "            \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "        ],\n",
    "        \"Genes\": [\n",
    "            \", \".join(sorted(genes_at_least_one_no)),\n",
    "            \", \".join(sorted(genes_at_least_two_no)),\n",
    "            \", \".join(sorted(genes_all_three_no))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Print results for current time point\n",
    "    print(f\"\\n🔹 Summary of Genes by Null Hypothesis Rejection (Time {time_point}):\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary to CSV for current time point\n",
    "    summary_csv_path = os.path.join(output_dir, f\"genes_null_hypothesis_summary_time_{time_point}.csv\")\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary for Time {time_point} saved to {summary_csv_path}\")\n",
    "\n",
    "    # Create Venn diagram for current time point\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    venn = venn3(\n",
    "        [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "        set_labels=('TV', 'KL', 'Sinkhorn')\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Genes NOT Rejecting Null Hypothesis (Time {time_point})\", fontsize=16)\n",
    "\n",
    "    # Add summary annotations\n",
    "    x_pos = 0.6\n",
    "    y_pos = 0.6\n",
    "    step = 0.07\n",
    "\n",
    "    plt.text(x_pos, y_pos, f\"Genes in ≥1 metric: {len(genes_at_least_one_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - step, f\"Genes in ≥2 metrics: {len(genes_at_least_two_no)}\", fontsize=12)\n",
    "    plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_all_three_no)}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save Venn diagram\n",
    "    venn_path = os.path.join(output_dir, f\"genes_venn_diagram_time_{time_point}.png\")\n",
    "    plt.savefig(venn_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✅ Venn diagram for Time {time_point} saved to {venn_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f1e37-24d5-4466-ad0b-27793a954566",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV files for all the genes - Stem Cell data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "genes_of_interest = ['DSP'] # Set of genes\n",
    "source_t, target_t = 0, 4\n",
    "optimal_k = 2\n",
    "index = 1\n",
    "max_i = 200\n",
    "intermediate_t = [2]  # Intermediate time points\n",
    "d_red = 8\n",
    "random_state = 40\n",
    "exp_memo = '72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64'\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results in a list\n",
    "all_gene_results = []\n",
    "\n",
    "# Iterate over each gene in the list\n",
    "for gene in genes_of_interest:\n",
    "    print(f\"Processing gene: {gene}\")\n",
    "    try:\n",
    "        # Compute statistical metrics\n",
    "        results = Compare_Distribution_Statistics(\n",
    "            source_t, target_t, optimal_k, gene, index, max_i,\n",
    "            intermediate_t=intermediate_t, d_red=d_red,\n",
    "            random_state=random_state, exp_memo=exp_memo, num_permutations=100,\n",
    "            save_csv=False  # Prevent saving individual CSVs for each gene\n",
    "        )\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results[\"Gene\"] = gene  # Add gene column\n",
    "\n",
    "        # **Add Reject Null Hypothesis column (Yes/No)**\n",
    "        df_results[\"Reject Null Hypothesis (TV)\"] = df_results[\"p-value TV\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (KL)\"] = df_results[\"p-value KL\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (T-test)\"] = df_results[\"T-test p-value\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (W2)\"] = df_results[\"p-value W2\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        df_results[\"Reject Null Hypothesis (Sinkhorn)\"] = df_results[\"p-value Sinkhorn\"].apply(lambda p: \"No\" if p > 0.05 else \"Yes\")\n",
    "        all_gene_results.append(df_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "\n",
    "# Combine results for all genes into one DataFrame\n",
    "if all_gene_results:\n",
    "    combined_df = pd.concat(all_gene_results, ignore_index=True)\n",
    "\n",
    "    # **Save combined results for all genes**\n",
    "    combined_csv_path = os.path.join(output_dir, \"all_genes_statistical_metrics.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"All genes' results saved to {combined_csv_path}\")\n",
    "\n",
    "    # **Save each metric separately**\n",
    "    metrics = {\n",
    "        \"TV\": [\"Gene\", \"Time\", \"Total Variation Distance\", \"Permutation TV ± Std\", \"p-value TV\", \"Reject Null Hypothesis (TV)\"],\n",
    "        \"KL\": [\"Gene\", \"Time\", \"KL Divergence\", \"Permutation KL ± Std\", \"p-value KL\", \"Reject Null Hypothesis (KL)\"],\n",
    "        \"W2\": [\"Gene\", \"Time\", \"W2 Distance\", \"Permutation W2 ± Std\", \"p-value W2\", \"Reject Null Hypothesis (W2)\"],\n",
    "        \"Sinkhorn\": [\"Gene\", \"Time\", \"Sinkhorn Distance\", \"Permutation Sinkhorn ± Std\", \"p-value Sinkhorn\", \"Reject Null Hypothesis (Sinkhorn)\"],\n",
    "        \"T-test\": [\"Gene\", \"Time\", \"Mean Test\", \"T-test Statistic\", \"T-test p-value\", \"Reject Null Hypothesis (T-test)\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    for metric, cols in metrics.items():\n",
    "        if all(col in combined_df.columns for col in cols):  # Ensure columns exist\n",
    "            metric_df = combined_df[cols]\n",
    "            metric_csv_path = os.path.join(output_dir, f\"{metric}_metrics.csv\")\n",
    "            metric_df.to_csv(metric_csv_path, index=False)\n",
    "            print(f\"{metric} results saved to {metric_csv_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Some columns missing for {metric} metric.\")\n",
    "\n",
    "    # **Print out genes that did not reject the null hypothesis**\n",
    "    \n",
    "    # Genes where \"Reject Null Hypothesis (TV)\" is \"No\"\n",
    "    genes_no_TV = combined_df[combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in TV:\", genes_no_TV)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_KL = combined_df[combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in KL:\", genes_no_KL)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_W2 = combined_df[combined_df[\"Reject Null Hypothesis (W2)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in W2:\", genes_no_W2)\n",
    "\n",
    "    # Genes where \"Reject Null Hypothesis (KL)\" is \"No\"\n",
    "    genes_no_Sinkhorn = combined_df[combined_df[\"Reject Null Hypothesis (Sinkhorn)\"] == \"No\"][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in Sinkhorn:\", genes_no_Sinkhorn)\n",
    "\n",
    "    # Genes where both TV and KL are \"No\"\n",
    "    genes_no_TV_KL = combined_df[(combined_df[\"Reject Null Hypothesis (TV)\"] == \"No\") &\n",
    "                                 (combined_df[\"Reject Null Hypothesis (KL)\"] == \"No\")][\"Gene\"].unique()\n",
    "    print(\"\\nGenes that did NOT reject null hypothesis in BOTH TV and KL:\", genes_no_TV_KL)\n",
    "\n",
    "else:\n",
    "    print(\"No valid results generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc9409-fb78-4961-b9f2-9381d20689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining genes (preprocess)\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "result_dir = '%s/assets/Transport_genes/' % main_dir\n",
    "csv_dir = os.path.join(result_dir, 'Sample 3')\n",
    "\n",
    "\n",
    "# List of prefixes\n",
    "prefixes = ['sinkhorn']\n",
    "\n",
    "# Mapping prefixes to their corresponding p-value column names\n",
    "pval_columns = {\n",
    "    'sinkhorn': 'p_Sinkhorn_1',\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through each prefix\n",
    "for prefix in prefixes:\n",
    "    # Use glob to find matching files\n",
    "    matching_files = glob.glob(os.path.join(csv_dir, f\"{prefix}*.csv\"))\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"No files found for prefix '{prefix}'\")\n",
    "        continue\n",
    "    \n",
    "    # Read and concatenate files\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in matching_files], ignore_index=True)\n",
    "\n",
    "    # Check if the p-value column exists\n",
    "    p_col = pval_columns[prefix]\n",
    "    if p_col in combined_df.columns:\n",
    "        combined_df[f'Reject Null Hypothesis ({prefix[:-1].upper()})'] = combined_df[p_col].apply(\n",
    "            lambda p: 'No' if p > 0.05 else 'Yes'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Column '{p_col}' not found in files for prefix '{prefix}'.\")\n",
    "    \n",
    "    # Save the combined dataframe to CSV\n",
    "    output_filename = os.path.join(csv_dir, f\"{prefix}_metric.csv\")\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"✅ Combined {len(matching_files)} files into '{output_filename}' with hypothesis test results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f349b0a-a140-466f-a287-0c264f2808a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sample 1 (only one time point)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(result_dir, 'output', exp_memo)\n",
    "\n",
    "# Load existing metric CSV files\n",
    "tv_df = pd.read_csv(os.path.join(output_dir, \"TV_metrics.csv\"))\n",
    "kl_df = pd.read_csv(os.path.join(output_dir, \"KL_metrics.csv\"))\n",
    "sinkhorn_df = pd.read_csv(os.path.join(output_dir, \"sinkhorn_metrics.csv\"))\n",
    "\n",
    "# Identify genes with \"No\" in each metric\n",
    "genes_no_tv = set(tv_df.loc[tv_df[\"Reject Null Hypothesis (TV)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_kl = set(kl_df.loc[kl_df[\"Reject Null Hypothesis (KL)\"] == \"No\", \"Gene\"].unique())\n",
    "genes_no_sinkhorn = set(sinkhorn_df.loc[sinkhorn_df[\"Reject Null Hypothesis (SINKHOR)\"] == \"No\", \"Gene\"].unique())\n",
    "\n",
    "# Genes with at least one metric showing \"No\"\n",
    "genes_at_least_one_no = genes_no_tv.union(genes_no_kl).union(genes_no_sinkhorn)\n",
    "\n",
    "# Genes with at least two metrics showing \"No\"\n",
    "genes_at_least_two_no = (\n",
    "    (genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn)\n",
    ")\n",
    "\n",
    "# Genes with all three metrics showing \"No\"\n",
    "genes_all_three_no = genes_no_tv & genes_no_kl & genes_no_sinkhorn\n",
    "\n",
    "# Prepare dataframes for easy viewing and saving\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Criteria\": [\n",
    "        \"At least one metric (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"At least two metrics (TV, KL, or Sinkhorn) showing No\",\n",
    "        \"All three metrics (TV, KL, and Sinkhorn) showing No\"\n",
    "    ],\n",
    "    \"Genes\": [\n",
    "        \", \".join(sorted(genes_at_least_one_no)),\n",
    "        \", \".join(sorted(genes_at_least_two_no)),\n",
    "        \", \".join(sorted(genes_all_three_no))\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSummary of Genes by Null Hypothesis Rejection:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_csv_path = os.path.join(output_dir, \"genes_null_hypothesis_summary.csv\")\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary saved to {summary_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Venn diagram\n",
    "plt.figure(figsize=(12, 8))\n",
    "venn = venn3(\n",
    "    [genes_no_tv, genes_no_kl, genes_no_sinkhorn],\n",
    "    set_labels=('TV', 'KL', 'SINKHORN')\n",
    ")\n",
    "\n",
    "plt.title(\"Genes NOT Rejecting Null Hypothesis\", fontsize=16)\n",
    "\n",
    "# Additional summaries (positions adjusted)\n",
    "x_pos = 0.6  # Move further to the right\n",
    "y_pos = 0.6\n",
    "step = 0.07\n",
    "\n",
    "plt.text(x_pos, y_pos, f\"Genes in at least 1 metric: {len(genes_no_tv | genes_no_kl | genes_no_sinkhorn)}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - step, f\"Genes in at least 2 metrics: {len((genes_no_tv & genes_no_kl) | (genes_no_tv & genes_no_sinkhorn) | (genes_no_kl & genes_no_sinkhorn))}\", fontsize=12)\n",
    "plt.text(x_pos, y_pos - 2*step, f\"Genes in all 3 metrics: {len(genes_no_tv & genes_no_kl & genes_no_sinkhorn)}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "venn_path = os.path.join(output_dir, \"genes_venn_diagram.png\")\n",
    "plt.savefig(venn_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Venn diagram saved to {venn_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0eda51-abb4-453f-a59b-d0779d2ae090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing distribution plots based on the gene sets which have similar distributions\n",
    "\n",
    "## PDF combination for comparison distributions \n",
    "## save the gene expression dynamics png as pdf\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from math import ceil\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "def create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=25, grid_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a PDF with gene expression PNG images arranged in a grid layout while preserving original resolution.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory containing the PNG files.\n",
    "        exp_memo (str): Base name used in the PNG filenames.\n",
    "        gene_list (list): List of genes corresponding to the PNG files.\n",
    "        pdf_path (str): Path to save the output PDF file.\n",
    "        images_per_page (int): Number of images per page (default: 25).\n",
    "        grid_size (tuple): Grid size (rows, cols) for each page (default: 5x5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of PNG file paths\n",
    "    png_files = [\n",
    "        f\"{output_dir}/KDE_Intermediate_Only_{gene}.png\" for gene in gene_list\n",
    "    ]\n",
    "\n",
    "    # Check if all PNG files exist\n",
    "    missing_files = [file for file in png_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing and will be skipped:\\n{missing_files}\")\n",
    "\n",
    "    # Filter out missing files\n",
    "    png_files = [file for file in png_files if os.path.exists(file)]\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    total_pages = ceil(len(png_files) / images_per_page)\n",
    "\n",
    "    # Create the PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            # Create a figure with dynamically sized subplots\n",
    "            fig, axes = plt.subplots(*grid_size, figsize=(15, 15))  # Increased size for better resolution\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Plot images for the current page\n",
    "            start_idx = page * images_per_page\n",
    "            end_idx = start_idx + images_per_page\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                img_idx = start_idx + i\n",
    "                if img_idx < len(png_files):\n",
    "                    img = plt.imread(png_files[img_idx])\n",
    "                    ax.imshow(img, aspect='auto')  # Preserve aspect ratio\n",
    "                    ax.axis('off')  # Remove axes\n",
    "                    # Add filename as the title\n",
    "                    gene_name = gene_list[img_idx]\n",
    "                    ax.set_title('', fontsize=8)\n",
    "                else:\n",
    "                    ax.axis('off')  # Hide empty axes\n",
    "\n",
    "            # Save the page to the PDF with high resolution\n",
    "            pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    print(f\"✅ PDF saved to {pdf_path} with original image resolution.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "exp_memo = \"72GS_dim8-f_Lip=5e-2-t_size=50-network=64_64_64\"\n",
    "gene_list = ['DSP', 'ENPP5', 'EPB41L5', 'KRTCAP3', 'MMP2', 'RAB25', 'SERINC2', 'TMEM45B']  # List of genes \n",
    "## Selected genes (no difference by TV)  ['AXL', 'HNMT', 'TMEM45B', 'SSH3', 'SHROOM3', 'PRSS22', 'SERINC2', 'EVPL', 'GALNT3', 'DSP', 'ELMO3', 'KRTCAP3', 'KRT19', 'C1orf116', 'CDS1', 'INADL']\n",
    "## Selected genes (no difference by TV and KL) ['HNMT', 'TMEM45B', 'SHROOM3', 'PRSS22', 'SERINC2', 'KRTCAP3', 'C1orf116', 'CDS1']  \n",
    "pdf_path = f\"{output_dir}/selected_gene_expression_KDE_Intermediate_Only(two metrics).pdf\"  # Output PDF path\n",
    "\n",
    "create_pdf_from_gene_images(output_dir, exp_memo, gene_list, pdf_path, images_per_page=6, grid_size=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64d61d-8af3-4348-be9d-e912ceb0f268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
